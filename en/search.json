[{"content":"APM Server extension The APM Server receives data from APM agents and transforms them into Elasticsearch documents that can be visualised in Kibana.\nUsage To include APM Server in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the apm-server-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/apm-server/apm-server-compose.yml up Meanwhile, you can navigate to the APM application in Kibana and follow the setup instructions to get started.\nConnecting an agent to APM Server The most basic configuration to send traces to APM server is to specify the SERVICE_NAME and SERVICE_URL. Here is an example Python Flask configuration:\nimport elasticapm from elasticapm.contrib.flask import ElasticAPM from flask import Flask app = Flask(__name__) app.config['ELASTIC_APM'] = { # Set required service name. Allowed characters: # a-z, A-Z, 0-9, -, _, and space 'SERVICE_NAME': 'PYTHON_FLASK_TEST_APP', # Set custom APM Server URL (default: http://localhost:8200) 'SERVER_URL': 'http://apm-server:8200', 'DEBUG': True, } Configuration settings for each supported language are available in the APM documentation: APM Agents.\nChecking connectivity and importing default APM dashboards On the Kibana home page, click Add APM under the Observability panel. Click Check APM Server status to confirm the server is up and running. Click Check agent status to verify your agent has registered properly. Click Load Kibana objects to create an index pattern for APM. Click Launch APM to be taken to the APM dashboard. See also Running APM Server on Docker\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/apm-server/readme/"},{"content":"Curator Elasticsearch Curator helps you curate or manage your indices.\nUsage If you want to include the Curator extension, run Docker Compose from the root of the repository with an additional command line argument referencing the curator-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/curator/curator-compose.yml up This sample setup demonstrates how to run curator every minute using cron.\nAll configuration files are available in the config/ directory.\nDocumentation Curator Reference\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/curator/readme/"},{"content":"Enterprise Search extension Elastic Enterprise Search is a suite of products for search applications backed by the Elastic Stack.\nRequirements 2 GB of free RAM, on top of the resources required by the other stack components and extensions. Enterprise Search exposes the TCP port 3002 for its Web UI and API.\nUsage Generate an encryption key Enterprise Search requires one or more encryption keys to be configured before the initial startup. Failing to do so prevents the server from starting.\nEncryption keys can contain any series of characters. Elastic recommends using 256-bit keys for optimal security.\nThose encryption keys must be added manually to the config/enterprise-search.yml file. By default, the list of encryption keys is empty and must be populated using one of the following formats:\nsecret_management.encryption_keys: - my_first_encryption_key - my_second_encryption_key - ... secret_management.encryption_keys: [my_first_encryption_key, my_second_encryption_key, ...] ℹ️ To generate a strong encryption key, for example using the AES-256 cipher, you can use the OpenSSL utility or any other online/offline tool of your choice:\n$ openssl enc -aes-256 -P enter aes-256-cbc encryption password: \u003ca strong password\u003e Verifying - enter aes-256-cbc encryption password: \u003crepeat your strong password\u003e ... key=\u003cgenerated AES key\u003e Enable Elasticsearch’s API key service Enterprise Search requires Elasticsearch’s built-in API key service to be enabled in order to start. Unless Elasticsearch is configured to enable TLS on the HTTP interface (disabled by default), this service is disabled by default.\nTo enable it, modify the Elasticsearch configuration file in elasticsearch/config/elasticsearch.yml and add the following setting:\nxpack.security.authc.api_key.enabled: true Configure the Enterprise Search host in Kibana Kibana acts as the management interface to Enterprise Search.\nTo enable the management experience for Enterprise Search, modify the Kibana configuration file in kibana/config/kibana.yml and add the following setting:\nenterpriseSearch.host: http://enterprise-search:3002 Start the server To include Enterprise Search in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the enterprise-search-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml up Allow a few minutes for the stack to start, then open your web browser at the address http://localhost:3002 to see the Enterprise Search home page.\nEnterprise Search is configured on first boot with the following default credentials:\nuser: enterprise_search password: changeme Security The Enterprise Search password is defined inside the Compose file via the ENT_SEARCH_DEFAULT_PASSWORD environment variable. We highly recommend choosing a more secure password than the default one for security reasons.\nTo do so, change the value ENT_SEARCH_DEFAULT_PASSWORD environment variable inside the Compose file before the first boot:\nenterprise-search: environment: ENT_SEARCH_DEFAULT_PASSWORD: {{some strong password}} ⚠️ The default Enterprise Search password can only be set during the initial boot. Once the password is persisted in Elasticsearch, it can only be changed via the Elasticsearch API.\nFor more information, please refer to User Management and Security.\nConfiguring Enterprise Search The Enterprise Search configuration is stored in config/enterprise-search.yml. You can modify this file using the Default Enterprise Search configuration as a reference.\nYou can also specify the options you want to override by setting environment variables inside the Compose file:\nenterprise-search: environment: ent_search.auth.source: standard worker.threads: '6' Any change to the Enterprise Search configuration requires a restart of the Enterprise Search container:\n$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml restart enterprise-search Please refer to the following documentation page for more details about how to configure Enterprise Search inside a Docker container: Running Enterprise Search Using Docker.\nSee also Enterprise Search documentation\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/enterprise-search/readme/"},{"content":"Filebeat Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.\nUsage To include Filebeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the filebeat-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up Configuring Filebeat The Filebeat configuration is stored in config/filebeat.yml. You can modify this file with the help of the Configuration reference.\nAny change to the Filebeat configuration requires a restart of the Filebeat container:\n$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml restart filebeat Please refer to the following documentation page for more details about how to configure Filebeat inside a Docker container: Run Filebeat on Docker.\nSee also Filebeat documentation\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/filebeat/readme/"},{"content":"Logspout extension Logspout collects all Docker logs using the Docker logs API, and forwards them to Logstash without any additional configuration.\nUsage If you want to include the Logspout extension, run Docker Compose from the root of the repository with an additional command line argument referencing the logspout-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/logspout/logspout-compose.yml up In your Logstash pipeline configuration, enable the udp input and set the input codec to json:\ninput { udp { port =\u003e 5000 codec =\u003e json } } Documentation https://github.com/looplab/logspout-logstash\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/logspout/readme/"},{"content":"Metricbeat Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server. Metricbeat takes the metrics and statistics that it collects and ships them to the output that you specify, such as Elasticsearch or Logstash.\nUsage To include Metricbeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the metricbeat-compose.yml file:\n$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml up Configuring Metricbeat The Metricbeat configuration is stored in config/metricbeat.yml. You can modify this file with the help of the Configuration reference.\nAny change to the Metricbeat configuration requires a restart of the Metricbeat container:\n$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml restart metricbeat Please refer to the following documentation page for more details about how to configure Metricbeat inside a Docker container: Run Metricbeat on Docker.\nSee also Metricbeat documentation\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/metricbeat/readme/"},{"content":"Extensions Third-party extensions that enable extra integrations with the Elastic stack.\n","description":"","title":"","uri":"/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/readme/"},{"content":"Initial I had the need to implement search functionality on my site. Content on is in different languages.\nThe goal is to impelemnt search for all pages and separate search results for each and every language.\nHow it works Hugo generates the search index. In this case it means that we get json file with every static page on the site.\nTo make search works we need to create index. lunr.js takes care of it.\nClient send query -\u003e our script “tries to find” in the index\nRender the results\nThis is how the logic looks like:\nImplementation Create search form Create popup modal where will render search results Connect Lunr.js script Generate pages data Connect search/result forms with lunr.js search TL;DR Files to change/create:\n1. `/layouts/partials/header.html` \u003cform id=\"search\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\"\u003e \u003c/form\u003e 2. `/layouts/partials/components/search-list-popup.html` \u003cdiv id=\"search-result\" tabindex=\"-1\" class=\"overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \" hidden\u003e \u003cdiv class=\"relative p-4 w-full max-w-xs h-full md:h-auto\"\u003e \u003cdiv class=\"relative bg-white rounded-lg shadow dark:bg-gray-700\"\u003e \u003cdiv class=\"p-6\"\u003e \u003ch3\u003eSearch results\u003c/h3\u003e \u003cdiv id=\"search-results\" class=\"prose\"\u003e\u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 3. `/layouts/partials/footer.html` ... {{ $languageMode := .Site.Language }} \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/js/search.js?1\" languageMode={{ $languageMode }} \u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... 4. `/layouts/_default/index.json` [ {{- range $index, $page := .Site.RegularPages.ByTitle -}} {{- if gt $index 0 -}} , {{- end -}} {{- $entry := dict \"uri\" $page.RelPermalink \"title\" $page.Title -}} {{- $entry = merge $entry (dict \"description\" .Description) -}} {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}} {{- $entry | jsonify -}} {{- end -}} ] 5. `config.yaml` # config.yaml # need for search popup service / creates search.json index fo lunr.js outputFormats: SearchIndex: baseName: search mediaType: application/json outputs: home: - HTML - RSS - SearchIndex 6. `static/js/search.js` const languageMode = window.document.currentScript.getAttribute('languageMode'); const MAX_SEARCH_RESULTS = 10 let searchIndex = {} let pagesStore = {} // Need to create ONLY once , maybe before push | during build const createIndex = (documents) =\u003e { searchIndex = lunr(function () { this.field(\"title\"); this.field(\"content\"); this.field(\"description\"); this.field(\"uri\"); this.ref('uri') documents.forEach(function (doc) { pagesStore[doc['uri']] = doc['title'] this.add(doc) }, this) }) } const loadIndexData = () =\u003e { const url = `/${languageMode}/search.json`; var xmlhttp = new XMLHttpRequest(); xmlhttp.onreadystatechange = function () { if (this.readyState == 4 \u0026\u0026 this.status == 200) { const pages_content = JSON.parse(this.responseText); createIndex(pages_content) } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); } const search = (text) =\u003e { let result = searchIndex.search(text) return result } const hideSearchResults = (event, divBlock) =\u003e { event.preventDefault() if (!divBlock.contains(event.target)) { divBlock.style.display = 'none'; divBlock.setAttribute('class', 'hidden') } } // TODO refactor const renderSearchResults = (results) =\u003e { const searchResultsViewBlock = document.getElementById('search-result') // hide on move mouse from results block document.addEventListener('mouseup', (e) =\u003e hideSearchResults(e, searchResultsViewBlock)); const searchResultsDiv = document.getElementById('search-results') searchResultsDiv.innerHTML = '' searchResultsViewBlock.style.display = 'initial'; searchResultsViewBlock.removeAttribute('hidden') const resultsBlock = document.createElement('ul') for (let post of results) { const url = post['ref'] const title = pagesStore[url] let commentBlock = document.createElement('li') let link = document.createElement('a',) let linkText = document.createTextNode(title); link.appendChild(linkText) link.href = url commentBlock.appendChild(link) resultsBlock.appendChild(commentBlock) } searchResultsDiv.appendChild(resultsBlock) } const searchFormObserver = () =\u003e { var form = document.getElementById(\"search\"); var input = document.getElementById(\"search-input\"); form.addEventListener(\"submit\", function (event) { event.preventDefault(); var term = input.value.trim(); if (!term) { return } const search_results = search(term, languageMode); renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) }, false); } // create indexes loadIndexData() searchFormObserver() Search form I am going to add search form to the header part. For thios purpose edit header.html file in the path /layouts/partials/header.html\nSet form id: search. By this id script can find this form\nMinimal form for work:\n\u003cform id=\"search\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\"\u003e \u003c/form\u003e I use Tailwind, so this is how my form looks like:\n\u003cdiv class=\"relative pt-4 md:pt-0\"\u003e \u003cform id=\"search\" class=\"flex items-center\"\u003e \u003clabel for=\"search-input\" class=\"sr-only\"\u003eSearch\u003c/label\u003e \u003cdiv class=\"relative w-full\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\" class=\"bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full pl-10 p-2.5 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500\" placeholder=\"Search\" required\u003e \u003c/div\u003e \u003c/form\u003e \u003c/div\u003e Modal with results By default this modal window is hidden. So don’t need to add this to any page. But need to add somewhere.\n1. Create .html component\npath: /layouts/partials/components/search-list-popup.html\nFor modal block to show or hide I use id: search-result\nFor block with search results id is: search-results\nContent:\n\u003cdiv id=\"search-result\" tabindex=\"-1\" class=\"overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \" hidden\u003e \u003cdiv class=\"relative p-4 w-full max-w-xs h-full md:h-auto\"\u003e \u003cdiv class=\"relative bg-white rounded-lg shadow dark:bg-gray-700\"\u003e \u003cdiv class=\"p-6\"\u003e \u003ch3\u003eSearch results\u003c/h3\u003e \u003cdiv id=\"search-results\" class=\"prose\"\u003e\u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 2. Add component to the site\nAdd this component to the footer. File path: /layouts/partials/footer.html\n... {{ partial \"components/search-list-popup.html\" . }} ... Connect Lunr.js Add link to this script to the footer template too\nPart of the footer template:\n... \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... Generate pages data Hugo can generate the search index the same way it generates RSS feeds for example, it’s just another output format.\n1. Generate script\nThis generator is for multilingual site\nCreates json in each language catalog in format:\n[{\"title\":\"title01\",...}] Fepends on fileds inckluded in the layout /layouts/_default/index.json\nCreate file /layouts/_default/index.json\n[ {{- range $index, $page := .Site.RegularPages.ByTitle -}} {{- if $page.IsTranslated -}} {{ if gt (index $page.Translations 0).WordCount 0 }} {{ range .Translations }} {{- if gt $translatedCount 0 -}} , {{- end -}} {{- $entry := dict \"uri\" .RelPermalink \"title\" .Title -}} {{- $entry = merge $entry (dict \"description\" .Description) -}} {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}} {{- $entry | jsonify -}} {{ $translatedCount = add $translatedCount 1 }} {{ end}} {{ end }} {{- end -}} {{- end -}} ] Creates search.json file with page indexes in /public/search.json\n2. Set index file path\nUpdate config.yaml file:\n# config.yaml # need for search popup service / creates search.json index fo lunr.js outputFormats: SearchIndex: baseName: search mediaType: application/json outputs: home: - HTML - RSS - SearchIndex Connect search/result forms with lunr.js search Create file in the path: static/js/search.js\nconst languageMode = window.document.currentScript.getAttribute('languageMode'); const MAX_SEARCH_RESULTS = 10 let searchIndex = {} let pagesStore = {} // Need to create ONLY once , maybe before push | during build const createIndex = (documents) =\u003e { searchIndex = lunr(function () { this.field(\"title\"); this.field(\"content\"); this.field(\"description\"); this.field(\"uri\"); this.ref('uri') documents.forEach(function (doc) { pagesStore[doc['uri']] = doc['title'] this.add(doc) }, this) }) } const loadIndexData = () =\u003e { const url = `/${languageMode}/search.json`; var xmlhttp = new XMLHttpRequest(); xmlhttp.onreadystatechange = function () { if (this.readyState == 4 \u0026\u0026 this.status == 200) { const pages_content = JSON.parse(this.responseText); createIndex(pages_content) } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); } const search = (text) =\u003e { let result = searchIndex.search(text) return result } const hideSearchResults = (event, divBlock) =\u003e { event.preventDefault() if (!divBlock.contains(event.target)) { divBlock.style.display = 'none'; divBlock.setAttribute('class', 'hidden') } } // TODO refactor const renderSearchResults = (results) =\u003e { const searchResultsViewBlock = document.getElementById('search-result') // hide on move mouse from results block document.addEventListener('mouseup', (e) =\u003e hideSearchResults(e, searchResultsViewBlock)); const searchResultsDiv = document.getElementById('search-results') searchResultsDiv.innerHTML = '' searchResultsViewBlock.style.display = 'initial'; searchResultsViewBlock.removeAttribute('hidden') const resultsBlock = document.createElement('ul') for (let post of results) { const url = post['ref'] const title = pagesStore[url] let commentBlock = document.createElement('li') let link = document.createElement('a',) let linkText = document.createTextNode(title); link.appendChild(linkText) link.href = url commentBlock.appendChild(link) resultsBlock.appendChild(commentBlock) } searchResultsDiv.appendChild(resultsBlock) } const searchFormObserver = () =\u003e { var form = document.getElementById(\"search\"); var input = document.getElementById(\"search-input\"); form.addEventListener(\"submit\", function (event) { event.preventDefault(); var term = input.value.trim(); if (!term) { return } const search_results = search(term, languageMode); renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) }, false); } // create indexes loadIndexData() searchFormObserver() Next need to add this file to the site: /layouts/partials/footer.html\nNow footer looks like this:\n... {{ $languageMode := .Site.Language }} \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/js/search.js?1\" languageMode={{ $languageMode }} \u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... ","description":"Make your multilingual Hugo static site searchable with a client-side search index","title":"Add search to Hugo multilingual static site with Lunr","uri":"/en/posts/hugo-add-search-lunr-popup/"},{"content":"About Documentation User Guide API Gateway provides the opportunity to create and expand your own REST and WebSocket APIs at any size.\nAPI endpoints can be cached to accommodate for frequent similar requests.\nUse Cases Build a network for micros­ervices archit­ectures.\nAmazon CloudWatch metrics - Collects near-real-time metrics Examples: 4XXError (client-side errors), 5XXError(server-side errors), CacheHitCount Amazon CloudWatch Logs - Debug issues related to request execution AWS CloudTrail - Record of actions taken by a user, role, or an AWS service in API Gateway AWS X-Ray - Trace your request across different AWS Services Digests Concepts REST API, HTTP API, WebSocket API\nDeployment - point-in-time snapshot of your API Gateway API\nEndpoint - https://api-id.execute-api.region-id.amazonaws.com\nEdge-optimized Private Regional Stage - A logical reference to a lifecycle state of your API. Route - URL path, Latency based routing, Integration - Lambda, HTTP, Private VPC, CORS Import/Export - Open API AM User should have permission to enable logging Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale.\nStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. The act like environment variables and can be used in your API setup and mapping templates.\nWith deployment stages in API Gateway you can manage multiple release stages for each API, such as: alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\nWhen you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through our API, you must manually add these permissions.\nIntegration timeout for AWS, Lambda, Lambda proxy, HTTP, HTTP proxy - 50 ms to 29 seconds\nYou can enable API caching to cache your endpoint’s responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API\nAWS Gateway Integration types:\nAWS_ Proxy - lambda proxy integration HTTP - http custom integration HTTP_PROXY - http proxy Practice Creating a RESTful API Using Amazon API Gateway Questions Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\nA) HTTP proxy B) mock C) AWS service proxy D) Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\nAmazon API Gateway supports mock integrations for API methods.\nB\nQ2 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ3 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\nWhich AWS service should be used to refactor this architecture?\nAWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4\nQ4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\nEnable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template\n4\nQ5 A developer is designing a full-stack serverless application. Files for the website are stored in an Amazon S3 bucket. AWS Lambda functions that use Amazon API Gateway endpoints return results from an Amazon DynamoDB table. The developer must create a solution that securely provides registration and authentication for the application while minimizing the amount of configuration.\nWhich solution meets these requirements?\nCreate an Amazon Cognito user pool and an app client. Configure the app client to use the user pool and provide the hosted web UI provided for sign-up and sign-in. Configure an Amazon Cognito identity pool. Map the users with IAM roles that are configured to access the S3 bucket that stores the website. Configure and launch an Amazon EC2 instance to set up an identity provider with an Amazon Cognito user pool. Configure the user pool to provide the hosted web UI for sign-up and sign-in. Create an IAM policy that allows access to the website that is stored in the S3 bucket. Attach the policy to an IAM group. Add IAM users to the group. Explanation 2\nQ6 A company has moved a legacy on-premises application to AWS by performing a lift and shift. The application exposes a REST API that can be used to retrieve billing information. The application is running on a single Amazon EC2 instance. The application code cannot support concurrent invocations. Many clients access the API, and the company adds new clients all the time.\nA developer is concerned that the application might become overwhelmed by too many requests. The developer needs to limit the number of requests to the API for all current and future clients. The developer must not change the API, the application, or the client code.\nWhat should the developer do to meet these requirements?\nPlace the API behind an Amazon API Gateway API. Set the server-side throttling limits. Place the API behind a Network Load Balancer. Set the target group throttling limits. Place the API behind an Application Load Balancer. Set the target group throttling limits. Place the API behind an Amazon API Gateway API. Set the per-client throttling limits. Explanation 4\n","description":"Create, maintain, and secure APIs at any scale with Amazon API Gateway","title":"API Gateway","uri":"/en/docs/aws-certified-developer-associate/api-gateway/"},{"content":"Lab Automating Code Reviews with Amazon CodeGuru Associating Amazon CodeGuru with a CodeCommit Repository 1. Navigate to the Amazon CodeCommit console.\n2. Click java-web-app:\n3. Notice that at the moment, only a README file has been committed to the master branch. Next, you’ll associate CodeGuru with this repository, so that CodeGuru can begin to analyze the code therein.\n4. Go to the CodeGuru dashboard.\n5. Click Associate Repository and run analysis:\n6. Select AWS CodeCommit as the provider, choose java-web-app from the repository dropdown, enter _master _into Source branch and click Associate:\nIn roughly one minute, you’ll see that CodeGuru has associated with your repository:\nTriggering an Amazon CodeGuru Review 1. Navigate to :8080 in your browser. Note: This is the IP of an EC2 instance that can be found in the EC2 console.\n2. Click the file icon in the top left to open the file tree:\nNote: During the creation of this lab, two things were performed automatically. One is that the CodeCommit repository you visited earlier was cloned to the directory you’re looking at in the IDE now. Another is that the framework for a Java web app was added in addition to the single README you saw. This is so that you can see the benefits of CodeGuru without having to work heavily with code.\nIn this lab step, you’ll push all the new code to the nearly-empty Code Commit repository, to trigger a CodeGuru review.\n3. Open the terminal in your IDE:\n4. In the terminal, add the new files to a Git branch, and commit and push the changes:\ncd /cloudacademy/lab git add . git checkout -b trigger_branch git commit -m \"trigger a CodeGuru analysis by pushing Java code\" git push origin trigger_branch This will create a Git commit that includes all the Java files in a branch called trigger_branch, so that you can make a pull request in CodeCommit. Since CodeGuru analyses are triggered by pull requests, this is what will trigger a CodeGuru analysis.\n5. Back on the CodeCommit dashboard, click Create pull request:\n6. Set the Destination to master and the source to trigger_branch and click Compare:\n7. Type Trigger a CodeGuru Reviewer Analysis into the Title field and click Create pull request:\nThis will create a pull request and trigger a CodeGuru review.\nViewing Amazon CodeGuru Comments 1. If you weren’t automatically brought to the pull request details page after creating your pull request, click Pull Requests beneath Repositories on the left side of the page:\n2. Click the only available pull request:\n3. Notice the section mentioning CodeGuru Reviewer:\nThis section will display in each pull request made in any repository associated with CodeGuru. As of the time this lab was released, CodeGuru is still in preview. As the section on your pull request details tab mentions, because it’s in preview mode, CodeGuru can take a while to process a pull request. There isn’t a way to track its progress, and you currently won’t be alerted when that processing begins or finishes.\n4. Select the Changes tab:\n5. In the Go to file filter, enter dockerservlet and click the result to navigate to the file:\nYou may need to scroll down the page to find the DockerServlet.java file changes. This file is known to have CodeGuru Reviewer comments that usually appear a few minutes after creating the pull request.\n6. Scroll down to line 60 to see an example of a comment from CodeGuru Reviewer (If you don’t see any comment you may try refreshing the page every minute until one appears):\nYou can then make updates as you see fit, and submit more pull requests to see if you’ve addressed CodeGuru’s suggestions.\n","description":"","title":"Automating Code Reviews with Amazon CodeGuru","uri":"/en/docs/aws-certified-developer-associate/codeguru/automating-code-reviews-amazon-codeguru/"},{"content":"Lab Monitor Like a DevOps Pro: Build A Log Aggregation System in AWS Navigating to Your Cloud’s Lambda Function 1. In the AWS Management Console search bar, enter Cloud Formation, and click the CloudFormation result under Services:\nThis will bring you to the CloudFormation Stacks table.\nThere will be one stack named cloudacademylabs in the table with a Status of CREATE_COMPLETE.\nNote: If the stack hasn’t reached the **Status **of CREATE_COMPLETE, try refreshing the page after a minute. It only takes a minute for the stack to fully create.\n2. To view details of the stack, under Stack name, click the cloudacademylabs link.\n3. Click the Resources tab:\nYour Physical IDs will be different than in the supplied image. Note in the **Type **column that a DynamoDB Table, a Lambda Function, and **IAM **resources to grant the Lambda access to the DynamoDB Table have all been created. You will be querying the DynamoDB table via Lambda function invocations to create CloudWatch Logs, that will be aggregated and searchable via a user interface (UI).\n4. Click on the Outputs tab, and open the DynamoLambdaConsoleLink link in the Value column:\nThis takes you to the Lambda function Console.\nCreating Some Logs Using AWS Lambda 1. Briefly look around the Lambda function console:\nThe **Designer **gives a visual representation of the AWS resources that trigger the function (there are none in this case), and the AWS resources the function has access to (CloudWatch Logs, and DynamoDB). The actual code that is executed by the function is farther down in the Function **code **section. You don’t need to worry about the actual implementation details of the function for this Lab.\n2. To configure a test event to trigger the function, scroll down to the Code source section and click Test:\n3. In the Configure test event form, enter the following values before scrolling to the bottom and click Save:\nEvent name:TestPutEvent Enter the following in the code editor at the bottom of the form: Copy code\n{ \"fn\": \"PUT\", \"data\": { \"id\": \"12345\", \"name\": \"foobar\" } } The PUT object event will update the DynamoDB database with an object with the given id.\n4. To run your function with your test event, click Test again:\nAfter a few seconds, in the code editor, a tab called Execution results will load:\nThe function succeeded and the Function Logsarea displays the logs that were generated and automatically sent to CloudWatch Logs by AWS Lambda.\n5. To view the Amazon CloudWatch logs, click the Monitor tab, and then click View logs in CloudWatch:\nNote: The Lab’s CloudFormation stack outputs also include a link to the Log Group if you need to access it at a later time.\nManually Viewing Logs in Amazon CloudWatch 1. Observe the Log Streams in the CloudWatch log group for the Lambda function you invoked:\nThe rows in the table are different Log Streams for the log group.\nEach log stream corresponds to log events from the same source. AWS Lambda creates a new log event for every Lambda invocation. However, it is possible to have multiple log streams for a single Lambda function since the log stream corresponds to the container actually running the function.\nBehind the scenes, a Lambda is run in a container. After a period of inactivity, the container is unloaded and the following requests will be served by a new container, thus creating a new log stream. Depending on how many times you invoked the test command in the previous step, you will see one or more rows in the log stream.\n2. Click on the latest Log Stream.\nThe log stream is a record of event Messages ordered in Time:\n3. Enter _PUT _into the **Filter events **search bar and click enter:\n4. Click the triangle to expand the event that matches the filter.\nYou will see the JSON formatted message:\nThe outermost data attribute wraps the test event you configured.\n5. Click custom to display the custom time range filter available in CloudWatch Logs:\nObserve the time-based options in the dialog box that displays:\nThe filter by text and by time capabilities are the tools that are available for sifting through logs in CloudWatch Logs. The text filters support some forms of conditions that can be expressed through a syntax specific to CloudWatch. These capabilities are handy, but you will see that there are more powerful tools available for log aggregation and retrieval.\nLaunching the OpenSearch Domain The first thing you need is an Amazon OpenSearch cluster/domain. Using the Amazon OpenSearch Service has the following benefits:\nIt’s distributed and resilient It supports aggregations It supports free-text search It’s managed and takes care of most of the operational complexities of operating a cluster In 2021 AWS renamed Amazon ElasticSearch Service to Amazon OpenSearch Service. You may see references to ElasticSearch in the Amazon Management Console. You should assume that ElasticSearch and OpenSearch refer to the same AWS service.\nThe following diagram illustrates the overall design of the AWS Lab environment and the part that you are building in this lab step is highlighted in the lower-left corner in the AWS cloud:\n1. In the search bar at the top, enter OpenSearch, and under Services, click the Amazon OpenSearch Service result:\n2. To begin creating your cluster, on the right-hand side of the welcome page, click Create domain:\nThe terms OpenSearch domain and an OpenSearch cluster can be used almost interchangeably. The former is the logical search resource, and the latter is the actual servers that are launched to create a domain.\nThe Create domain form will load. 3. In the Name section, in the Domain name textbox, enter ca-labs-domain-###, replacing ### with some random numbers:\n4. In the Deployment type section, select the following:\nDeployment type: Select Development and testing Version: Select **6.8 **under ElasticSearch In this short-lived lab, you are using a Development and testing deployment because it allows public access and reliability isn’t a concern. In a production environment, you will want to use a Production deployment to get the full availability benefits and meet security requirements.\n5. In the Auto-Tune section, select Disable.\nIn this short-lived lab, Auto-Tune is not necessary.\n6. In the Data nodes section, enter and select the following and leave remaining defaults:\nInstance type: Select t3.small.search Number of nodes: Enter 1 The storage type values correspond to the storage types available for Amazon EC2 instances.\nWhen deploying a cluster that uses multiple nodes, you can specify that the nodes are deployed in two or three availability zones. Deploying in multiple availability zones makes the cluster highly available and more reliable in the case of failures of outages.\n7. Scroll down to the Network section, and select Public access:\nIn this lab, you are creating a publicly available Amazon OpenSearch Service cluster for convenience. Be aware that you can also deploy a cluster into an Amazon Virtual Private Cloud (VPC) and receive the network isolation and security advantages of using a VPC.\n8. In the **Fine-grained access control **section, uncheck the **Enable fine-grained access control **box.\n9. In a new browser tab, enter the following URL:\nhttps://checkip.amazonaws.com/\nYou will see an IP address displayed. This is the public IPv4 address of your internet connection. You will use this IP address to restrict access to your Amazon OpenSearch Service cluster.\n10. Scroll down to the Access Policy section and under Domain access policy, select Configure domain level access policy:\nYou will see a policy editor form display with the tabs Visual editor and JSON.\n11. In the Visual editor tab, enter and select the following:\nType: Select IPv4 address Principal: Enter the IP address you saw on the Check IP Page Action: Select Allow You are specifying an access policy that allows access to the cluster from your IP address. In a non-lab environment, you could deploy the cluster into an Amazon VPC and configure private or public access using a VPC’s networking features.\n12. To finish creating your cluster, scroll to the bottom and click Create:\nA page displaying details of your cluster will load and you will see a green notification that you have successfully created a cluster.\n13. In the General information section, observe the Domain status:\nAWS is setting up and deploying your cluster. This process can take up to 15 or 30 minutes to complete.\n12. To see the latest status of your Amazon OpenSearch Service cluster, refresh the page in your browser.\nRefresh the page for your domain periodically to check if it has finished deploying.\nWhilst waiting for the domain to finish provisioning, feel free to consult the Amazon OpenSearch Service Developer Guide to learn more about the OpenSearch service.\nWhen the cluster has been provisioned you will see the Domain status change to Active:\nSending CloudWatch Logs to OpenSearch 1. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n2. In the left-hand menu, under Logs, click on Log groups:\n3. Select the log group beginning with /aws/lambda/cloudacademylabs-DynamoLambda-:\nNext, you will create a subscription filter to send the log data to your ElasticSearch domain.\n4. Click Actions, in the menu that opens, under Subscription filters, click Create Amazon OpenSearch Service subscription filter:\nThe Create Amazon OpenSearch Service subscription filter form will load.\n5. In the Choose destination section, select the following:\nSelect account: Ensure **This account **is selected Amazon OpenSearch Service cluster: Select the cluster you created previously After selecting the Amazon OpenSearchService cluster, the Lambda function section will appear.\n6. In the Lambda IAM Execution Role drop-down select LambdaElasticSearch:\n7. In the Configure log format and filters section enter the following:\nLog Format: Select Amazon Lambda Subscription filter name: ca-lab-filter The default Subscription Filter Pattern matches the timestamp, request_id, and event JSON. The **Test Pattern **button is available to see which events match the pattern.\n8. To start sending the logs to ElasticSearch, at the bottom, click Start streaming:\nMomentarily, you will see a notification that the subscription filter has been created and logs are being streamed to OpenSearch:\nDiscovering and Searching Events 1. Navigate back to the Lambda function you invoked earlier and click the Test button a few times to submit more PUT events:\n2. Click the arrow on the Test button and click Configure test event:\n3. In the **Configure test events **form, click the radio button for Create new test event and enter the following non-default values:\nEvent name:TestGetEvent Enter the following in the code editor at the bottom of the form: { \"fn\": \"GET\", \"id\": \"12345\" } You will submit more test events of a different type - GET operations on the object that was PUT in the database. This gives two different event types to look at in Kibana (the Log Aggregation UI).\n4. Click Save.\n5. Click Test several times to make GET events.\n6. Return to the Amazon OpenSearch Search Console for the domain you created and click the link under Kibana URL:\n7. In the Add Data to Kibana section, on the right-hand side under Use Elasticsearch data, click Connect to your Elasticsearch index:\nThe log data is stored in OpenSearch, but you need to tell Kibana which index to use for discovering the data.\n8. In the **Create an index pattern **wizard, enter the following value and click Next step:\nIndex pattern: cwl-* The pattern will match the daily CloudWatch Logs (cwl) indices that are created in Amazon OpenSearch.\n9. In the second step, enter the following value and click Create index pattern:\nIndex pattern: Select @timestamp The Time filter field name allows Kibana to determine which attribute represents the timestamp of each event. The confirmation page displays all of the fields in the log data index:\nNow that the index settings for Kibana are configured, you can begin using the Log Aggregation system!\n10. Click Discover in the sidebar menu on the left of the page.\n11. Explore the Discover interface:\nYou see some events and a graph. These are your aggregated log events! The system is online! Notice the search bar up top. It is initially empty so all log events will show up. But what if you only want to see the PUT events for objects containing 12345?\n12. Enter PUT 12345 in the search bar and press enter:\nThe matching terms in the event show up highlighted, and the bar graph updates to show only the count of PUT 12345 events that you made by clicking Test in the Lambda interface.\n13. Click on the timestamp range in the upper-right corner to display the time filter:\nJust as with CloudWatch Logs, you can filter the logs by time. However, in Kibana you can also drag on the bar chart to select a time range visually: Visualizing Aggregated Events 1. Click Visualize in the Kibana sidebar menu.\n2. Click Create a visualization:\n3. Select **Area **chart visualization:\n4. In the **From a New Search, Select Index **area, click on the *cwl- **index name:\nIf you had any saved searches in the system, you could use them to make this Visualization from this step.\nOn the left-hand side, the visualization configuration tools will appear:\n5. Enter the following values in the visualization configuration:\nSelect buckets type: X-Axis Aggregation: Date Histogram (to track log trends over time) Field: @timestamp Interval: Auto To make the graph more interesting, you will split the PUTs and GETs and display each stacked in on the chart with different colors. This requires a sub-buckets.\n6. Click Add sub-buckets below the rest of the X-Axis settings, and enter the following values:\nSelect buckets type: Split Series Sub Aggregation: Terms (Terms splits the data based on the unique values of a field) Field: $event.data.fn.keyword (The test requests used the fn key for request type, which maps to the $event.data.fn.keyword field in OpenSearch) 7. Click the play button to apply the changes and produce the visualization:\nIt will look something like the image below, with two regions in an area graph corresponding to GET and PUT event count over time:\nTo use the visualization in a Dashboard in the next step, you need to save the visualization.\n8. Click Save in the top toolbar:\n9. Enter PUTs and GETs Over Time in the Save Visualization field, and click Save:\nCreating a Kibana Dashboard 1. Click on Dashboard in the sidebar menu.\n2. Click Create a dashboard:\n3. Click Add to add saved visualizations to the dashboard:\n4. Select the PUT and GETs Over Time visualization:\nThe visualization is added to the dashboard, but the size may not be what you like. You can adjust the size of the visualization by dragging the arrow in the lower-right corner:\n5. Click Save and enter the following values before clicking the revealed **Save **button:\nTitle: Log Dashboard Description: Lambda API Logs You’ve done it! The Dashboard will always contain the up-to-date statistics for your GET and PUT events that run through the Lambda function:\n6. Return to the Lambda console and create as many test events as you want.\n7. Refresh the Kibana dashboard and see the new requests in the visualization:\nYou can also configure Auto-refresh to avoid having to refresh the view:\n","description":"Will create a distributed, scalable log aggregation system within AWS running on Amazon OpenSearch Service. This Log Aggregation System will ingest as much of your CloudWatch log stream events as you want, events generated from AWS EC2 Instances, Lambda functions, Databases, and anything else you want to submit log events from.","title":"Build A Log Aggregation System in AWS","uri":"/en/docs/aws-certified-developer-associate/opensearch-service/build-log-aggregation-system/"},{"content":"Changing the Metadata of an Amazon S3 Object Introduction Each object in Amazon S3 has a set of key/value pairs representing its metadata. There are two types of metadata: “System metadata” (for example, Content-Type and Content-Length) and custom “User metadata”. User metadata is stored with the object and returned with it.\nAs an example, you might have your own taxonomy for various images, such as “logo”, “screenshot”, “diagram”, “flowchart” and so on. In this lab step, you will change the Content-Type of your image to “text/plain”. You will also create custom user metadata.\nNote: With the new Amazon S3 UI you can set the metadata as part of the upload process, or add it later.\nInstructions Return to the cloudfolder/ and delete the cloudacademy-logo.png from your Amazon S3 bucket by checking the checkbox and clicking Delete: The Delete objects form page will load. Because a deleted object is not retrievable, AWS asks you to confirm that you want to delete the object before deletion.\nIn the textbox at the bottom of the page, enter permanently delete and click Delete objects:\nTo return to the bucket object view, at the top-right, click Close.\nClick Upload, then Add files and browse to the logo file (or drag-and-drop it into the Upload wizard) in order to upload it again.\nNear the bottom of the page, expand the Properties section:\nScroll down to the Metadata section and click Add metadata:\nA row of form elements will appear.\nEnter the following: Type: Select System defined Key: Select Content-Type Value: Enter text/plain The drop-down list contains the System metadata that you can set.\nIn this lab, you have set the content type to text/plain as an example to see how to add metadata to an object when uploading to Amazon S3.\nNext you will add custom user metadata. User metadata must be prefaced with “x-amz-meta-”. The remaining instructions will add a custom user type for imagetype, and imagestatus.\nClick Add metadata again to add another row. Enter the following to define custom metadata:\nType: Select User defined Key: Enter imagetype after x-amz-meta Value: Enter logo You have added two metadata key-value pairs to the object you are going to upload. One system metadata and one user-defined.\nAt the bottom of the page, click Upload:\nTo exit the upload form, at the top-right, click Close.\nIn the Objects table click the cloudacademy-logo.png object:\nScroll down to the Metadata section and observe the key-value pairs you added:\nThis is also where you can add, remove, and edit metadata after you have uploaded objects to Amazon S3.\n","description":"Changing the Metadata of an Amazon S3 Object","title":"Change metadata of S3 Object","uri":"/en/docs/aws-certified-developer-associate/s3/how-to-change-metadata-s3/"},{"content":"About AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.\nDocumentation User Guide CodeAr­tifact is a secure storage, publishing, and sharing of software code packages used in a development process organisation’s software development. CodeAr­tifact makes it easy for small organisations to store, publish, and share software packages.\nCodeArtifact can be configured to automatically fetch software packages and dependencies from public artifact repositories.\nCodeArtifact works with commonly used package managers and build tools like Maven, Gradle, npm, yarn, twine, pip, and NuGet making it easy to integrate into existing development workflows.\nPrice Pay only for what you use – the size of the artifacts stored, the number of requests made, and the amount of data transferred out of an AWS Region. CodeArtifact includes a monthly free tier for storage and requests\nCurrent price\nUse Cases Type: Developer Tools\nAlternatives JFrog Artifactory Docker hub Sonatype Nexus Platform Helm Azure DevOps Services Github Usage $ aws codeartifact list-domains --region us-east-1 Practice Getting started using the console\n","description":"Amazon CodeArtifact","title":"CodeArtifact","uri":"/en/docs/aws-certified-developer-associate/codeartifact/"},{"content":"About CodeBuild is a fully managed service that assembles source code, runs unit tests, \u0026 also generates artefacts ready to deploy.\nDocumentation User Guide CodeBuild is a code creation service that also produces code artefacts upon request.\nCodeBuild is an alternative to other build tools such as Jenkins.\nCodeBuild is integrated with KMS for encryption of build artifacts, IAM for build permissions, VPC for network security, and CloudTrail for logging API calls.\nCodeBuild is a fully managed build service to compile source code, run unit tests and produce artifacts that are ready for deployment. Not the best fit for serverless template deployment or serverless application initialization.\nbuildspec.yml Build instructions can be defined in the code (buildspec.yml).\nCodeBuild Local Build In case you need to do deep troubleshooting beyond analyzing log files.\nCan run CodeBuild locally on your computer using Docker.\nLeverages the CodeBuild agent.\nPrice Current price\nYou pay based on the time it takes to complete the builds.\nLab cicd-aws-code-services Chapters:\nLogging in to the Amazon Web Services Console Creating an AWS CodeCommit Repository Committing Code to Your AWS CodeCommit Repository Building and Testing with AWS CodeBuild Deploying with AWS CodeDeploy Automating Your Deployment with AWS CodePipeline Following the Continuous Deployment Pipeline Recovering Automatically from a Failed Deployment Using AWS CodeDeploy Blue/Green Deployments in Your Pipeline Questions Q1 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\nWhat is the easiest way to develop these simple applications?\nInstall AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model\nhttps://aws.amazon.com/serverless/sam/\n1\n","description":"Amazon CodeBuild - Build and test code with continuous scaling. Pay only for the build time you use.","title":"CodeBuild","uri":"/en/docs/aws-certified-developer-associate/codebuild/"},{"content":"About CodeDeploy is a fully managed deployment service that automates software deploy­ments to a variety of compute services such as EC2, Fargate, Lambda, \u0026 on-pre­mises servers\nDocumentation User Guide CodeDeploy can also deploy a serverless Lambda function.\nCodeDeploy can be connected to CodePipeline and use artifacts from there.\nPlatforms Need to choose the compute platform:\nEC2/On-premises. AWS Lambda. Amazon ECS. AppSpec File The application specification file (AppSpec file) is a YAML-formatted, or JSON-formatted file used by CodeDeploy to manage a deployment.\nThe AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.\nDeployment types In-place deployment (EC2 only)\nBlue/green deployments:\nAWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.\nAmazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.\nEC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.\nPrice Current price\nUse Cases Type: Developer Tools\nPractice Continuous Integration and Deployment with AWS Code Services Questions Q1 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\nCan delete only if unused.\n1\nQ2 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n3\n","description":"Amazon CodeDeploy - Automate code deployments to maintain application uptime","title":"CodeDeploy","uri":"/en/docs/aws-certified-developer-associate/codedeploy/"},{"content":"About AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\nDocumentation User Guide CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.\nYou can easily integrate AWS CodePipeline with third-party services such as GitHub or with your own custom plugin. With AWS CodePipeline, you only pay for what you use.\nAlternatives Bamboo. CircleCI. Jenkins. Travis CI. GitLab. TeamCity. Azure DevOps Server. Google Cloud Build. Terminology Pipelines\nA workflow that describes how software changes go through the release process.\nArtifacts\nFiles or changes that will be worked on by the actions and stages in the pipeline. Each pipeline stage can create “artifacts”. Artifacts are passed, stored in Amazon S3, and then passed on to the next stage. Stages\nPipelines are broken up into stages, e.g., build stage, deployment stage. Each stage can have sequential actions and or parallel actions. Stage examples would be build, test, deploy, load test etc. Manual approval can be defined at any stage. Actions\nStages contain at least one action, these actions take some action on artifacts and will have artifacts as either an input, and output, or both.\nTransitions\nThe progressing from one stage to another inside of a pipeline.\nPrice Current price\nQuestions Q1 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\n","description":"Amazon CodePipeline Automate continuous delivery pipelines for fast and reliable updates","title":"CodePipeline","uri":"/en/docs/aws-certified-developer-associate/codepipeline/"},{"content":"About Amazon Cognito - Simple and Secure User Sign-Up, Sign-In, and Access Control\nDocumentation User Guide Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.\nUsers can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.\nAlternatives Auth0 Microsoft Azure Active Directory OneLogin Google Cloud Identity Platform IBM Security Verify Keycloak Terminology Credentials: The temporary security credentials, which include an access key ID, a secret access key, and a security token.\nAssumedRoleUser: The ARN and the assumed role ID, which are identifiers for the temporary security credentials that you can programatically refer to.\nPrice Pay only for what you use. First 50,000 (monthly active users (MAUs) - Free.\nCurrent price\nUse Cases Type: Identity \u0026 access management\nSame type services: Identity \u0026 Access Management (IAM), Single Sign-On, Cognito, Directory Service, Resource Access Manager, Organisations\nWorkflow The process of authenticating a user with Cognito is as follows:\nThe user signs in with a Web ID provider (Google, Facebook, Amazon, etc.) The Web ID provider returns a JWT token to the user The user application makes an STS API call: sts assume-role-with-web-identity STS returns an API response with the temporary credentials The user application now has AWS access e.g. for S3, DynamoDB, etc. Practice Manage Authentication with Amazon Cognito\nQuestions Q1 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\nWhat action will best solve this issue?\nUse AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n3\nQ2 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\n","description":"Amazon Cognito","title":"Cognito","uri":"/en/docs/aws-certified-developer-associate/cognito/"},{"content":"","description":"Monitors date expiration of access and id tokens provided by Amazon Cognito. Refreshes when expired.","title":"cognito-token-observer","uri":"/en/apps/npm/cognito-token-observer/"},{"content":"Practice Lab link Creating an Amazon S3 Bucket for a Static Website 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\nYou will be placed in the Amazon S3 console.\n2. To start creating a new Amazon S3 bucket, in the top-right, click Create bucket:\nThe Amazon S3 bucket creation form will load.\n3. Under General configuration, enter the following:\nBucket name: Enter _calabs-bucket-\u003cUniqueNumber\u003e _(Append a unique number to the end of calabs-bucket-) Region: Ensure US West (Oregon) us-west-2 is selected You have added a unique number to the bucket name because Amazon S3 bucket names must be unique regardless of the AWS region in which the bucket is created.\nA bucket name must also be DNS compliant. Here are some of the rules it must adhere to:\nThey must be at least 3 and no more than 63 characters long. They may contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. They cannot be formatted as an IP address (for example, 192.168.1.1). The following are examples of valid bucket names:\ncalabs-bucket-1 cloudacademybucket cloudacademy.bucket calabs.1 ca-labs-bucket Make a note of the name of your bucket, you will use it later.\n4. Make sure to select ACLs Enabled:\n5. In the Block Public Access section, un-check the Block all public access check-box:\n6. To acknowledge that you want to make this bucket publicly accessible, check I acknowledge that the current settings might result in this bucket and the objects within becoming public:\n7. To finish creating your Amazon S3 bucket, scroll to the bottom of the form and click Create bucket:\nNote: If you see an error because your bucket name is not unique, append a different unique number to the bucket name. For example, change “calabs-bucket” to “calabs-bucket-1” (or a unique number/character string) and click Create bucket again.\nThe Buckets list page will load and you will see a notification that your bucket has been created:\nNext, you will enable static website hosting for your bucket.\n8. In the list, click the name of your bucket:\nYou will see an overview of your Amazon S3 bucket, and a row of tabs with Objects selected.\n9. In the row of tabs under Bucket overview, click Properties:\nThe Properties tab allows you to enable and disable various Amazon S3 bucket features, including:\nBucket Versioning: Old versions can be kept when objects are updated Default encryption: A bucket can be configured to encrypt all objects by default Server access logging: Web-server style access logs can be enabled Requester pays: When enabled, the entity downloading data from this bucket will pay data transfer costs incurred 10. Scroll to the bottom of the Properties page and in the Static website hosting section, on the right, click Edit:\nThe Edit static website hosting form will load.\n11. In the Static website hosting field, select Enable:\nThe form will expand to reveal more configuration options.\n12. Enter the following, leaving all other fields at their defaults:\nIndex document: Enter index.html Error document: Enter error/index.html 13. To finish enabling static website hosting, scroll to the bottom, and click Save changes:\nThe bucket overview page will load and you’ll see a notification that you have successfully enabled static website hosting:\nYour S3 bucket is ready to host content.\nNext, you will create a bucket policy that will apply to all objects uploaded to your bucket.\n14. In the row of tabs, click Permissions:\n15. Scroll down to the Bucket policy section, and on the right, click Edit.\nThe Edit bucket policy form will load.\nAmazon S3 bucket policies are defined in JavaScript Object Notation, commonly referred to as JSON.\n16. In the Policy editor, copy and paste the following and replace YOUR_BUCKET_NAME with the name of your S3 bucket:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } This policy will allow public access to all objects in your S3 bucket.\nThis is a permissive policy. In a non-lab environment, security concerns may require you to implement a more restrictive policy. To learn more about bucket policies, visit the AWS Policies and Permissions in Amazon S3 documentation.\n17. To save your bucket policy, at the bottom of the page, click Save changes.\nThe bucket overview page will load and you will see a notification that the policy has been edited.\nNext, you will download a basic website from a public GitHub repository and load it into your S3 bucket.\n18. To download a zip file containing a basic website, click here.\nThis is an example website provided by CloudAcademy that is suitable for static website hosting.\n19. Extract the zip to your local file system.\nExact instructions will vary depending on your operating system and browser. In most browsers, you can click the downloaded file and a file extraction utility will open.\n20. In the row of tabs, click Objects.\n21. To begin uploading the website to your Amazon S3 bucket, scroll down and click Upload:\nThe Upload form will load.\n22. In the Files and folders section, click Add files:\nA file picker will open.\n23. Using the file picker, select all files and folders from inside the zip file you downloaded and extracted earlier.\nIf your extraction utility extracted the files to a folder called static-website-example-master, ensure you upload all the files and folders inside it but not the static-website-example-master folder itself. To be able to access the website, the index.html file must be at the top-level of your Amazon S3 bucket.\nYou may find it easier to drag and drop the files and folders onto the Upload page instead of using the file picker.\nYou may also see a browser confirmation dialog asking you to confirm you want to upload the files.\nOnce selected, the files and folders from the zip file will appear in the Files and folders section.\n24. Scroll to the bottom of the page and click Upload.\nYou will see a blue notification displaying the progress of the upload, and when complete you will see a green Upload succeeded notification.\n25. To exit the Upload form, on the right, click Close.\nThe bucket overview page will load.\nYour Objects section should look similar to:\nNext, you will test that your website is accessible.\n26. To retrieve the endpoint for your bucket, click the Properties tab, scroll to the bottom, and click the copy icon next to the Bucket website endpoint:\n27. Paste the endpoint into the address bar of a new browser tab.\nYou will see a website load that looks like this:\nThis website is being served by your Amazon S3 bucket.\nCreating an Amazon CloudFront Distribution for the Static Website 1. In the AWS Management Console search bar, enter CloudFront, and click the CloudFront result under Services:\nThe Amazon CloudFront console will load.\n2. To start creating a distribution, click Create a CloudFront Distribution:\n3. Under Origin, in the Origin Domain text-box, enter the Amazon S3 static website hosting endpoint that you created earlier:\n4. Scroll down to the** Settings** settings, in the Default Root Object text-box, enter index.html:\nYou are setting this field because Amazon CloudFront doesn’t always transparently relay requests to the origin. If you did not set a default root object on the distribution you would see an AccessDenied error when you access the CloudFront distribution’s domain later in this lab step.\nTo learn more, see the How CloudFront Works if You Don’t Define a Root Object section of the AWS developer guide for Specifying a Default Root Object.\n5. Leave all other settings at their default values, scroll to the bottom, and click Create Distribution.\nThe CloudFront distribution list page will load and you will see your distribution listed.\nYou will see the Last Modified of your distribution is Deploying:\nIt can take up to 15 minutes to deploy a new Amazon CloudFront distribution. Once complete, the Status will change to Enabled.\nThere are two main types of origin that Amazon CloudFront supports, Amazon S3 buckets, and custom origins. A custom origin could be a website being served by an EC2 instance, or it could be a web server outside of AWS. To learn more while your CloudFront distribution is deploying, visit the AWS Using Amazon S3 Origins, MediaPackage Channels, and Custom Origins for Web Distributions page. Once your deployment is complete, continue with the instructions.\n6. To view details of your distribution, click the random alphanumeric ID:\nNote: Your ID will be different.\n7. Copy the value of the Distribution Domain Name field:\n8. Paste the domain name into the address bar of a new browser tab.\nYou will see the website that you uploaded to your Amazon S3 bucket display:\nYou are accessing the website through your Amazon CloudFront distribution.\nNote: The instructions below are optional. Perform them if there is enough time left in the lab.\n9. On the website, click through and visit the different pages a few times to generate traffic.\nIf you have a different web browser available, try accessing the site in the other browser.\n","description":"Configuring a Static Website With S3 And CloudFront","title":"Configuring a Static Website With S3 And CloudFront","uri":"/en/docs/aws-certified-developer-associate/cloudfront/configuring-static-website-s3-and-cloudfront/"},{"content":"Creating a Folder inside an Amazon S3 Bucket Introduction The AWS S3 console allows you to create folders for grouping objects. This can be a very helpful organizational tool. However, in Amazon S3, buckets and objects are the primary resources. A folder simply becomes a prefix for object key names that are virtually archived into it.\nInstructions Return to the Buckets menu by clicking here, and click on the calabs-bucket you created earlier. (Reminder: Your bucket name will differ slightly.)\nClick Create folder:\nIn the Folder name textbox, enter cloudfolder:\nScroll to the bottom and click Create folder:\nThe folder is created inside your S3 bucket:\n","description":"Create a Folder inside an Amazon S3 Bucket","title":"Create a folder inside S3 Bucket","uri":"/en/docs/aws-certified-developer-associate/s3/create-folder-s3/"},{"content":"Practice Creating Classic Load Balancer Planning the Classic Load Balancer When you connected to the AWS account provided in the former step, you had a few things that were already deployed. This is the current infrastructure that was already deployed for you:\nYou already have a VPC with some subnets and 2 EC2 instances running inside the VPC in different Availability Zones. Both instances are inside the same Security Group called , which is allowing HTTP access from port 80 to anywhere (0.0.0.0/0). Each EC2 instance is running the same web application. We want to configure an LB to create a central point of access to our application, and we also want to configure our architecture in a way that users can only access the application through the ELB.\nIn the end, we should have a solution similar to this one:\nTo do that we will have to create and configure a Classic Load Balancer, and properly configure the needed Security Groups to make sure that our application will work as expected.\nCreating a Classic Load Balancer and Registering EC2 Instances A Classic Load Balancer allows traffic to be balanced across many Amazon EC2 instances, it performs this balancing at the request and connection level.\n1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. In the left-hand menu, under Load Balancing, click Load Balancers:\n3. To start creating your classic load balancer, click Create Load Balancer:\nThree tiles will be displayed detailing the different types of load balancer supported by Amazon EC2.\n4. At the bottom of the page, click Classic Load Balancer:\n5. In the Classic Load Balancer tile, click Create:\nA multi-step wizard will open allowing you to configure and customize your load balancer.\n6. Under Basic Configuration, enter the following values:\nLoad Balancer name: Enter classic-elb Enable advanced VPC configuration: checked Be aware there are limitations on the name field, only the characters a-z, A-Z, 0-9 and hyphens are allowed.\nCreate LB Inside lets you select which VPC you want the load balancer to be created in, leave this at the default.\nThe Create an internal load balancer option determines whether the load balancer can accept public internet traffic or not. If checked, the load balancer will have a private IP address and will only be able to accept traffic from another source inside the VPC.\nThe default Listener Configuration, listening on port eighty (HTTP), is all that is required for this lab.\n7. Under Select Subnets, click the plus icon next to each subnet.\nAs you click for each subnet, it will move from the Available subnets table, to the Selected subnets table:\nAn Availability Zone, often referred to as an AZ, helps make your infrastructure more reliable. You can think of each zone as a separate data center (in many cases they are exactly that), they are guaranteed to have redundant power, networking, and connectivity within an AWS region.\nTo learn more about regions, availability zones, and redundancy in AWS, visit the documentation here.\nEach subnet is mapped to one availability zone. It’s important to configure the selected subnets correctly. If a subnet containing an EC2 instance is not selected, the load balancer will not be able to communicate with that EC2 instance. 8. To move to the next step of the wizard, click Next: Assign Security Groups:\n9. In the form, enter and select the following values:\nAssign a security group: Select Create a new security group Security group name: Enter elb-sg Description: Enter Security group for the classic load balancer You will see a default security group rule allowing traffic on port eighty.\n10. In the default security group rule, in the Source drop-down, select Anywhere:\n11. To advance to the next page of the wizard, click Next: Configure Security Settings:\nThis wizard step display’s a warning that your load balancer isn’t configured to use HTTPS or SSL.\nIt’s strongly recommended that you always enable encrypted traffic on your load balancers for security reasons. Configuring SSL is beyond the scope of this lab. If you would like to learn more about SSL and load balancing, it’s covered in the Using Elastic Load Balancing \u0026 EC2 Auto Scaling to Support AWS Workloads course.\n12. To move to the next wizard step, click Next: Configure Health Check:\n13. In the Ping Path field, replace the contents with /:\nBy default, the fields on this page specify that the health check will be performed using the HTTP protocol on port eighty. This means the load balancer will assume an instance is healthy when the instance returns a 200 OK response.\nThe Advanced Details allow you to further customize different aspects of the health check:\nResponse Timeout: How long to the load balancer should wait for a response from the EC2 instance. Interval: Amount of time between health checks. Unhealthy threshold: The number of consecutive failed healthy checks that must occur before the load balancer declares the EC2 instance unhealthy. Healthy threshold: The number of consecutive health checks that must occur before declaring an EC2 instance healthy. To learn more about Elastic Load Balancing health checks, see the AWS documentation here.\n14. To move to the next wizard step, click Next: Add EC2 Instances:\nThis step of the wizard displays the EC2 instances that currently exist and can be added to the load balancer:\n15. Select the instances named web-node:\nTake a look at the configuration options on this page:\nCross-Zone Load Balancing ensures that your LB distributes incoming requests evenly across all instances in its enabled Availability Zones. This means that the LB will ignore the default of round-robin and will also take into consideration the Availability Zone in which the instance is running. This reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone and improves your application’s ability to handle the loss of one or more instances.\nConnection Draining is used to ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open.\nLeave these options at their defaults.\n16. To advance to the next wizard step, click Next: Add Tags:\nIn a non-lab environment, it is best practice to add tags to resources you create. Tags help make managing, organizing, and filtering resources in AWS easier.\nTo read more about tagging resources in AWS, see this document from AWS.\n17. To proceed to the review step, click Review and Create:\nThis page allows you to review the load balancing settings you have configured:\n18. To create your load balancer, click Create:\nYou will see a notification that your load balancer has been successfully created:\n19. To return to the EC2 management console, click Close:\nConfiguring Security Groups for Load Balanced EC2 Instances 1. In the list of load balancers, ensure your load balancer is selected:\nYou will see some tabs beneath the list and the Description tab will be selected.\nThis tab shows general information about your load balancer.\n2. To view information about instances registered with this load balancer, click the Instances tab:\nYou will see the instances and availability zones listed:\nThe instances will have a status of InService. This means the load balancer is performing successful health checks on the instances.\nNote: If you see the Status as OutOfService then the instances are still be registered. Wait a minute or two and then click the refresh icon in the top-right corner.\n3. To see the DNS of your load balancer, click the Description tab.\n4. Copy the domain name from the value of the DNS name field:\nWarning: Don’t include the (A Record) part of the value when copying.\n5. In a new browser tab, paste the domain name, and press enter.\nYou will see an instance Id displayed:\nNote: Your instance Id will be different.\nAn application has been pre-installed on the EC2 instances that will respond to web requests with the instance Id of the instance serving the request.\nTo see the Id of the other EC2 instance, refresh the page. If the Id doesn’t change, you may need to open an incognito or private browsing tab and visit the DNS name again.\nSeeing the Id change shows that the load balancer is working as expected, routing traffic to both registered instances.\nLeave this tab open and remember this is the tab for the load balancer, you will use it again later in the lab step.\n6. In the left-hand menu, under Instances, click Instances:\nYou will see two instances named web-node with a status of Running:\n7. Select one of the instances:\nYou will see tabs displayed below the list of instances.\n8. In the Details tab, in the Public IPv4 DNS field, click the copy icon:\nThe public DNS name of the EC2 has been copied to your clipboard.\n9. In a new browser tab, paste the DNS name and press enter.\nYou will see an instance Id displayed again.\nHowever, this time, because you are accessing the instance directly if you refresh or visit the DNS name in an incognito or private browsing tab, the Id won’t change.\nNote that you are accessing the instance directly, this is allowed by the security group associated with the EC2 instances. Allowing load-balanced instances to be publicly accessible is a bad security practice, and there is rarely a good reason for it.\nIn the rest of this lab step, you will modify the EC2 instance’s security group to only allow traffic from the load balancer.\nLeave this browser tab open and remember this is the tab for an EC2 instance, you will use this tab again later.\nNavigate to Load Balancers in the EC2 Management Console. 11. Ensure the classic-elb load balancer is selected.\n12. In the Description tab, scroll down to the Security section:\nThis is the security group you configured when you created the load balancer.\n13. In the left-hand menu, under Network \u0026 Security, click Security Groups:\nYou will see a list of security groups:\n14. Select the SG which has the Group Name starting with cloudacademylabs- .\nThis is the security group of the EC2 instances.\nYou will see tabs displayed beneath the list.\n15. In the row of tabs, click Inbound rules:\n16. To modify the rules of this security group, click Edit inbound rules:\nYou want to allow only connections coming from the load balancer to the instances, however, the balancer doesn’t have a particular IP address associated with it so you can’t specify an IP address here. Instead, you will restrict the access by using the security group you created for the balancer.\nYou will change the current rule to deny access to anywhere and allow it only to members of the load balancer’s security group.\n17. Delete the existing rule, and create a new one whose Type is HTTP. In the Source drop-down, ensure Custom is selected and in the box next to it, select elb-sg:\n18. To save your changes, in the bottom-right, click Save rules:\nWith your rule saved, reload the browser tab with the DNS of the load balancer.\nThis will continue to work, you will see an instance Id displayed.\n19. Reload the browser tab with the DNS of an instance in the address bar:\nThe exact behavior will vary depending upon your web browser.\nMost likely you see the loading symbol in the browser tab spinning indefinitely:\nIf you wait long enough, your browser will report that it timed out trying to reach the instance:\nChecking Your Load Balancer’s Behavior During Instance Failures Navigate to Instances in the EC2 Management Console. You will see two instances named web-node listed.\n2. To stop an instance, right-click one of them.\n3. In the menu that appears, click Instance state, and then click Stop instance:\nYou will see a dialog box asking you to confirm that you want to stop the instance.\n4. To confirm, click Stop:\nThe instance’s Instance state column will change to Stopping. A few moments later you will see it changed to Stopped:\nStopping the instance will make it fail your load balancer’s health checks.\nNavigate to Load Balancers in the EC2 Management Console. 6. Ensure the classic-elb load balancer is selected.\n7. In the row of tabs below the load balancer list, click Instances:\nLook at the Status column in the instances table, one of the instances will still be InService, and the other will be OutOfService:\nThis means that there is only one instance serving the application, and therefore all the requests will be forwarded to the same instance.\nYou can test this behavior by clicking on the Description tab and accessing the **DNS name **of the load balancer in a new browser tab. Your request will be served by the instance that you didn’t stop.\nLeave the browser tab with the load balancer’s DNS name open. You will test it again after starting the stopped instance.\n8. To start the stopped instance, in the left-hand menu, under Instances, click Instances:\n9. Right-click the stopped instance.\n10. Click Instance state, and click Start instance:\nNote: You can also access this menu using the Actions button in the top-right.\nThe Instance state column will change to Pending, and a few moments later, to Running.\nTest accessing the load balancer by it’s DNS name again. This time, you will see that both instances are serving requests.\nNote: You may need to open the load balancer’s domain name in an incognito or private browsing tab to see both instance Ids.\nMonitoring your Classic Load Balancer Navigate to Load Balancers in the EC2 Management Console. 2. In the list of load balancers, ensure the classic-elb load balancer is selected, and click the Monitoring tab:\nYou will see a number of graphs of different CloudWatch metrics.\nThe Elastic Load Balancing (ELB) service reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, the load balancing service measures and sends its metrics in sixty-second intervals. If there are no requests flowing through the load balancer, or no data for a metric, the metric is not reported.\nThere are a few metrics related to a Classic Load Balancer, and most are self-explanatory if you are familiar with HTTP requests. If some of them are unfamiliar to you, visit the Amazon AWS documentation to read more.\nThe metrics called HealthyHostCount, and **UnHealthyHostCount **will count the number of Healthy and Unhealthy instances respectively. These metrics can be useful for you to identify a major problem in your AWS account. A healthy instance is one that is passing the health checks performed by the load balancer.\nYou could use CloudWatch Alarms to notify you when you have less than 2 instances running your application, though to be clear this is not a general rule: the number of instances that might identify a problem will vary depending on your environment.\nAlso notice that in these metrics, there is no way of seeing the Availability Zone to which the Healthy/Unhealthy instance belongs. In our lab, we stopped an instance for a few minutes, therefore you should be able to see something like this:\nIf the Healthy Hosts metric reaches zero, that means that people won’t see anything when accessing your load balancer, and it is probable that you have a big problem in your infrastructure.\nThe Average Latency metric might be useful to identify potential issues in your setup. Maybe everything is working in your application, but you notice an increase in this metric. If you haven’t changed anything in your application, that can be a potential issue - maybe you haven’t provisioned enough EC2 instances, or you even have lots of instances but they don’t have enough power to serve your increasing traffic.\nThe other metrics can be very useful for troubleshooting specific scenarios and will vary depending on your setup.\n","description":"tutorial how to create AWS Classic Load Balancer","title":"Create Classic Load Balancer","uri":"/en/docs/aws-certified-developer-associate/elasticloadbalancing/create-amazon-load-balancing/"},{"content":"Creating an Amazon S3 Bucket Introduction You can create an Amazon S3 bucket using the AWS Management Console. As with many other AWS services, you can use the AWS API or CLI (command-line interface) as well.\nIn this lab step, you will create a new Amazon S3 bucket.\nInstructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: You will be placed in the S3 console.\nFrom the S3 console, click the orange Create Bucket button: Enter a unique Bucket name on the Name and region screen of the wizard: Region: US West (Oregon) (This should be set for you. If not, please select this region.) **Important!**Bucket names must be globally unique, regardless of the AWS region in which you create the bucket. Buckets must also be DNS-compliant.\nThe rules for DNS-compliant bucket names are:\nBucket names must be at least 3 and no more than 63 characters long. Bucket names can contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. Bucket names must not be formatted as an IP address (for example, 192.168.1.1). The following examples are valid bucket names: calabs-bucket-1, cloudacademybucket , cloudacademy.bucket , calabs.1 or ca-labs-bucket.\nTroubleshooting Tip: If you receive an error because your bucket name is not unique, append a unique number to the bucket name in order to guarantee its uniqueness:\nFor example, change “calabs-bucket” to “calabs-bucket-1” (or a unique number/character string) and try again. Leave the Block public access (bucket settings) at the default values: No changes are needed. This is where you can set public access permissions.\n5. Click on Create bucket:\nA page with a table listing buckets will load and you will see a green notification that your bucket was created successfully.\nIn the Buckets table, click the name of your bucket in the Name column: A page will load with a row of tabs at the top.\nTo see details and options for your bucket, click on the Properties: This page allows you to configure your Amazon S3 bucket in many different ways. No changes are needed in this lab at this time.\nFeel free to look at the other tabs and see the configuration options that are available.\n","description":"Create an Amazon S3 Bucket","title":"Create S3 Bucket","uri":"/en/docs/aws-certified-developer-associate/s3/create-s3-bucket/"},{"content":"Deleting an Amazon S3 Bucket Introduction You can delete an Amazon S3 bucket using the S3 console. You will delete all objects within the bucket as well.\nInstructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: From the top level of the S3 console, notice the Delete button is not actionable.\n2. Check the name of your bucket to select it:\nWith the bucket selected, click Empty: The Empty bucket form page will load.\nIt’s not possible to delete a bucket that contains objects.\nTo confirm that you want to delete all objects in this bucket, in the textbox at the bottom, enter permanently delete and click Empty:\nTo exit the empty bucket page, at the top-right, click Exit:\nYou will be returned to the Buckets page.\nTo delete your bucket, select it in the list, and click Delete\nTo confirm that you want to delete the bucket, in the textbox, enter the name of your bucket:\nClick Delete bucket to delete the bucket.\nWarning: Make sure to delete all the files/folders inside the bucket before deleting it, otherwise AWS won’t allow you to delete the S3 bucket.\nImportant! Notice the message from AWS: “Amazon S3 buckets are unique. If you delete this bucket, you may lose the bucket name to another AWS user.”\nIf retaining the bucket name is important to you, consider using the Empty bucket feature and not actually deleting the bucket.\n","description":"Delete an Amazon S3 Bucket","title":"Delete S3 Bucket","uri":"/en/docs/aws-certified-developer-associate/s3/delete-from-s3/"},{"content":"Web Create IB notification Login to https://www.interactivebrokers.co.uk/portal/#/ Click Deposit Click Use a new deposit method if no one exist Bank Wire -\u003e Get instructions Account Number: Bank account number\nNext you get Bank Wire Instructions These data you need to make a payment from Discount bank\nSend money from Discount bank Login start.telebank.co.il Click: ביצוע העברה\nFill the form\nClick המשך and proceed ","description":"Deposit Interactive Brokers from Israel Discount bank","title":"Deposit Interactive Brokers from Israel Discount bank","uri":"/en/posts/interactivebrokers-deposit/"},{"content":"Lab Develop and Deploy an Application with AWS CodeStar Creating an AWS CodeStar Project 1. In the AWS Management Console search bar, enter CodeStar, and click the CodeStar result under Services:\n2. On the welcome page, click Create project.\nTake a moment to see all of the different templates available in AWS CodeStar.\n3. Check the following boxes on the left filter bar to narrow down the listed templates:\nAWS services: EC2 Application category: Web application Programming languages: Node.js The choice of **Application category **and Programming language will be driven by the requirements of your project and skills available to you. The choice of AWS services may not be as easy. Some guidelines for choosing between the alternatives are:\nAWS Elastic Beanstalk: A good choice for a fully managed application environment running on EC2 servers. This option allows you to stay focused on your code. Amazon EC2: Preferable when you want to host the application on servers that you manage yourself, including on-premise servers. AWS Lambda: Choose this option if you want to run a serverless application. 4. Select the **Express.js **project template:\nExpress.js is a popular Node.js web application framework.\n5. In the next step of the Create project wizard, enter the following:\nProject name: ca-app-\u003cUnique_String\u003e (Replace \u003cUnique_String\u003e with a 6 characters. The name must be unique for the region because of AWS CodeCommit repository name restrictions) Project ID: Accept the default value The instructions in this Lab use ca-app for the project name, but you should use a different name or the project creation may fail if it is already in use.\n6. Make sure that CodeCommit is selected under Project repository:\nYou will see the EC2 Configuration section of the form.\n7. Ensure the following values are selected:\nInstance type: _t2.micro _(default value) VPC: Select the non-default VPC (The VPC without “(Default)”),or the VPC with only two subnets if there is no (Default) label Subnet: Select the subnet in the us-west-2a availability zone If you can’t see which subnet is in us-west-2a hover your mouse over each subnet.\n8. Click Next and then** Create Project**:\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nTouring the AWS CodeStar Project Website 1. Observe the tiles that are included in your Dashboard:\nIDE: References for how to get started with a variety of integrated development environments (IDEs) under Access your project code. You will simply use the EC2 instance to edit the code interact with CodeCommit in this lab.\nRepository: You can see the main details related to the code repository here. The most recent code commits for the selected branch:\nThe View commits button opens the detailed view list of the commits. Currently, there is only a master branch and the initial commit to display. The committer, AWS CodeStar, made the initial commit during the project creation. Each commit also includes a button on the right to view the code changes in AWS CodeCommit. You will look at the code in a future Lab Step.\nPipeline: This shows a graphical representation of the release pipeline for your project:\nAny time you commit a code change to the master branch, the pipeline will automatically deploy your application. As your application grows and the requirements for your release pipeline change, you can modify the pipeline by clicking Edit. For example, you may want to add an automated test stage, invoke an AWS Lambda function, or modify the deployment group to deploy to an Auto Scaling group. The **Release change **button can be used to force a deployment of the latest commit. That can be useful if you modify the pipeline or something went wrong with the release. If something does go wrong with a pipeline stage, you will see the bar on the left turn red.\nMonitoring: This shows the CPUUtilization and other metrics of the EC2 instance where your application is deployed.\nIssues: This Lab doesn’t include a JIRA project, but for projects requiring issue tracking you can find link to JIRA from here.\n3. Click View application in the upper-right to view the application included in the template:\nDepending on your time of day, the background will change. You will commit a code change later to modify the appearance of the application.\n4. Look at the Project resources tab under the Overview.\nThe most interesting thing to see here is the list of all the Project Resources created by the project template:\nAWS CodeStar saved you a lot of time compared to manually configuring everything that is included. Notice that AWS CloudFormation includes a stack resource. That is how AWS CodeStar works behind the scenes. Each project template creates a stack in AWS CloudFormation. Of course, you don’t need to know any of the details. AWS CodeStar does everything for you so you can focus on development.\nIf you need to delete an AWS CodeStar project, you can do so from the CodeStar project page. You will be given a choice of keeping the associated resources or also deleting the associated resources.\nDeveloping Your AWS CodeStar Project 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n2. Click on Users in the left navigation panel.\n3. In the Users table, click on student.\nNote: You will see error messages. This is normal. You only have the permissions required to complete the Lab.\n4. Click on the Security credentials tab.\n5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section, and click Generate credentials:\nThis will show a pop-up dialog showing you your credentials. 6. Click Download credentials:\nYour browser will download a file containing a username and password. Keep this file, you will use the credentials to connect to your AWS CodeStar repository.\n7. Return to your AWS CodeStar project’s Repository tab and click HTTPS under Clone repository:\nThis copies the HTTPS url of the CodeCommit repository to your clipboard.\n8. Paste the repository into the file with your code repository credentials.\nYou will use this URL later to access your repository.\n9. Return to the SSH shell connected to the dev-instance EC2 instance and enter cd to ensure you are in your home directory of /home/ec2-user.\nRefresh the instance connect browser tab if the session has expired.\n10. To tell Git to cache your credentials for a few hours, enter the following command:\ngit config --global credential.helper 'cache --timeout=10800' 11. Tell Git your user name:\ngit config --global user.name \"student\" This name will show up on the commits in your project dashboard.\n12. To clone your AWS CodeStar project repository, enter:\ngit clone \u003cYOUR_PROJECT_REPOSITORY_URL\u003e Replace \u003cYOUR_PROJECT_REPOSITORY_URL\u003e with the URL you copied in a previous instruction.\nYour URL will be similar to https://git-codecommit.us-west-2.amazonaws.com/v1/repos/ca-app.\n13. When prompted, enter the Username and Password you saved in a text file earlier in this Lab Step.\nTip: The password generated by AWS is long and it is easy to make a typo when entering it. To avoid errors copy and paste the password.\n14. Change the repository directory name to ca-app:\nmv ca-app-\u003cUnique_string\u003e ca-app Note: Change ca-app-\u003cUnique_string\u003e to the name of your repository.\nThis won’t change the repository name. It will only simplify the instructions at the command-line by not having to enter your unique string following ca-app in this and later Lab Steps. 15. Change into the directory:\ncd ca-app 16. Enter ls to get a quick overview of the project structure.\nThere are several files:\napp.js: JavaScript file that starts the server appspec.yml: Configuration file that instructs AWS CodeDeploy what steps to perform to deploy your application package.json: Metadata and dependencies related to your project README.md: Text file explaining the project template There is no need to get into the details of the file contents at this time. However, it is good to know that the appspec.yml file specifies scripts that run during the deployment of your application. The scripts are contained in one of the two project directories:\npublic: Static assets used for your application scripts: Scripts executed by AWS CodeDeploy during the deployment of your application Now you can get the server running on your development machine.\n17. Install the project dependencies using Node package manager (npm) and start the Node.js server:\nnpm install node app.js While the server is running you won’t be able to enter new commands. That won’t be a problem. Now you can test that the development server is serving the application.\nNavigate to Instances in the EC2 service in the AWS Console. 19. Select the instance named cloudacademylabs:\nIn the Description tab, you will see a field called Public DNS (IPv4).\n20. To copy the public DNS, click the click the copy icon under Public IPv4 DNS:\n21. Open a new browser taband paste the public DNS and append :3000 to the end and press enter:\nNow that you verified the application works on the development machine, you can make some code changes.\n22. Return to the SSH shell and press Ctrl+C to kill the running Node.js server.\n23. Enter the following multiline command at the shell prompt to update a file in the project:\necho 'var idx = Math.floor(new Date().getHours()); var body = document.getElementsByTagName(\"body\")[0]; var idxStep = 1; var refreshRate = 1000; function adjustIdx() { if (idx \u003c= 0) { // Start increasing idx idxStep = 1; } else if (idx \u003e= 23) { // Start decreasing idx idxStep = -1; } idx += idxStep; body.className = \"heaven-\" + idx; } body.className = \"heaven-\" + idx; setInterval(adjustIdx, refreshRate);' \u003e public/js/set-background.js 24. Test the changes by running the server again with node app.js and refresh the browser tab with your development application. You will see a similar page as the previous one, but the color will change roughly once a second.\n25. Stop the Node.js server with Ctrl+C.\n26. View the local repository status:\ngit status This tells you that you are on the master branch and working from the initial code commit. The output also shows the set-background.js file was modified. You need to add the file to stage it before committing.\n27. Add the modified file to the staged changes in the commit:\ngit add public/js/set-background.js 28. Commit the staged changes to the local repository and add a short message about the changes:\ngit commit -m \"animation\" 29. Push the changes in your local repository to the remote AWS CodeStar project repository so they are synchronized:\ngit push Now that you have made a change to your code, you will see how the changes are deployed in the next Lab Step.\nSummary In this Lab Step, you committed a code change to your AWS CodeStar project repository. You created the required credentials and tested the application on your development server.\nDeploying Your AWS CodeStar Project 1. Return to your AWS CodeStar project view.\nThere are a few things to notice since you were here last:\nYour commit is now visible in the **Repository **\u003e **Most recent commit **tile Your Monitoring \u003e CPUUtilization tile might show some spikes if your application has already been deployed Your Pipeline tab may show one of the pipeline stages In progress or you may see a recent timestamp inside each stage box telling you the new version has been deployed. If you missed the release flowing through the stages of the pipeline, click Release change and click Continue in the pop-up.\n2. To inspect the code, in the Repository tab, click the most recent Commit ID:\nYour commit Id will be different.\n3. Look at the code changes:\nAdditions appear in green and removals would appear in red, if any were present. This is an easy way to keep track of what is happening to the code in your AWS CodeStar project.\n4. Navigate back to the Pipeline tab. Click on AWS Code Deploy under Deploy:\nThis opens your application in AWS CodeDeploy:\nYou can see the Deployment Groups created for deploying your application. In this case there will be just one with a **Name **ending in -Env. The **Status **column will tell you if your last deployment **Succeeded **or failed. The time of the Last attempted deployment and Last successful deployment are also recorded.\n5. Click the name of your deployment group beginning with ca-app:\nNotice that by default Rollback enabled is false. That means if your deployment fails, AWS CodeDeploy will not attempt to deploy the last successful version. That is something you might consider changing when you use AWS CodeStar for one of your projects.\n6. Scroll down the page and inspect the Deployment group deployment history section.\nEach deployment that was attempted to be deployed is recorded here along with a link to where the artifacts are located on Amazon S3.\n7. Click on the most recent deployment in the Deployment Id column:\nYour deployment will have a different deployment id.\nThis opens a page with details of the most recent deployment:\nDeployment status: shows the state of the deployment operation Deployment details: shows information similar to what you saw on the AWS CodeDeploy application page Revision details: shows information about the revision deployed, including the location in AWS S3 Deployment lifecycle events: tells you the start and end times as well as the **Duration **of the deployment 8. To view the deployment life-cycle events, click View events down the bottom:\n9. To view the events, scroll down to the event list:\nYou will see events similar to the above.\nIn case of a failed deployment, one of the events will record the failure and provide a link under the Logs column to investigate the command and logs related to the failure. If you recall, the appspec.yml file in the code project was used to instruct AWS CodeDeploy on how to deploy your application. Your project provides different scripts to run for some of the events listed in the table.\n10. Finally, return to the AWS CodeStar and click View application.\nYou will see the latest version of your application including the animation commit deployed and available to the world.\nManaging Your AWS CodeStar Project Team 1. Return to your AWS CodeStar project’s Overview and click on Add team members:\n2. Click on the **User **drop-down menu and click on Logan.\n3. Set the team member values for Logan to:\nEmail address: test@cloudacademy.com Project Role: Contributor Remote Access: Checked (This allows the team member to upload an SSH public key to connect to EC2 instances) The difference between the default Project Roles is:\nViewer: Access to the project dashboard and able to view a few project resources Contributor: Everything Viewer can access plus view, modify, and access all project resources Owner: Everything Contributor has plus adding and removing team members, and deleting the project 4. Click Add team member:\nAfter adding a team member, you will be asked to create a profile for yourself.\n5. In the Create user profile form, enter the following values before clicking Create user profile:\nDisplay name: student Email address: student@cloudacademy.com You will see Logan and **student **appear in the **Team members **list.\n6. Click Add team member and select Bessie from the drop-down menu.\n7. Enter the following values and click Add:\nEmail address: bessie@cloudacademy.com Project Role: Viewer Remote Access: Unchecked Now you can briefly experience the differences between the project roles.\n8. At the top of this Lab page, click on the Open Environment button.\nThis will sign you out of the student user and allow you to sign in as a different user.\n9. Log in to AWS using the team member in the viewer role:\nUser Name: Bessie Password: Lab-Viewer1 Navigate to AWS CodeStar in the AWS Console. 11. Click on your project name.\nObserve that the viewer role has access to view the same tabs as your student user.\n13. Click on the Repository \u003e Commit ID and see that a viewer is allowed to view code changes.\n14. Return to the project Pipeline section and click Release change, then Release.\nYou will receive an error message stating that you are not authorized to perform that action:\n15. At the top of this Lab page, click on the Open Environment button and sign in again with the following credentials:\nUser Name: Logan Password: Lab-Contributor1 The user Logan is in the contributor role, which has additional permissions than the viewer role.\n16. Click Release change, then Continue.\nThe contributor has permission to perform this action:\n17. Click Team in the left sidebar.\nNotice that you can only remove yourself from the team and not other members. That is a distinction between the contributor and owner roles.\n18. One last time, in the lab, click on the Open Environment button and sign in with the student credentials given in the Credentialssection of the Lab.\nCleaning Up Your AWS CodeStar Project 1. Return to your AWS CodeStar project dashboard and click on Settings:\n2. Click Delete project.\n3. Enter delete in the pop-up dialog:\n4. Click Delete:\nIn a few seconds you will return to the AWS CodeStar start page and all of the resources in the project will begin terminating. ","description":"Develop and Deploy an Application with AWS CodeStar","title":"Develop and Deploy an Application with AWS CodeStar","uri":"/en/docs/aws-certified-developer-associate/codestar/develop-and-deploy-app-with-codestar/"},{"content":"About AWS EC2 AWS EC2 User Guide Amazon Elastic Compute Cloud (EC2) - one of the most popular AWS services.\nAllows:\nto run different types of cloud instances and pay-per-use models. to control computing resources at the operating system level working in an Amazon computing environment. Digest EC2 \u0026 EBS EC2 (Elastic Compute Cloud) Instance EBS (Elastic Block Store) - Persistent storage volume AMI (Amazon Machine Image) - Packages OS and additional installations in a reusable template Instance and Instance Types: General Purpose (t-type and m-type), Compute Optimized(c-type), GPU Graphics, GPU Compute, Memory Optimized(r, × and z-type), and Storage Optimized(d, h and i-type) Purchasing Options: On Demand, Reserved, Scheduled, Spot, Dedicated Instance and Dedicated Host Spot: Partial hours are not billed if terminated by AWS EC2 Secure login information for your instances using key pairs Placement group: Cluster and Spread For root:\nGeneral purpose SSD (balances price \u0026 performance) Provisioned OPS SD (Highest performance for mission critical low-latency or high throughput workloads) Magnetic HDD (previous generation) For other:\nThroughput Provisioned HDD (low cost for frequently accessed, throughput intensive workloads) Cold HDD (lowest cost for less frequently workloads) Instance Store - temporary storage volume in which data is deleted when you STOP or TERMINATE your instance Price Pricing models:\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time. Reserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper. Spot - Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use. Current price\nPractice TL;DR Choose a region close to you Go to EC2 service Click on “Instances” in the menu and click on “Launch instances” Choose image: Amazon Linux 2 Choose instance type: t2.micro Make sure “Delete on Termination” is checked in the storage section Under the “User data” field the following: yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"\u003ch1\u003eHello from web!\u003c/h1\u003e\" \u003e /var/www/html/index.html Add tags with the following keys and values: key “Type” and the value “web” key “Name” and the value “web-1” In the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere Click on “Review” and then click on “Launch” after reviewing. If you don’t have a key pair, create one and download it. Now HTTP traffic (port 80) should be accepted from anywhere Create an EC2 Instance Go to EC2 page -\u003e Launch Instance\nEC2 image Choose the image we want Create keys Let’s create a key to use to connect to the instance externally\nEnter any name you want. Leave all other parameters by default\nAfter the key is created it will start automatic downloading. You need it to connect to EC2 from your local terminal\nNetwork Settings Under Network Settings I leave Allow SSH traffic from\nCreate Click Launch Instance\nThe Instance has been created and is available for connection\nConnecting to EC2 from the terminal Connect to EC2 from a local terminal\nLet’s move previously created and downloaded mykey key to home folder of current user and give permissions to file CHMOD 400\ncd ~ cd Downloads/ mv mykey.pem $HOME cd .. chmod 400 mykey.pem To connect, we need a public iPv4 address. Find it on the instance page\nConnect with the command ssh.\nssh -i mykey.pem ec2-user@52.24.109.78 Questions Q1 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\nWhich approach meets these requirements?\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager\nSecrets Manager offers secret rotation\n4\nQ2 An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.\nIn this scenario, which of the following can be used to fulfill this requirement?\nDirectly attach multiple Instance Store volumes in an EC2 instance to deliver maximum IOPS performance. Launch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Launch an Amazon EFS file system and mount it to a Nitro-based Amazon EC2 instance and set the performance mode to Max I/O. Launch any type of Amazon EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Explanation An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible.\nThe AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.\nAmazon EBS is a persistent block storage volume. It can persist independently from the life of an instance. Since the scenario requires you to have an EBS volume with up to 64,000 IOPS, you have to launch a Nitro-based EC2 instance.\nAmazon EBS volume types\n2\nQ3 A Database Specialist manages an EBS-Optimized Amazon RDS for MySQL DB instance with Provisioned IOPS storage. The users recently raised a database IO latency issue during peak hours when it was always under a heavy workload. Upon review, the Specialist noticed that the RDS DB instance was barely using the maximum IOPS configured but was fully utilizing the maximum bandwidth for the required throughput. CloudWatch metrics showed that CPU and Memory utilization were at optimum levels.\nWhich action should the Database Specialist take to fix the performance issue?\nChange the underlying EBS storage type of the instance to General Purpose (SSD). Modify the DB instance to an EBS-Optimized instance class with higher maximum bandwidth. Disable EBS optimization on the MySQL DB instance to allow higher maximum bandwidth. Modify the DB instance to increase the size and corresponding Provisioned IOPS allocated to the storage. Explanation Amazon RDS volumes are built using Amazon EBS volumes, except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads. RDS currently supports both magnetic and SSD-based storage volume types. There are two supported Amazon EBS SSD-based storage types, Provisioned IOPS (called io1) and General Purpose (called gp2).\nProvisioned IOPS storage is a storage type that delivers predictable performance and consistently low latency. If your workload is I/O constrained, using Provisioned IOPS SSD storage can increase the number of I/O requests that the system can process concurrently.\nProvisioned IOPS SSD storage provides a way to reserve I/O capacity by specifying IOPS. However, as with any other system capacity attribute, its maximum throughput under load is constrained by the resource that is consumed first. That resource might be network bandwidth, CPU, memory, or database internal resources.\nEBS–optimized instances deliver dedicated bandwidth to Amazon EBS. When attached to an EBS–optimized instance, Provisioned IOPS SSD (io1) volumes are designed to achieve their provisioned performance, 99.9% of the time. Choose an EBS–optimized instance that provides more dedicated Amazon EBS throughput than your application needs; otherwise, the connection between Amazon EBS and Amazon EC2 can become a performance bottleneck.\n2\nQ4 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.\nHow can the application find this information?\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1\nQ5 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\nResources EC2 Linux Hands-On Lab EB FAQ EC2 Digest EB Digest Community posts https://dev.to/romankurnovskii/aws-ec2-cheat-sheet-2mhp ","description":"A step-by-step guide to Amazon EC2","title":"EC2","uri":"/en/docs/aws-certified-developer-associate/ec2/"},{"content":"About AWS Elastic Beanstalk AWS Elastic Beanstalk User Guide AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nDeploying new application versions to existing resources in AWS Elastic Beanstalk happens much faster (typically under a minute) and once again is mostly dependent on the size of the new application version.\nDigest When you want to use new run time capabilities with elastic bean stalk, it is better to use blue-green deployment Security group will not be removed when removing the stack with elastic bean stalk For long running tasks - Use Elastic Beanstalk worker environment to process the tasks asynchronously Launch configuration is used for modifying instance type, key pair, elastic block storage and other settings that can be configured only when launching the instance Rolling with Additional Batch and Immutable both involve provisioning new servers to ensure capacity is not reduced. All At Once means the application will be offline for the duration of the update. Performing a Rolling Update without an additional batch of servers means a reduction in capacity. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html For Blue green deployment - Use Elastic beanstalk swap URL feature or route 53 with weighted routing policies You create your own Elastic Beanstalk platform using Packer, which is an open-source tool for creating machine images for many platforms, including AMIs for use with Amazon Elastic Compute Cloud (Amazon EC2). Price There is no additional charge for AWS Elastic Beanstalk. Only the AWS resources required to store and run applications are charged.\nConcepts AWS doc Applications An application is a collection of different elements, such as environments, environment configurations, and application versions.\nYou can have multiple application versions held within an application.\nApplication Version An application version is a very specific reference to a section of deployable code. The application version will point typically to simple storage service (S3) where the deployable code may reside.\nEnvironment Configurations An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.\nEnvironment An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage the application is deployed as a solution and becomes operational within your environment.\nThe “environment” is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.\nEnvironment Tier Reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment.\nConfiguration Template This is the template that provides the baseline for creating a new, unique, environment configuration.\nPlatform Culmination of components in which you can build your application upon using Elastic Beanstalk. These are comprised of the OS of the instance, the programming language, the server type (web or application), and components of Elastic Beanstalk\nDeployment policies All at once – deploys the new version to all instances simultaneously and will be out of service for a short time. Rolling – deploys the new version in batches. Rolling with additional batch – deploys the new version in batches, but first launch a new batch of instances. Immutable – deploys the new version to a new set of instances. Traffic splitting – deploys the new version to a new set of instances and temporarily split incoming client traffic. Practice Controlled deployment with AWS Elastic Beanstalk Lab Controlled deployment with AWS Elastic Beanstalk\nIn this lab, we will deploy several application version updates in a load-balanced, auto-scaling environment.\nThe first update is deployed using a simple deployment. The second update is deployed using a `blue-green’ deployment, where a separate environment is created to run the new version of the application, and the DNS switch switches incoming traffic to the new environment.\nThe final deployment architecture will look like this\nLoading the application In this review, I’m using the code that Cloudacademy provided me, but I have a ready-made launch script that you can download from Elastic Beanstalk: download\nCreate Go to Elastic Beanstalk page and click Create Application.\nSet Name Specify a name for the new application Choose platform Under Platform choose the desired platform of the application. In our case - Node.js. Download source code Under Source code origin specify the version of the application and download the archive with the application. Example\nApplication Configuration Change the preset Configuration to Custom configuration:\nClick Edit under Rolling updates and deployments\nIn the default configuration, updates are distributed to all instances at the same time. This leads to application downtime, which is unacceptable for production environments.\nWe will set Rolling and Batch size to 30%\nNetwork Back in the main application form, click Edit in the Network configuration.\nOn the Modify network form, configure the following values, then Save. VPC: Select VPC with CIDR block 10.0.0.0/16. This will not be the default VPC. Load balancer settings: Load balancer subnets: Select subnets with CIDR blocks **10.0.100.0/24 **(us-west-2a)and 10.0.101.0/24 (us-west-2b). These are public subnets. The application load balancer requires at least two subnets in different availability zones Instance settings: * Instance subnets: Select a subnet with CIDR block 10.0.1.0/24. This is a private subnet.\nConfirmation. Press Create app.\nThe app creation process takes from 5 minutes.\nThen go to Dasboard This concludes the loading phase of the app in Elastic Beanstalk. Next, let’s break down how to switch the downloading of the new version of the application to the clients.\nDownloading version 2 of the app Downloading version 2.0 Press Upload and deploy and download the updated code. For example, you can change the text in the same source code for comparison.\nSpecify new version and publication settings Version comparison Now we can compare both versions by following the links. In my case the applications look like this\nChanging the url of the apps Now let’s swap the apps around. So that a user who previously went to one address will now see the 2nd version of the app.\nUnder Actions, click on Swap environment URLs and then select the app you want to swap\nRemoving Elastic Beanstalk resources Elastic Beanstalk runs EC2 instances as well as other services to deploy applications. But you can remove all services from a single window.\ngo to the Applications section Select an application.f Click on Actions -\u003e Terminate environment Translated with www.DeepL.com/Translator (free version) Questions Q1 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\nQ2 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\nWhich of the following options would meet this requirement?\nUse an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n2\nQ3 An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.\nIn Elastic Beanstalk, where does it store the application files and server log files?\nApplication files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk. Application files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs. Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs. Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs. Explanation AWS Elastic Beanstalk stores your application files and optionally, server log files in Amazon S3. If you are using the AWS Management Console, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account and the files you upload will be automatically copied from your local client to Amazon S3.\nOptionally, you may configure Elastic Beanstalk to copy your server log files every hour to Amazon S3. You do this by editing the environment configuration settings.\nWith CloudWatch Logs, you can monitor and archive your Elastic Beanstalk application, system, and custom log files from Amazon EC2 instances of your environments. You can also configure alarms that make it easier for you to react to specific log stream events that your metric filters extract.\nThe CloudWatch Logs agent installed on each Amazon EC2 instance in your environment publishes metric data points to the CloudWatch service for each log group you configure.\nEach log group applies its own filter patterns to determine what log stream events to send to CloudWatch as data points. Log streams that belong to the same log group share the same retention, monitoring, and access control settings. You can configure Elastic Beanstalk to automatically stream logs to the CloudWatch service.\nThe option that says: Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs is incorrect because the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to CloudTrail as this service is primarily used for auditing API calls.\n4\nQ4 A former colleague reached out to you for consultation. He uploads a Django project in Elastic Beanstalk through CLI using instructions he read in a blog post, but for some reason he could not create the environment he needs for his project. He encounters an error message saying “The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist.”\nWhat are the possible causes of this issue? (Select TWO.)\nHe selected the wrong platform for the Django code. Elastic Beanstalk CLI did not create one because your IAM role has no permission to create roles. Instance profile container for the role needs to be manually replaced every time a new environment is launched. You have not associated an Elastic Beanstalk role to your CLI. IAM role already exists but has insufficient permissions that Elastic Beanstalk needs. Explanation AWS EB CLI cannot create the instance profile for your beanstalk environment if your IAM role has no access to creating roles.\nThis error is also thrown when the instance profile has insufficient or outdates policies that beanstalk needs to function. More details on this can be seen on the references provided.\n2, 5\nResources https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html Tutorials and samples Community posts https://dev.to/romankurnovskii/todo-aws-aws-elastic-beanstalk-cheat-sheet-1718 https://dev.to/romankurnovskii/aws-elastic-beanstalk-top-questions-certified-developer-exam-478g ","description":"AWS Elastic Beanstalk","title":"Elastic Beanstalk","uri":"/en/docs/aws-certified-developer-associate/elasticbeanstalk/"},{"content":"About Amazon Elastic Container Registry (Amazon ECR) - Fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere\nDocumentation User Guide Hosted private Docker registry\nAlternatives Docker Hub JFrog Artifactory Azure Container Registry Harbor Google Container Registry Red Hat Quay JFrog Container Registry Price Current price\nUse Cases Store, encrypt, and manage container images\nManage software vulnerabilities Streamline your deployment workloads Manage image lifecycle policies Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nPractice This commands returns the command to execute to be able to login to ECR:\nLogin get-login-password:aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository: aws ecr create-repository \\ --repository-name hello-repository \\ --image-scanning-configuration scanOnPush=true \\ --region region Tag image docker tag hello-world:latest aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Push docker push aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Pull docker pull aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository:latest Delete an image aws ecr batch-delete-image \\ --repository-name hello-repository \\ --image-ids imageTag=latest \\ --region region Delete a repository aws ecr delete-repository \\ --repository-name hello-repository \\ --force \\ --region region Labs:\nUse AWS Fargate for Serverless Deployment of Container Applications Quick start: Publishing to Amazon ECR Public using the AWS CLI Notes:\nIf you get a 503 Service Temporarily Unavailable error, try again after 30 seconds to let the load balancer finish adding the task to the target group. ","description":"Run highly secure, reliable, and scalable containers","title":"Elastic Container Registry","uri":"/en/docs/aws-certified-developer-associate/ecr/"},{"content":"About Documentation User Guide Highly secure, reliable, \u0026 scalable way to run contai­ners\nAlternatives Google Container Engine (GKE) Azure Container Service IBM Bluemix Container Service Jelastic Multi-Cloud PaaS Terminology Amazon ECS Term\tDefinition Cluster Logical Grouping of EC2 Instances Container Instance\tEC2 instance running the ECS agent Task Definition Blueprint that describes how a docker container should launch Task A running container using settings in a Task Definition Service Defines long running tasks – can control task count with Auto Scaling and attach an ELB Digest Microservices are built in multiple programming languages Containers simplify deployment of microservices: Step 1 : Create a self contained Docker image Application Runtime (JDK or Python), Application code and Dependencies Step 2 : Run it as a container any where Local machine OR Corporate data center OR Cloud Use On-Demand instances or Spot instances Launch type: EC2 or Fargate Data volumes attached to containers Deployment type: Rolling update Blue/green deployment (powered by AWS CodeDeploy) Task Placement Strategies: binpack - Leave least amount of unused CPU or memory. Minimizes number of container instances in use random - Random task placement spread - Based on specified values: Host (instanceId) (OR) Availability Zone(attribute:ecs.availability-zone) (Alowed) Combine strategies and prioritize How do you manage 100s of containers? ECS - Fully managed service for container orchestration Step 1 : Create a Cluster (Group of one or more EC2 instances) Step 2: Deploy your microservice containers AWS Fargate: Serverless ECS. DON’T worry about EC2 instances. Cloud Neutral: Kubernetes AWS - AWS Elastic Kubernetes Service (EKS) Load balancing: Performed using Application Load Balancers Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.myapp.com/microservice-a and www.myapp.com/microservice-b) Price Current price\nUse Cases Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nBest practice:\n10 Microservices =\u003e 10 Task Definitions =\u003e 10 Task IAM Roles with individual permissions needed by each microservice ECS vs EKS Amazon also provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\nAmazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Questions Q1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.\nmemory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_monitoring.html\n1, 2\n","description":"Run highly secure, reliable, and scalable containers","title":"Elastic Container Service","uri":"/en/docs/aws-certified-developer-associate/ecs/"},{"content":"About Kubernetes (K8) Docker Container/Cluster management\nRun highly secure, reliable, and scalable containers\nDocumentation User Guide Alternatives Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Google Kubernetes Engine (GKE) Oracle Cloud Infrastructure Container Engine for Kubernetes Mirantis Kubernetes Engine (formerly Docker Enterprise) Kubernetes Cloud Foundry Price Current price\nUse Cases Build and run web applications Deploy across hybrid environments Model machine learning (ML) workflows ECS vs EKS Amazon provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\nAmazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Practice Building a Cloud Native Application\n","description":"Amazon Elastic Kubernetes Service","title":"Elastic Kubernetes Service","uri":"/en/docs/aws-certified-developer-associate/eks/"},{"content":"About Documentation User Guide Amazon Elasticache is a fully managed Redis or Memcached in-memory data store.\nIt’s great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\nYou can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don’t require durability like session stores, gaming leaderboards, streaming, and analytics.\nCompatible with Redis and Memcached\nPrice Current price\nUse Cases Type: In-memory\nUse Case Benefit Web session store In cases with load-balanced web servers, store web session information in Redis so if a server is lost, the session info is not lost, and another web server can pick it up Database caching Use Memcached in front of AWS RDS to cache popular queries to offload work from RDS and return results faster to users Leaderboards Use Redis to provide a live leaderboard for millions of users of your mobile app Streaming data dashboards Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays Caching Engines Memcached Redis Simple, no-frills You need encryption You need to elasticity (scale out and in) You need HIPAA compliance You need to run multiple CPU cores and threads Support for clustering You need to cache objects (e.g. database queries) You need complex data types You need HA (replication Backup and restore features Pub/Sub capability Multi-AZ with Auto-Failover Non persistent. No backups Multi-node for partitioning of data (sharding) Memcached ElastiCache manages Memcached nodes as a pool that can grow and shrink (similar to an EC2 Auto Scaling group); individual nodes are expendable and non-persistent.\nMemcached provides a simple caching solution that best supports object caching and lets you scale out horizontally. Ideal for offloading a DB’s contents into a cache.\nRedis ElastiCache manages Redis more as a relational database, i.e. Redis clusters are managed as persistent, stateful entities that include using multi-AZ redundancy for handling failover (similar to RDS).\nRedis supports complex data structures, hence would be ideal in cases where sorting and ranking datasets in memory are important (e.g. such as in leaderboards for games).\nCaching Strategies Lazy Loading The data that is read from the DB is stored in the cache. The data can become stale The data becomes stale because there are no updates to the cache when data is changed in the database Only cache data when it is requested. Cache miss penalty on initial request. Chance to produce stale data; can be mitigated by setting a TTL. Shorter TTL = less stale data.\nWrite-Through The data is added/updated into the cache everytime the data is written to the DB (no stale data) Because the data in the cache is updated every time it’s written to the database, the data in the cache is always current. Every database write will write to the cache as well. Data is never stale however there will be alot more operations to perform; and these resources are wasted if most of the data is never used.\nSession Store Stores temporary session data in cache (with TTL) - Time to Live. Data expires after the given time Practice Configuring a Lambda function to access Amazon ElastiCache in an Amazon VPC\nQuestions Q1 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\nThe metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to Elasticache is required for test or development purposes, it can be done through a VPN.\n2\nQ2 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\n","description":"Amazon ElastiCache","title":"ElastiCache","uri":"/en/docs/aws-certified-developer-associate/elasticache/"},{"content":"Lab Encrypting S3 Objects Using SSE-KMS Creating a Customer Master Key (CMK) 1. In the AWS Management Console search bar, enter KMS, and click the KMS result under Services:\n2. Select Customer managed** keys** in the left pane of the KMS console.\nWarning: Cloud Academy cleans up the lab environment for you after a lab is completed or terminated. As a precaution, AWS prevents keys from being deleted immediately. Rather, they are queued for deletion, and an expiration period is set (of 7-30 days). For this reason, you may see residual keys from other students within the last week. For this reason, you may need to append a unique number to the Alias field in the next instruction.\n3. Click Create Key, then expand **Advanced Options **and set the following values:\nKey type: **Symmetric **(Symmetric keys are suitable for most data encryption applications. The same key is used for both encrypt and decrypt operations with symmetric key algorithms.) Key usage: Encrypt and decrypt Advanced options: Key Material Origin: Leave as KMS (default). AWS will generate the key material for encryption. Note that another common use case is for customers to generate their own keys, and have AWS keep a back up encrypted copy and help manage them with KMS. Regionality:** Single-Region key** 4. Click Next to advance to the Add Labels page of the wizard.\n5. Set the following values before clicking Next (leave the default values for other fields)\nAlias: _calabs-CMK-key _(Append a unique number to the key’s Alias if needed to be unique. For example, calabs-CMK-key2.) Description: 6. Click Next to advance to Define Key Administrative Permissions and leave the default values.\nAdministrative permissions allow users and roles to administer CMKs but not to perform cryptographic operations. In production environments, this is sometimes used to easily grant limited access to other users. The **Allow key administrators to delete this key **checkbox makes it explicit if deleting keys is allowed, since the key can’t be recovered once deleted, making recovery of encrypted data impossible. Note that key deletion is not immediate and first enters into a pending state before the key is deleted. The delete operation can be canceled while in the pending state.\nThese settings generate a key policy. The default policy allows IAM policies to grant access the key, which is why you don’t require selecting your student user as an administrator. The lab IAM policy of your student user allows you to perform the required actions of the lab.\n7. Click Next to advance to Define Key Usage Permissions.\nUsage permissions grant access to perform cryptographic operations such as encrypting and decrypting. Enterprises usually have different permissions for administrators and users, hence the wizard walks you through defining both.\nNotice that you can grant access to the key so other AWS accounts can use it for encryption/decryption. 8. Click **Next **to preview the key policy and then click Finish when ready. The CMK is created.\n9. Confirm the key created correctly and that the Status is Enabled:\nEncrypting S3 Data using Server-Side Encryption with KMS Managed Keys (SSE-KMS) You will upload a file and encrypt it using SSE-KMS in this lab step.\n1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n2. Click the name of the bucket the Cloud Academy lab environment created for you (name begins with cloudacademylabs-ssekms):\n3. Click Upload.\n4. Click Add files and select a small file, or download this sample file and select it.\n5. Expand the Properties tab and scroll until the Server-side encryption settings.\n6. Check the Specify an encryption key checkbox. 7. Check the AWS Key Management Service key (SSE-KMS) checkbox and then the Choose from your AWS KMS keys checkbox:\n8. Choose the AWS KMS key you previously generated:\n9. Click on Upload.\n10. Click Close and then click the name of the object to open its properties panel: You can verify the object is encrypted using SSE-KMS by checking that the Encryption field is AWS-KMS.\nEnforcing S3 Encryption Using Bucket Policies 1. In the S3 bucket console, click the Permissions tab followed by Bucket Policy to open the Bucket policy editor:\nBucket policies are IAM policies applied to a bucket rather than to a user or role as is conventionally done with IAM policies. Similar to how a key policy applied to the CMK. These are examples of resource-based policies in AWS.\n2. Paste the following bucket policy into the policy editor:\n{ \"Version\": \"2012-10-17\", \"Id\": \"RequireSSEKMS\", \"Statement\": [ { \"Sid\": \"DenyUploadIfNotSSEKMSEncrypted\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::\u003cYour_Bucket_Name\u003e/*\", \"Condition\": { \"StringNotEquals\": { \"s3:x-amz-server-side-encryption\": \"aws:kms\" } } } ] } This policy denies (\"Effect\": \"Deny\") all users’ (\"Principal\": \"*\") uploads (\"Action\": \"s3:PutObject\") to the bucket (\"Resource\": \"arn:aws:s3:::\u003cYour_Bucket_Name\u003e/*\") if the s3:x-amz-server-side-encryption is not set to aws:kms, which corresponds to SSE-KMS. The lab provides you with the policy but you could recreate it using the policy generator linked to beneath the policy editor.\n3. Replace \u003cYour_Bucket_Name\u003e with the name of your lab bucket (it begins with cloudacademylabs-ssekms- and can be copied from the S3 console):\n4. Click Save changes to save the policy and have it start being enforced.\n5. Click the **Objects **tab followed by Upload.\n6. Click Add files and select a small file, or download this sample file and select it.\n7. Click Upload and observe the image does not appear in the bucket contents table.\nClicking upload without configuring any properties of the object uses the default of no encryption.\nYou can see the upload Failed. 8. Retry the upload but this time use the Set properties step to configure **Encryption **to AWS KMS master-key using your CMK.\nThe upload now succeeds since the bucket policy condition is satisfied:\nThe policy does not require the use of your CMK however, so the default S3 KMS key in the region is also allowed. You can change the policy condition to enforce a specific CMK is used.\n","description":"How to encrypt S3 Objects Using SSE-KMS","title":"Encrypting S3 Objects Using SSE-KMS","uri":"/en/docs/aws-certified-developer-associate/kms/encrypting-s3-objects-using-sse-kms/"},{"content":"About EventB­ridge is a serverless event bus that makes it easy to connect applic­ations together using data from apps, integrated SaaS apps, \u0026 AWS services.\nDocumentation User Guide EventB­ridge is a low-cost alternative to building a new backend infrastructure for every new app. With Serverless EventB­ridge, you can connect your existing apps with a few lines of code. You don’t have to build a new backend for every new app you want to connect to.\nYou can use existing infrastructure as a provider of event data, and connect your apps using Serverless EventB­ridge.\nAlternatives Azure Service Bus TIBCO Cloud Integration (including BusinessWorks and Scribe) IBM App Connect Google Cloud Pub/Sub Apache Camel Peregrine Connect Software AG webMethods IBM Cloud Pak for Integration Price Current price\nUse Cases Type: Applic­ation integr­ation\nSame type services: SNS, SQS, AppSync, EventBridge\nRe-architect for speed Extend functionality via SaaS integrations Monitoring and Auditing Customize SaaS with AI/ML EventBridge vs Amazon SNS In comparison with Amazon SNS, EventBridge:\nIntegrates with more AWS services than SNS Supports registering message schemas Has sophisticated third-party integrations available Supports transforming event messages before sending them You should choose to use Amazon EventBridge over Amazon SNS when the system you are building is expected to:\nSupport significant asynchronous functionality Grow significantly in terms of both usage and complexity Have changing requirements over time Have components built by different teams that interact Need support for disparate event sources and targets Amazon EventBridge vs CloudWatch Events Amazon EventBridge extends CloudWatch Events - Build event-driven architectures Original goal with CloudWatch Events was to help with monitoring usecases specific to AWS services. React to events from Your Applications, AWS services and Partner Services Example: EC2 status change, change in your application or SaaS partner application Event Targets can be a Lambda function, an SNS Topic, an SQS queues etc Rules map events to targets (Make sure that IAM Roles have permissions) Event buses receive the events: Default event bus (for AWS services) Custom event bus (custom applications) Partner event bus (partner applications) Over time, Amazon EventBridge will replace Amazon CloudWatch Events Practice Processing File Uploads Asynchronously with Amazon EventBridge\nQuestions Q1 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\n","description":"Amazon EventBridge - Build event-driven applications at scale across AWS, existing systems, or SaaS apps","title":"EventBridge","uri":"/en/docs/aws-certified-developer-associate/eventbridge/"},{"content":"Lab Fan-Out Orders using Amazon SNS and SQS Creating an Amazon SNS Topic and Amazon SQS Queues Here’s a diagram of what you will build and configure in this lab step:\nIn the search bar at the top, enter SNS and under Services, click the Simple Notification Service result: In the Create topic card on the right, in the Topic name textbox, enter new-orders and click Next step: The Create topic form will load.\nBy default, the Type of topic selected will be Standard. This is the most scalable topic type. The cost of this scalability is that message order and exactly-once delivery attempts can not be guaranteed.\nIf you are building a solution requires strict message ordering and exactly-once message delivery, you should use a FIFO type topic.\nStandard is fine for this lab.\nClick the black triangle next to Access policy - optional to expand the section: In the Access policy section, under Define who can publish messages to the topic, select Everyone: Under Define who can subscribe to this topic, select Everyone: You are using a permissive access policy to save time and because the focus of this lab is on demonstrating the fan-out scenario.\nIn a non-lab environment, you should carefully consider the access policy required and make sure if conforms with your company or organization’s security requirements.\nScroll to the bottom of the page, and click Create topic: You will see a page load displaying details of your newly created topic:\nIn the order processing system your are building, this Amazon SNS topic is where orders are published to. In a non-lab environment it would most likely be a web application or other application that accepts orders that will publish messages to this topic.\nNext, you will create two queues using Amazon Simple Queue Service and subscribe them to your Amazon SNS topic.\nOpen a new tab by right-clicking the AWS icon in the top-left and selecting Open in new tab. Note: The above instruction may vary slightly depending upon the web browser you are using.\nIn the search bar at the top, enter SQS, and under Services, click the Simple Queue Service result: In the middle right of the screen, in the Get started card, click Create queue: The Create queue form will open.\nIn the Name textbox, enter orders-for-inventory: Scroll down to the bottom, click Create queue: You will see a web page load showing you details of your newly created Amazon SQS queue:\nYou will now create a second Amazon SQS queue for analytics.\nTo navigate to the Queues list page, at the top-left, click Queues: On the right-hand side, click Create queue.\nRepeat the queue creation process, only this time enter orders-for-analytics as the Name of the queue.\nReturn to the Queues list page by clicking Queues in the top-left.\nYou will see the two queues you have created:\nClick the radio button for the orders-for-analytics queue.\nOn the right-hand side, click Actions and click Subscribe to Amazon SNS topic:\nThe Subscribe to Amazon SNS topic form will load.\nIn the Choose a topic drop down, select the topic ending with new-orders: This is the Amazon SNS topic you created earlier.\nClick Save to finish subscribing this queue to your topic.\nAt the top-left, click Queues again.\nRepeat the topic subscription process for your orders-for-inventory Amazon SQS queue.\nYou now have both of your Amazon SQS queues subscribed to your Amazon SNS topic. Any messages published to the topic will fan-out to both queues.\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nPublishing and Processing Messages In the terminal, enter the following command: aws sns list-topics You will see one topic displayed:\nNote: Your TopicArn will have a different account identifier.\nBy default, the AWS command-line interface tool uses the JSON format for responses. This response contains an array of Topics with one element. The element consists of a TopicArn.\nArn is short for Amazon Resource Name. An ARN is used to uniquely identify resources in AWS.\nIn this lab, the EC2 instance has been configured with an IAM role that has permissions to interact with Amazon SNS topics and Amazon SQS queues.\nStore the value of the TopicArn attribute in a shell variable (topic_arn): topic_arn=$(aws sns list-topics --query 'Topics[0].TopicArn' --output text) The above command uses the --query option to select only the value of the TopicArn and the --output option is used to specify plaintext format which removes the quotation marks from the value.\nTo publish a message, enter the following, utilizing the ARN you stored in the topic_arn shell variable: aws sns publish \\ --topic-arn $topic_arn \\ --message \"1 x Widget @ 21.99 USD\\n2 x Widget Cables @ 5.99 USD\" In response, you will see a MessageId:\nNote: Your message identifier will be different.\nYou have successfully published an order message to your Amazon Simple Notification Service topic.\nIn this lab, you are using the AWS command-line interface tool to simulate an application publishing an order message.\nIn a non-lab environment, the message could be published by a web application that accepts orders from customers.\nTo list Amazon Simple Queue Service queues, enter the following command: aws sqs list-queues You will see a JSON response:\nThe queues that you created earlier are listed.\nStore each of the QueueUrls in shell variables: analytics_queue_url=$(aws sqs list-queues --query 'QueueUrls[0]' --output text) inventory_queue_url=$(aws sqs list-queues --query 'QueueUrls[1]' --output text) To retrieve a message from the orders-for-analytics queue, enter the following command, utilizing the analytics queue URL you stored previously: aws sqs receive-message \\ --queue-url $analytics_queue_url You will see a JSON response containing an array with one Message:\nThe response contains the following fields:\nBody: A JSON representation of the message ReceiptHandle: You are required to supply this to delete a message after processing MD5OfBody: An MD5 hash of the message body MessageId: The message identifier that Amazon SNS saw when pushing the message to the queues Note that this is not the same as the MessageId that Amazon SNS returned to you when you published to the topic Repeat the previous instruction, using the orders-for-inventory queue but store the message response in a shell variable (for use later) and output the shell variable (using Python’s JSON tool to pretty print it): inventory_message=$(aws sqs receive-message --queue-url $inventory_queue_url) echo $inventory_message | python -m json.tool You will see the same message displayed again.\nThe message you published to the Amazon SNS topic has been sent to the Amazon SQS queues you subscribed the topic. This is an example of fanning out a message to multiple receivers.\nIn a non-lab environment, you could have worker applications constantly running and asking the Amazon SQS queues for more messages. One worker may be updating an inventory database for the order, whilst another worker could be recording the order details in a data lake for future analysis.\nUsing Amazon SNS and Amazon SQS like this allows you to build scalable systems that are decoupled and resilient. If a worker went offline, messages would queue up in the Amazon SQS queues. When the worker is available again, it can pick up new messages where it left off.\nYou can also have multiple worker applications, to help ensure there’s no downtime in message processing.\nAfter successfully processing a message, a worker application should delete the message to prevent it from being processed again.\nStore the value of the ReceiptHandle attribute in a shell variable: receipt_handle=$(echo $inventory_message | python -m json.tool | grep ReceiptHandle | cut -d\\\" -f 4) To delete a message, enter the following command for the orders-for-inventory queue: aws sqs delete-message \\ --queue-url $inventory_queue_url \\ --receipt-handle $receipt_handle Return to your browser tab with the Amazon SQS management console open. Note: If the SQS management console appears to only have one SQS queue, click the refresh button above the table:\nThe correct number of SQS queues will be displayed after a refresh.\nNavigate to the Queues list and click the orders-for-inventory queue.\nIn the top-right, click Send and receive messages:\nVerify that in that Receive messages section, under Messages available, it says 0. This is the queue you deleted a message for, simulating a long-running background application that receives an Amazon SQS message and then deletes the message after processing.\nRepeat the last three instructions for the orders-for-analytics queue and verify Messages available is 1: This is the queue you did not delete the message for. The message is still available to be picked up for processing by an application receiving messages from the queue.\n","description":"Fan-Out Orders using Amazon SNS and SQS","title":"Fan-Out Orders using Amazon SNS and SQS","uri":"/en/docs/aws-certified-developer-associate/sqs/fan-out-orders-with-sns-sqs/"},{"content":"About Serverless version of ECS.\nServerless compute for contai­ners.\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.\nDeploy and manage your applications, not infrastructure. Fargate removes the operational overhead of scaling, patching, securing, and managing servers.\nCompatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\nDocumentation User Guide Alternatives Google Kubernetes Engine (GKE) Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Azure Container Instances Cloud Foundry Oracle Cloud Infrastructure Container Engine for Kubernetes Price Current price\nUse Cases Web apps, APIs, and microservices Run and scale container workloads Support AI and ML training applications Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nQuestions Q1 How AWS Fargate different from AWS ECS?\nExplanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don’t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n","description":"Serverless compute for containers","title":"Fargate","uri":"/en/docs/aws-certified-developer-associate/fargate/"},{"content":"About AWS Fault Injection Simulator (FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application’s performance, observability, and resiliency.\nDocumentation User Guide Price Current price\nWith AWS FIS, you pay only for what you use. There are no upfront costs or minimum fees. You are charged based on the duration that an action is active. The AWS FIS price is $0.10 per action-minute.\nTerminology and Concepts Everything starts with an experiment template. The experiment template defines the targets that participate in the experiment. Supported targets are:\nEC2 Instances EKS node groups RDS clusters \u0026 instances IAM roles The actions define the injected faults. You can run actions in parallel or sequence.\nSome action examples:\nAWS API level errors for the EC2 service Stop/reboot/terminate EC2 instances Run SSM commands on EC2 instances to stress CPU or memory, add network latency, or kill a process Reboot RDS instance Failover RDS cluster Drain ECS container instance Terminate EKS node group instance Use Cases Periodic Game Days Continuous Delivery Pipeline Integration Practice Test instance stop and start using\nQuestions Q1 What is Chaos Engineering?\nExplanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n","description":"Improve resiliency and performance with controlled experiments with AWS Fault Injection Simulator","title":"Fault Injection Simulator","uri":"/en/docs/aws-certified-developer-associate/fis/"},{"content":"Granting Public Access to an Amazon S3 Object Introduction All uploaded files are private by default and can only be viewed, edited, or downloaded by you. In order to illustrate this point, complete the instructions below.\nNote: The terms “file” and “object” are often used interchangeably when discussing Amazon S3. Technically, Amazon S3 is an object-store. It is not a block storage device and does not contain a file system as your local computer does. However, files such as images, movies, and sound clips are often uploaded from your file system to Amazon S3.\nInstructions 1. Click on the object you just uploaded to the S3 bucket.\nTake a look at the Object overview section:\nUnder Object URL, right-click the link and open the URL in a new browser tab: You will see an XML (eXtensible Markup Language) response telling you that access is denied for this object:\nNote: The response may appear differently depending upon your web browser.\nLeave the browser tab open. You will return to it shortly.\nTo allow public access to objects, you need to disable the default safety guards that prevent them from being made publicly accessible.\nTo return to the bucket view, at the top of the page, click the name of your bucket in the bread crumb trail:\nClick the Permissions tab and click Edit in the Block public access section:\nUncheck all of the options to allow all kinds of public access:\nYou should carefully consider anytime you allow public access to S3 buckets. AWS has implemented these security features to help prevent data breaches. For this lab, there is no sensitive data and you do want to allow public access.\nPoorly managed Amazon S3 permissions have been a contributing factor to many unauthorized data access events. AWS is making sure you understand the implications of allowing public access to an Amazon S3 bucket.\nAt the bottom of the page, click Save changes: A confirmation dialog box will appear.\nEnter confirm in the confirmation dialog box and click Confirm: You will see a green notification that the public access settings have been edited.\nTurning off Block all public access does not automatically make objects in an Amazon S3 bucket public. There are several ways of of explicitly granting public access including:\nBucket policies IAM policies Access control lists Pre-signed URLs In this lab, you will use a bucket policy to grant public access to your Amazon S3 bucket.\nScroll down to the Bucket policy section and click Edit: The Edit bucket policy page will load. Here you can specify a JSON (JavaScript Object Notation) policy to control access to your Amazon S3 bucket.\nReplace the contents of the Policy editor with the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"BUCKET_ARN/*\", \"Principal\": \"*\" } ] } This is a permissive policy that allows GetObject access to anyone. More restrictive policies are possible such as\nRestricting access to specific principals Allow cross AWS account access Using conditions to restrict access to a specific IP address Notice the Resource is currently “BUCKET_ARN/*\", which is causing an error. We need to replace this with the ARN of the bucket we created:\nClick the copy icon under **Bucket ARN **and replace BUCKET_ARN in the value of the Resource key with the ARN you just copied : Note: Ensure that you preserve the /* at the end of the value. This means that the policy will apply to all objects inside the bucket recursively. Public access won’t be granted if this is not present.\nAt the bottom of the page, click Save changes: You will see a green notification that the bucket policy was edited.\nReturn to the browser tab where access was denied and fresh the browser tab. You will see the response change from “Access Denied” to the logo: ","description":"Grant Public Access to an Amazon S3 Object","title":"Grant public access to S3 Object","uri":"/en/docs/aws-certified-developer-associate/s3/grant-access-s3/"},{"content":"We can use recursion. Use Object.assign() and an empty object ({}) to create a shallow clone of the original. Use Object.keys() and Array.prototype.forEach() to determine which key-value pairs need to be deep cloned.\nconst deepClone = obj =\u003e { let clone = Object.assign({}, obj); Object.keys(clone).forEach( key =\u003e (clone[key] = typeof obj[key] === 'object' ? deepClone(obj[key]) : obj[key]) ); return Array.isArray(obj) \u0026\u0026 obj.length ? (clone.length = obj.length) \u0026\u0026 Array.from(clone) : Array.isArray(obj) ? Array.from(obj) : clone; }; const a = { foo: 'bar', obj: { a: 1, b: 2 } }; const b = deepClone(a); // a !== b, a.obj !== b.obj ","description":"How to create a deep clone of an object in JavaScript","title":"How to create a deep clone of an object in JavaScript","uri":"/en/posts/howto-create-deepclone-js/"},{"content":"Red Hat Enterprise Linux 9 (RHEL 9), codenamed Plow, has gone public (GA). Red Hat announced it on May 18, 2022. It replaced the beta version, which had been in existence since November 3, 2021.\nRHEL 9 is the first few releases in the Red Hat family. It is the first major release since IBM acquired Red Hat in July 2019, and the first major release since abandoning the CentOS project in favor of CentOS Stream, which is now RHEL’s predecessor.\nRHEL 9 is the latest major version of RHEL and comes with a 5.14 kernel, lots of new software packages and a host of improvements. It emphasizes security, stability, flexibility and reliability.\nDescription. RHEL 9 ships with new versions of software including Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0, and many more.\nPreparing for installation Registration on the Red Hat portal Red Hat Developer Subscription is a free Red Hat Developer Program offer designed for individual developers who want to take full advantage of Red Hat Enterprise Linux.\nIt gives developers access to all versions of Red Hat Enterprise Linux, as well as other Red Hat products such as add-ons, software updates and security bugs.\nFirst of all, make sure you have an active Red Hat account. If you don’t already have an account, go to the Red Hat Customer Portal, click on “Register” and fill out your information to create a Red Hat account. Downloading the installation image After creating a Red Hat account, you can start downloading RHEL 9. To download Red Hat Enterprise Linux 9 absolutely free, go to Red Hat Developer Portal and log in with your account credentials. Then go to the download RHEL 9 page and click on the download button shown below.\nI’m using a MacBook M1, so I download the RHEL 9 image for the M1 processor aarch64 Virtual machine I use the free UTM virtual machine as a virtual machine to install RHEL 9. You can install using Homebrew by running the command brew install --cask utm.\nInstalling Red Hat Enterprise Linux 9 Setting up the UTM virtual machine In UTM click Create a New Virtual Machine -\u003e Virtualize Choose the downloaded RHEL 9 image and click Continue. Main Setup Menu The marked fields need to be filled in\nCreate Root Password User Creation. Create the user you want to log in with. Connect to Red Hat. Here we will use the account created above.\nHere you will enter your account data and click Register. Press Done\nUnder Installation Destination choose your default drive.\nWe can now continue with the installation. A Begin installation button will appear on the main screen\nAfter installation is complete, we will have to reboot the system. Sometimes rebooting will unload the installation image again. It’s necessary to either disable the disk in the installer setup or reboot the UTM.\nRunning Red Hat Enterprise Linux 9 Enter your password and see the RHEL 9 desktop To access the applications, click the Activities button in the upper left corner\nConfiguring Red Hat Enterprise Linux 9 Checking the ROOT user In a Linux system users belong to different groups which have certain rights. If during the installation process we did not check the checkbox to make the user an administrator, by default he will not be able to install some system programs.\nExit and log in as root (the same user we created earlier on the main screen). Press Log out Now log in as root. The user may not be listed. Press Not listed and enter the account data. Open terminal and check Configuring system settings Button to minimize the application The first thing that seems unusual about using the GUI is that there are no buttons to minimize windows Install the necessary package\nyum install gnome-tweaks -y After installation, the Tweaks application will appear. Find it by searching.\nThere are many other tweaks in the app as well. We will show you the minimize buttons for the applications.\nLet’s go to Windows titlebars and set the Maximize, Minimize options\nUser access to install applications To avoid constantly switching to a root user to install applications, we can give the normal user access to install applications. We will continue to do this as root. Open /etc/sudoers and add the user\nsudo vi /etc/sudoers Add user data to the end of the file. My user name: rhel-user\nrhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf Let’s install Visual Studio Code as a normal user Installation consists of the following steps:\nadding the desired repository. Rights to add the repository (changing the files in the directory is still only for root user) Downloading and installing. First step is done as root user Go to https://code.visualstudio.com/docs/setup/linux\nCopy the code and run it in the terminal\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" \u003e /etc/yum.repos.d/vscode.repo' Switch to user rhel-user. This can also be done in the terminal. Updating the repositories Install VSCode su rhel-user dnf check-update sudo dnf install code References https://developers.redhat.com/products/rhel/getting-started https://www.redhat.com/sysadmin/install-linux-rhel-9 ","description":"How to Download and Install Linux RHEL 9 for Free","title":"How to Download and Install Linux RHEL 9 for Free","uri":"/en/posts/howto-install-rhel-9-free/"},{"content":"Learn different ways to rename files in Python using the os and pathlib modules.\nos.rename Rename files with os\nYou can use\nos.rename(old_name, new_name) For example we can combine it with os.path.splitext() to get the base name and file extension, and then combine it to a new name:\nimport os for file in os.listdir(): name, ext = os.path.splitext(file) new_name = f\"{name}_new{ext}\" os.rename(file, new_name) pathlib Rename files with pathlib\nThe same could be achieved with the pathlib module and\nPath.rename(new_name) With a Path object we can access .stem and .suffix:\nfrom pathlib import Path for file in os.listdir(): f = Path(file) new_name = f\"{f.stem}_new{f.suffix}\" f.rename(new_name) shutil.move The shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module.\nimport shutil old_source = '/Users/r/Desktop/old_source.txt' new_source = '/Users/r/Desktop/new_source.txt' newFileName = shutil.move(old_source, new_source) print(\"New file:\", newFileName) # New file: /Users/r/Desktop/new_source.txt ","description":"How to rename files in Python","title":"How to rename files in Python","uri":"/en/posts/howto-rename-files-in-python/"},{"content":"Introduction Hugo by default uses parsing of markdown files. This means that we get the html code as it is written in markdown.\nIn order to understand which images we can enhance, we add a separate tag/key/id to those images\nTools To implement the functionality, we need to:\nWrite/connect a script/handler that will perform the zoomin effect on the images we need Add the necessary metadata to the images, so the script can find them zoomin script To add the ability to zoom on click, we will use the medium-zoom package.\nThis package provides this functionality in a non-loaded, handy style.\nDemo\nScript logic The script finds images with id and so understands to apply the zoomin property to those images\nPossible id:\nzoom-default zoom-margin zoom-background zoom-scrollOffset zoom-trigger zoom-detach zoom-center Connecting the scripts In order for the script to work, we need to connect the logic as well as the handler.\nHugo has a static folder in the root of the project, which can be used to store static files (styles, scripts) and used to connect them to the site. If there is no such folder, you can create one.\nIn the static folder create a folder zoom-image and add two scripts to it\nstatic/js/zoom-image/index.js const zoomDefault = mediumZoom('#zoom-default') const zoomMargin = mediumZoom('#zoom-margin', { margin: 48 }) const zoomBackground = mediumZoom('#zoom-background', { background: '#212530' }) const zoomScrollOffset = mediumZoom('#zoom-scrollOffset', { scrollOffset: 0, background: 'rgba(25, 18, 25, .9)', }) // Trigger the zoom when the button is clicked const zoomToTrigger = mediumZoom('#zoom-trigger') const button = document.querySelector('#button-trigger') button.addEventListener('click', () =\u003e zoomToTrigger.open()) // Detach the zoom after having been zoomed once const zoomToDetach = mediumZoom('#zoom-detach') zoomToDetach.on('closed', () =\u003e zoomToDetach.detach()) // Observe zooms to write the history const observedZooms = [ zoomDefault, zoomMargin, zoomBackground, zoomScrollOffset, zoomToTrigger, zoomToDetach, ] // Log all interactions in the history const history = document.querySelector('#history') observedZooms.forEach(zoom =\u003e { zoom.on('open', event =\u003e { const time = new Date().toLocaleTimeString() history.innerHTML += `\u003cli\u003eImage \"\u003cem\u003e${event.target.alt }\u003c/em\u003e\" was zoomed at ${time}\u003c/li\u003e` }) zoom.on('detach', event =\u003e { const time = new Date().toLocaleTimeString() history.innerHTML += `\u003cli\u003eImage \u003cem\u003e\"${event.target.alt }\"\u003c/em\u003e was detached at ${time}\u003c/li\u003e` }) }) static/js/zoom-image/placeholders.js // Show placeholders for paragraphs const paragraphs = [].slice.call(document.querySelectorAll('p.placeholder')) paragraphs.forEach(paragraph =\u003e { // eslint-disable-next-line no-param-reassign paragraph.innerHTML = paragraph.textContent .split(' ') .filter(text =\u003e text.length \u003e 4) .map(text =\u003e `\u003cspan class=\"placeholder__word\"\u003e${text}\u003c/span\u003e`) .join(' ') }) CDN script You can download the script, or you can upload it\nScript Link\nAdding to template In order for these scripts to work in the website template, they must be connected.\nI use for this the template baseof.html. I simply add links to the scripts in body of the template.\n# baseof.html ... \u003c/footer\u003e \u003cscript src=\"https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js\" defer\u003e\u003c/script\u003e \u003cscript src=\"/js/zoom-image/placeholders.js\" defer\u003e\u003c/script\u003e \u003cscript src=\"/js/zoom-image/index.js\" defer\u003e\u003c/script\u003e \u003c/body\u003e \u003c/html\u003e image ID Hugo allows you to change the parsing behavior of markdown files with hooks. You can read more about render-hooks at website.\nIn the *layouts folder.\nLet’s add the file render-image.html to the following path layouts -\u003e _default -\u003e _markup file code:\n\u003cp class=\"md__image\"\u003e \u003cimg src=\"{{ .Destination | safeURL }}\" id=\"zoom-default\" alt=\"{{ .Text }}\" {{ with .Title}} title=\"{{ . }}\" {{ end }} /\u003e \u003c/p\u003e We only added id=\"zoom-default\" to the default code\nResult Your browser does not support the video tag. Process ","description":"Script will zoom in on a picture on click in Hugo","title":"Hugo resize a picture on click","uri":"/en/posts/hugo-add-image-zoomin/"},{"content":"","description":"Create lunr index file for multilingual hugo static site","title":"hugo-lunr-ml","uri":"/en/apps/npm/hugo-lunr-ml/"},{"content":"About IAM - AWS Identity and Access Management\nAWS IAM AWS IAM User Guide AWS Identity and Access Management (IAM) allows to securely control user access to AWS services and resources.\nDesigned for organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon RDS, and AWS Management Console.\nWith IAM, you can centrally manage users, security credentials such as access keys, and permissions that control user access to AWS resources.\nThere are three ways IAM authenticates a principal:\nUser Name/Password Access Key Access Key/Session Token Digest IAM consists of the following: Users Groups Roles Policy Documents IAM is Global. It doesn’t apply to any specific region. There is no charge to use IAM. IAM is compliant with Payment Card Industry (PCI) Data Security Standard (DSS) The “root account” has complete Admin access. Don’t use “root account” for everyday use. Instead, create users. A new user will have NO permissions by default. Grant least privilege needed for their job. New user will be assigned with password, Access Key ID \u0026 Secret Access Keys. The password will be used to login to AWS management console. Access Key ID \u0026 Secret Access Key will be used to login via the APIs and CLI Always setup MFA on your root account. Use Groups to assign permissions to IAM users Use Roles to Delegate permissions. Role is more secure than creating individual user. Roles gives temporary credentials for access; whereas User has long term credentials. Create and customize password rotation policies Policies can be attached to users, groups and roles. Use AWS defined policies, assign permissions wherever possible. Policy is defined in JSON format and contains version, statements, - effect, action, resource, principal, and condition. STS Security Token Service provides temporary security credentials to the trusted users. STS is global and there is no charge to use it. Digest: https://tutorialsdojo.com/aws-identity-and-access-management-iam/ IAM best practices - Question might ask you to identify best practices among the given choices. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Difference between when to use Role and User. IAM Policy Simulator - service for testing and troubleshooting IAM Policies. Details Practice Go to IAM page\nCreating IAM groups On the User Groups page, click Create group\nSpecify the name of the group. Mine is: DevOps. Add permission to view EC2: AmazonEC2ReadOnlyAccess. create The group was created\nCreating IAM users On the Users page, click Create user Type in user name (login) Permissions Add user to the group Tags Skip section or put tags. It is useful and popular to set tags for resources in companies with a lot of connected AWS resources\nLogin/Password At the last step, download the .csv file with login, keys and password. You will need the password later to log in as this user. On this page there is a link to log in. We will use it in the next step Logging in as a new user Checking privileges. This user has access to view EC2 instances. Let’s check whether or not the S3 garbage cans have access.\nLet’s try to create an S3 bucket After trying to create a recycle bucket, we get a window indicating no permissions Questions Q1 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\nWhat are the best practice guidelines for use of the root account?\nNever use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1\nQ2 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\nAssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1\nQ3 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\nWhich of the following APIs will be leveraged to provide temporary security credentials?\nAssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\n(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA\n3\nQ4 A leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC.\nThe access should allow the users to create and configure various AWS resources, such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user’s organization, including the master account email and organization limitations.\nWhich of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?\nAttach the PowerUserAccess AWS managed policy to the IAM users. Attach the AdministratorAccess AWS managed policy to the IAM users. Create a new IAM role and attach the SystemAdministrator AWS managed policy to it. Assign the IAM Role to the IAM users. Create a new IAM role and attach the AdministratorAccess AWS managed policy to it. Assign the IAM Role to the IAM users. Explanation AWS managed policies for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function.\nThese policies consolidate permissions for many services into a single policy that’s easier to work with than having permissions scattered across many policies.\nFor Developer Power Users, you can use the AWS managed policy name: PowerUserAccess if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development.\nThe first statement of this policy uses the NotAction element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role.\nThis is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user’s organization, including the master account email and organization limitations.\n1\nQ5 A company has 100 AWS accounts that are consolidated using AWS Organizations. The accountants from the finance department log in as IAM users in the TD-Finance AWS account. The finance team members need to read the consolidated billing information in the TD-Master AWS master account that pays the charges of all the member (linked) accounts. The required IAM access to the AWS billing services has already been provisioned in the master account.\nThe Security Officer should ensure that the finance team must not be able to view any other resources in the master account.\nWhich of the following grants the finance team the necessary permissions for the above requirement?\nSet up an IAM group for the finance users in the TD-Finance account then attach a ViewBilling permission and AWS managed ReadOnlyAccess IAM policy to the group. Set up individual IAM users for the finance users in the TD-Master account then attach the AWS managed ReadOnlyAccess IAM policy to the group with cross-account access. Set up an AWS IAM role in the TD-Finance account with the ViewBilling permission then grant the finance users in the TD-Master account the permission to assume that role. Set up an IAM role in the TD-Master account with the ViewBilling permission then grant the finance users in the TD-Finance account the permission to assume the role. Explanation You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Internet Services Pvt. Ltd (AISPL) accounts. Every organization in AWS Organizations has a master (payer) account that pays the charges of all the member (linked) accounts.\nModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyBilling – Allow or deny IAM users permission to modify billing settings. ModifyPaymentMethods – Allow or deny IAM users permission to modify payment methods. ViewAccount – Allow or deny IAM users permission to view account settings. ViewBilling – Allow or deny IAM users permission to view billing pages in the console. ViewPaymentMethods – Allow or deny IAM users permission to view payment methods. ViewUsage – Allow or deny IAM users permission to view AWS usage reports. Use policies to grant permissions to perform an operation in AWS. When you use an action in a policy, you usually allow or deny access to the API operation or CLI command with the same name. However, in some cases, a single action controls access to more than one operation.\n4\nResources Security best practices in IAM IAM Hands-On Lab IAM Workshops Security workshop tutorialsdojo digest Community posts https://dev.to/romankurnovskii/aws-iam-cheet-sheet-3if4 ","description":"A step-by-step guide to setting up AWS Identity and Access Management (IAM)","title":"IAM","uri":"/en/docs/aws-certified-developer-associate/iam/"},{"content":"Lab Initializing Amazon EC2 Instances with AWS CloudFormation Init Establishing Desired EC2 Instance State with AWS CloudFormation Init 1. In the AWS Console search bar, search for cloudformation and click the CloudFormation result under Services:\n2. Click the Create stack dropdown menu and select With new resources:\n3. In the Create stack form, in the Specify template section, ensure **Amazon S3 URL **is selected for the Template source.\n4. Paste in the following URL in the Amazon S3 URL field:\nAWSTemplateFormatVersion: '2010-09-09' Description: Provision a Single Amazon EC2 Instance with CFN Helper Scripts Parameters: AmiID: Description: The ID of the AMI. Type: AWS::SSM::Parameter::Value\u003cAWS::EC2::Image::Id\u003e Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 Resources: WebServer: Type: AWS::EC2::Instance Properties: ImageId: !Ref AmiID InstanceType: t3.micro SecurityGroupIds: - !Ref WebServerSecurityGroup UserData: # Update aws-cfn-bootstrap # Run cfn-init to initialize WebServer content # Return cfn-init run result to CloudFormation upon completion Fn::Base64: !Sub | #!/bin/bash -xe yum update -y aws-cfn-bootstrap /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region} /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region} CreationPolicy: ResourceSignal: Count: 1 Timeout: PT5M Metadata: AWS::CloudFormation::Init: config: packages: yum: httpd: [] files: \"/var/www/html/index.html\": content: | \u003ccenter\u003e \u003ch1\u003eCloud Academy EC2 Instance\u003c/h1\u003e \u003ch3\u003eThis content has been initialized with \u003ca href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html\" target=\"_blank\"\u003eAWS CloudFormation Helper Scripts\u003c/a\u003e\u003c/h3\u003e \u003c/center\u003e mode: '000644' services: sysvinit: httpd: enabled: 'true' ensureRunning: 'true' WebServerSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: SSH and HTTP SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 Outputs: WebServerPublicDNS: Description: Public DNS of EC2 instance Value: !GetAtt WebServer.PublicDnsName The CloudFormation stack template is stored in a public S3 bucket. The EC2 instance resource definition is shown below:\nThe WebServer EC2 instance is defined above. It is a size t3.micro instance that references a WebServerSecurityGroup resource for its security group and the AmiID parameter for its image ID. Both of these referenced configurations are defined elsewhere in the template. The UserData script defined next performs the following tasks once the EC2 instance is created:\nUpdates the aws-cfn-bootstrap package to retrieve the latest version of the helper scripts Runs the cfn-init helper script to execute the WebServer instance Metadata scripts Runs the cfn-signalhelper script to notify CloudFormation after all the service(s) (Apache in this case) is installed and configured on the EC2 instance Note: The cfn-init helper script is not executed automatically. You must run the cfn-init script within the EC2 instance UserData in order to execute your metadata scripts.\nThe cfn-signal helper script works hand-in-hand with the CreationPolicy configuration. The ResourceSignal property has a Count of 1 and a Timeout of PT5M. This instructs CloudFormation to wait for up to 5 minutes to receive 1 resource signal from the EC2 instance.\nThe cfn-signal helper script call in the UserData uses $? to retrieve the return code of the previous script. If the cfn-init script is successful and the EC2 instance is configured properly, cfn-signal returns a success to CloudFormation which then transitions the EC2 instance to the CREATE_COMPLETEstatus. If the cfn-init script is unsuccessful or the timeout of 5 minutes expires before receiving a signal, then the EC2 instance will be transitioned to a CREATE_FAILEDstatus and the stack deployment will fail. The EC2 instance Metadata configuration is the same as the previous lab step. It defines a AWS::CloudFormation::Init script to install the httpd package using yum, generate an index.html file within /var/www/html/ and start the httpd service to serve the content from the EC2 instance.\n5. Click **Next **to continue:\n6. Enter web-server-stack for the Stack name and click Next:\n7. You will not be configuring additional stack options. Scroll to the bottom of the page and click Next.\n8. On the review page, scroll to the bottom and click **Create stack **to deploy your stack:\nYour stack will begin deploying and you will be brought to the Events page of your web-server-stack:\nThe stack can take up to 3 minutes to deploy successfully. 9. If the Events section does not automatically refresh after 3 minutes, click the refresh icon:\nThe WebServer instance remains in a CREATE_IN_PROGRESS status until CloudFormation receives a SUCCESS signal from the instance. In the screenshot above, the UniqueId of i-0fd18c8deb52983d5 belongs to the WebServer instance. After the success signal is received, the WebServer instance is transitioned into the CREATE_COMPLETE status. Without the CloudFormation signal helper script, CloudFormation would have transitioned the EC2 instance to a completed status when the resource was created instead of waiting until the Apache service has been installed and running on the instance. 10. Click the Outputs tab on the web-server-stack page:\n11. Right-click and open the WebServerPublicDNS URL in a new browser tab:\nThe HTMLpage generated in the cfn-init script is now being served from the Apache server running within your WebServer EC2 instance:\n","description":"Initializing Amazon EC2 Instances with AWS CloudFormation Init","title":"Initializing Amazon EC2 Instances with AWS CloudFormation Init","uri":"/en/docs/aws-certified-developer-associate/cloudformation/initializing-ec2-with-cloudformation/"},{"content":"Lab Introduction to CloudWatch Explore CloudWatch 1. AWS has done an excellent job defining CloudWatch key concepts. Read the abbreviated excerpt from their official documentation below to obtain an understanding of Metrics, Namespaces and Alarms: Metrics\nA metric is the fundamental concept in CloudWatch and represents a time-ordered set of data points. These data points can be either your custom metrics or metrics from other services in AWS. You or AWS products publish metric data points into CloudWatch and you retrieve statistics about those data points as an ordered set of time-series data. Metrics exist only in the region in which they are created.\nThink of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another.\nNamespaces\nCloudWatch namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\nNote: In this lab you will see namespaces that AWS has created for you, and a custom namespace created by the steps performed in this lab.\nAlarms\nYou can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.\nAlarms invoke actions for sustained state changes only. CloudWatch alarms will not invoke actions simply because they are in a particular state. The state must have changed and been maintained for a specified number of periods.\nThe interested student can take a look at the full version of the documentation here. Due to time constraints, you should look at additional documentation once you have completed the lab.\n2. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n3. Click Metrics \u003e All metrics in the left navigation pane. At this point, there are most likely no custom namespaces. But several AWS namespaces may already be established for you. What metrics are listed on the **All metrics **tab depends on a couple of factors:\nHow quickly you arrived at this view after starting your lab. This lab creates an EC2 instance and EBS volume when you start the lab. After a couple of minutes of delay, metrics for the EC2 and EBS namespaces are included. How recently your Cloud Academy AWS account has been used to complete other Cloud Academy labs. If the AWS account you logged in to recently completed other labs, you may see namespace related to metrics collected in those labs. 4. Spend a few minutes to explore what metrics and namespaces look like in the CloudWatch console. Simply select any namespace and then any particular metric. As an example, the EC2 namespace and **CPUUtilization **metric for the HighCPUInstance are selected in the image below:\nNote: The image above is for illustrative purposes only, you do not need to choose the same instance or metric to explore CloudWatch metrics.\nThe longer the instance has been running, the more data points will appear in the graph. By default, EC2 metrics are collected every five minutes. You may need to adjust the displayed timeline to 1 week (1w) or further in the past to see some metrics.\nMonitoring EC2 Instances 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. Click **Instances **from the navigation pane and select the box near the instance name. A wealth of instance information is displayed in the Details tab:\nWhen you started the Lab, Cloud Academy configured the lab environment for you. This includes a medium instance named HighCPUInstance.\nNote: Your information will vary. There is additional instance information not shown in the example above.\n3. Switch to the Monitoring tab and take a look at the standard metrics:\nNote: If you don’t see an instance yet, it’s possible that it’s still provisioning in the background. Refresh the page every minute or so until it appears.\nThese are the standard metrics that CloudWatch monitors for all your EC2 instances. Please refer to the documentation for details. (Due to possible time constraints, please look up additional information in the documentation after completing this lab.)\nYou should be aware that all the metrics in this tab related to Disk (Disk Reads, Disk Read Operations, Disk Writes, Disk Write Operations) pertain to ephemeral storage disks. Those metrics will not represent anything if you have launched an EBS backed instance. To see the metrics related to EBS volumes you need to look elsewhere. Next you will take a look at the metrics of the EBS volume for this particular instance.\nNote: Ephemeral storage is also known as instance storage. It is temporary storage that is added to your instance, unlike EBS which is an attached volume that is permanent in nature.\n4. To enable and disable detailed monitoring, click Manage detailed monitoring:\nThe Detailed monitoring page will open :\nHere you can enable and disable detailed monitoring by checking or unchecking the Enable checkbox followed by clicking Save. 5. Click Cancel as we will not be enabling detailed monitoring in this lab:\n6. Reselect the HighCPUInstance , click the Storage tab. Scroll down and click on the Volume Id (lower right):\n7. Select the volume and click on the Monitoring tab to see the metrics for this EBS volume:\nAs you can see, Amazon does quite a bit out of the box with respect to monitoring EC2 Instances and EBS volumes. However, you can enable Detailed Monitoring for even more control over the monitoring frequency of EC2 instances. CloudWatch monitors EC2 instances every 5 minutes by default. If you need more frequent monitoring, you can enable CloudWatch’s Detailed Monitoring feature to monitor your instances every minute. You can enable Detailed Monitoring during the instance launch or change it anytime afterwards. Note: Detailed Monitoring does come with an associated cost.\nInstall the EC2 Monitoring Scripts 1. Navigate to EC2 Instances by clicking here.\n2. Click on Launch instances:\n3. In the Application and OS Images section, select the Amazon Linux option under Quick Start:\n4. In the Instance Type section, you should not change any options. Simply make sure the default **t2.micro **is selected:\n5. In the Key pair section, select the keypair:\nNote: Your keypair may differ from the screenshot. Reminder: The PEM or PPK formatted key pair can be downloaded directly from the Your lab data section of the Cloud Academy Lab page at any time.\n6. Scroll down and expand the Advanced details section. Under IAM instance profile, select the IAM role provided. It will have a name that looks similar to cloudacademylabs-EC2MonitoringRole-XXXXXXXXXX :\n7. Scroll down to Detailed CloudWatch monitoring and select Enable:\n8. Scroll down to User data and copy and paste the following bash script code in the User data (As text) field:\nThis is where the magic happens. Next you will insert the code to execute during the instance launch. However, in order to send metrics to CloudWatch, you need to configure some credentials first. You can use either Access Keys or IAM roles for this task. In this Lab, you will follow the best practices and use IAM roles. There is an instance role already created in you account configured with the proper permissions.\nCopy code\n#!/bin/bash yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 wget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip unzip CloudWatchMonitoringScripts-1.2.2.zip rm CloudWatchMonitoringScripts-1.2.2.zip echo “*/1 * * * * /aws-scripts-mon/mon-put-instance-data.pl –mem-util –disk-space-util –disk-path=/ –from-cron” \u003e monitoring.txt crontab monitoring.txt rm monitoring.txt\nThis bash script will get executed the first time the instances launches. In summary, the script will:\nInstall Perl libraries Retrieve and install the AWS CloudWatch Monitoring scripts Configure crontab to run the monitoring script every minute 9. In the Summary section, click Launch instance:\nA confirmation page will let you know that your instance is launching:\n10. Click View all instances.\nNotice the Name for the new instance is blank by default. Although not mandatory, it is helpful to have a name. Move your mouse into the blank space in the Name column. It turns to an edit pencil. Use the pencil to change your Instance Name to Monitoring Scripts:\nWait until the** Instance State** is R****unning for the new Instance. It typically takes less than one minute for the state to transition from P****ending to R****unning.\n11. Navigate back to **CloudWatch by clicking here and clickAll metrics **from the navigation pane. Notice that there is a new namespace called System/Linux under Custom namespaces:\nThis name is configured when you send the custom metrics.\nNote: If you don’t see the new Namespace wait a few minutes and refresh the page. CloudWatch takes some time to display the information in the dashboard. Recall that the newly installed monitoring scripts send data every minute based on the crontab configuration setup in the User data bash script for the instance.\n12. Click on the new System/Linux namespace:\nThere are two metrics being monitored by CloudWatch in the custom System/Linux namespace. (Filesystem, InstanceId, MountPath and InstanceId)\n13. Click the metric on the left (Filesystem…), then select the checkbox so the first metric is graphed.\n14. Click Linux System, so the Metrics path is All \u003e Linux System again. Now select the metric on the right (InstanceId) and select its checkbox as well. 15. Switch to the Graphed metrics tab. If you selected both metrics correctly the tab will include a “(2)” at the end of it indicating how many metrics are graphed. Your graph should look similar to the following:\nIt is simple to customize the display to meet your needs for the metrics displayed.\n16. Click the custom graph period drop-down above the graph display area and select **15 **from the **Minutes **row:\n17. Select the Period drop-down column menu for each metric in the lower Graphed metrics tab and choose 1 Minute:\nYou can now see the highest resolution metrics that are being sent to CloudWatch every minute. (You may need to refresh the chart after setting the new periods)\n18. Select Maximum for the **Statistic **column. Instead of an average of the datapoints, the maximum will be graphed. (Note: In the lab example it is probably the same since the disk really has not been touched) Your configuration should look like:\nCreating Your First CloudWatch Alarm 1. Navigate to CloudWatch by clicking here, click on Alarms \u003e All Alarms in the left pane:\nThere are no Alarms configured, so there are no records found. Further, the three types of Alarms are all at zero (0).\nNote: More information on Alarm states will be covered soon.\n2. Click Create Alarm and click Select metric. Select the EC2 namespace:\nMany different metrics are displayed for both the HighCPUInstance and the Monitoring Scripts instances.\n3. Click Per-instance metrics, scroll down and select the metric with **HighCPUInstance **under **Instance Name and CPUUtilization **under Metric Name:\nTip: You may need to use the arrows in the upper right to find the HighCPUInstance on another page. Alternatively, you can make note of the last 3 or 4 characters in the InstanceId from the EC2 console, then enter those in the Search Metrics field. The search applies to all pages of information.\nOnce selected it is graphed immediately. Notice that you could tailor the graph to a specific Time Range (upper-right). For example, the time range can be specified in Relative or Absolute terms. 4. Click Select metric when ready. 5. Under Conditions, set the following values leaving the defaults for the rest:\n_Whenever High CPU is…: _Greater/Equal Than…: 50 An alarm watches for a metric to go beyond an allowable value range when monitored over time. If violated the alarm’s state is changed. There are three possibles states for an alarm:\nOK—The metric is within the defined threshold\nIn alarm—The metric is outside of the defined threshold\nInsufficient data—The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state\n6. Click **Next **and fill out the form as described:\nAlarm state trigger: In alarm **Select an SNS topic: **Create new topic Insert a valid e-mail and click on Create topic.\n7. Click Next and fill the form as described before clicking Next:\nDefine a unique name: High CPU Alarm description: When CPU utilization \u003e= 50% Tip: Be sure to use your valid email address in the Email list field so you can verify the Alarm later. AWS Simple Notification Service (SNS) is used to send the email when the alarm is triggered. However, you will not need to configure anything in SNS.\n8. Click Create alarm when ready.\n9. Check for an email from AWS Notifications. Open up the email and click the Confirm subscription link:\nYou should receive a subscription confirmation. (For example, a confirmation message from Amazon Simple Notification Service (SNS) in a new browser tab if using a browser-based email client like Gmail.) To summarize, you have created a new alarm, along with a new SNS topic. Since you subscribed to this new SNS topic, every time the state of the alarm switches to ALARM you will receive an email. You may not receive an alarm email if the time it took to confirm the SNS topic subscription took longer than the time it took for the alarm to trigger. Emails will only be sent to subscribers at the time of the alarm transition.\n10. You should be put to the Alarm page:\nNote your Alarm state may differ. For example, you may be in an **Insufficient data **state briefly and then transition either to In alarm or OK. Troubleshooting Tip: If the state of your alarm does not change to In alarm almost immediately, it is probably because you picked the incorrect instance. The HighCPUInstance is designed to trigger an alarm due to a high CPU utilization metric. The Monitoring Scripts instance is not taxed at all. To remedy the situation you can either create a new alarm with the correct instance, lower the threshold to something artificially low (1), or change the \u003e= to \u003c= (which is not very realistic but will test the alarm).\n11. Click the Alarm. You can see very useful information about the alarm itself. In the Details tab, there is a general overview of the alarm, and in the History tab you can see up to the last 50 states of the alarm:\n12. After an **In alarm **state is raised, check for an email titled ALARM: “High CPU Alarm” in US West - Oregon from AWS Notifications.\nAgain, you may not have received an email because the alarm triggers before you had time to subscribe to the notification topic. Don’t worry, in the next Lab Step, you will reuse the topic for another alarm. Because you are already subscribed, you will receive an email. You could also retrigger the alarm by editing the alarm to trigger when CPUUtilization is \u003e= 500 (which can never happen for the single CPU instance). Wait five minutes until the alarm is disabled, then edit the alarm to trigger when CPUUtilization \u003e= 50.\n13. Now move to the History tab:\nYour History is likely similar to the example shown above. The oldest event is the furthest down. In succession, the Alarm was created; the state changed from INSUFFICIENT DATA to ALARM; SNS sent off an email notification.\n15. Spend a few minutes exploring the latest alarm history and try to understand what is going on with each entry. You can see more details for each entry by clicking the date.\nCreate an Alarm using the EC2 console 1. Navigate to EC2 Instances by clicking here.\n2. Select the Monitoring Scripts instance, then switch to the Status Checks tab:\n3. Click **Actions **\u003e Create Status Check Alarm:\nThis dialog is similar in function to the create alarm wizard you saw in an earlier Lab Step.\n4. For the Alarm notification, select the SNS topic name you set up before.\nOther fields can be kept at their defaults. The Alarm thresholds section uses Status check failed: either to trigger the alarm for either instance or system status check failures:\n5. Click Create when ready. An alarm creation confirmation message is displayed:\nNow you know two different ways to create alarms: one from CloudWatch and the other from the EC2 console. Next, you will learn how to attach EC2 actions to alarms.\n6. Return to the** Alarms by clicking here. **Notice that the first alarm you created is stuck in the In alarm state.\nThe alarm is stuck in the In alarm state because the instance is running an application that consumes 100% of the CPU utilization. Clearly an indicator that something may have gone wrong with the instance. Imagine that you are managing a production environment and you have an instance that is becoming unavailable intermittently because of high CPU utilization. You would like to receive a notification every time the CPU utilization is high, but this can happen anytime, in the middle of the night, or during a weekend or holiday. It would be helpful to have a pre-defined action in this case – at least until you find a definitive solution for the problem.\nTo help you address the scenario, you can set EC2 actions on your alarms. 8. Select the High CPU alarm and then Actions \u003e Edit:\nTo make your alarm more suitable to the training environment needs, set a new EC2 Action to Reboot this instance whenever the state of this alarm is ALARM.\n9. Click on Next and click on **Add EC2 action **under EC2 action. Select Reboot this instance.\n10. Click Update alarm when ready.\nAlthough the changes have been made to the alarm, the alarm remains in the In alarm state. CloudWatch will only perform actions when the state transitions to the **In alarm **state from another state. In the next instruction, you will modify the alarm to quickly have it change to the **OK **state and then change it back to return to the **In alarm **state. 11. Select the **High CPU **Alarm and choose Actions \u003e Edit. Toggle the relationship from \u003e= to \u003c= and click Update alarm:\n12. Refresh the page to ensure the alarm has transitioned to the **OK **state. Then toggle the condition back to \u003e= and save the alarm to have it transition to the In alarm state.\n_Note: _The state change may not be immediate and may take up to 2 minutes.\n13. Navigate back to the Instances by clicking here and watch CloudWatch reboot the instance when the Alarm Status changes to In alarm.\nIn case you miss it, you can return to the alarm in CloudWatch and see the Reboot Action listed in the **History **section:\n14. Check your email client and confirm that you received a notification of the alarm:\nSharing CloudWatch Metrics with others 1. Go to CloudWatch by clicking here and click on **Metrics **\u003e All metrics.\n2. Select an interesting metric, such as the DiskspaceUtilization metric for the Monitoring Scripts instance, and click Actions \u003e Share:\nThis metric can be found under** System/Linux** \u003e Filesystem, InstanceId, MountPath.\n3. In the Share Graph URL dialog, right-click and copy the URL, then Close the dialog:\n4. The URL for the specific graph you were looking at is copied into the clipboard. You can paste it into a test email to confirm this. For example:\nThe URL is quite complex. To confirm that it is indeed correct, test it out in another browser tab.\n5. Open another browser tab. Paste the URL into the address field and refresh your browser. You should see the exact same graph as the one you shared earlier. Notice that you need to be logged into the AWS console in order to view the information referenced by the URL. For security reasons, you can only share URLs with other AWS Identity and Access Management (IAM) users who have the appropriate CloudWatch IAM permissions in your AWS account.\n","description":"Introduction to CloudWatch","title":"Introduction to CloudWatch","uri":"/en/docs/aws-certified-developer-associate/cloudwatch/introduction-to-cloudwatch/"},{"content":"Lab https://cloudacademy.com/lab/introduction-codecommit/\nCreate a repository 1. In the AWS Management Console search bar, enter CodeCommit, and click the CodeCommit result under Services:\n2. Click Create repository:\n3. In the Create repository form enter the following values accepting the defaults for values not specified:\nRepository name: lab-repository You can leave the Description field empty for this lab. Usually this field would contain a short description of the purpose of the repository. Attaching meaningful descriptions to repositories makes managing large numbers of repositories easier.\n4. Click Create to create the repository.\nCreating credentials to access your repository 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n2. Under Access Management, click Users in the left-hand sidebar menu:\n3. In the IAM user list, click student:\n4. Click the Security credentials tab:\n5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section and click Generate credentials:\n6. In the box that opens, click Download credentials:\nYour browser will download a file called credentials.csv.\nKeep these credentials saved, you will use them in later steps.\nAccessing a shell with Git available 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. In the left-hand side menu, click Instances:\n3. Select the instance and in the row of buttons above the instance list, click Connect:\n4. In the Connect to instance dialog, ensure EC2 Instance Connect is selected and enter in the User name field:\n5. To open a shell on this instance, click Connect.\nEC2 Instance Connect allows you to connect to the instance over SSH using your web browser. With EC2 Instance Connect a new browser window opens an SSH shell on a Linux host that has git installed.\nKeep this window open, you will use it in later steps.\nAdding files to your repository 1. Navigate to the CodeCommit Console.\n2. In the list of repositories, click lab-repository:\n3. Click Clone URL and select Clone HTTPS in the drop-down menu that opens:\nThe URL of the repository has been copied to your clipboard.\n4. To copy the repository to your Linux host, in your shell, type git clone followed by a space and paste the repository URL: git clone https://git-codecommit.us-west-2.amazonaws.com/v1/repos/lab-repository 5. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\nYou can ignore the warning about cloning an empty repository.\nYou have copied the repository from AWS CodeCommit and stored it locally on the Linux host.\n6. To change to the directory of the repository, enter the cd command:\ncd lab-repository 7. To create a file, enter the following command:\necho \"lab\" \u003e lab.txt With this command you have created a file called lab.txt that can be added to your local repository.\n8. To add the file to your local repository, enter the following commands:\ngit add lab.txt git commit -m \"Lab commit\" In Git terminology, with the first command you are “staging” the file before you commit it. This process enables you to specify which files you want to add to the repository and which ones you want to ignore when committing.\nYou will see output similar to the following:\nYou can ignore the message about your name and email address. Usually when using Git you will configure the name and email address so that your commits are labeled with these details.\nYou have added the lab.txt file to your local copy of the repository on the Linux host.\nPushing your commit to your remote repository 1. In the shell on the Linux host, enter the following command:\ngit push In Git terminology, with this command you are “pushing” your local commit from your “local” repository to the “remote” repository that you “cloned” from.\n2. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\nYou have copied the file from the local repository on the Linux host, to the repository hosted in AWS CodeCommit.\n3. Navigate to the CodeCommit Console.\n4. In the Repositories list click lab-repository:\nYou will see the lab.txt file you pushed in the previous Lab step listed.\n","description":"Tutorial Introduction to CodeCommit","title":"Introduction to CodeCommit","uri":"/en/docs/aws-certified-developer-associate/codecommit/introduction-codecommit/"},{"content":"Lab\nCreating a DynamoDB Table with a Partition Key 1. From the AWS Management Console, in the search bar at the top, enter DynamoDB, and under Services, click the DynamoDB result:\nThe Amazon DynamoDB product overview page will load.\n2. To start creating a new DyanmoDB table, on the right-hand side, click Create table:\n3. In the Table details section, enter the following:\nTable Name: Partition Key: Enter Name and ensure type is 4. In the Settings section, select Customize settings:\nChoosing this option allows you to specify values for the table’s read and write capacities.\n5. In the Read/write capacity settings section, under Capacity mode, select Provisioned and enter the following:\nRead Capacity: Provisioned capacity units: Write Capacity: Provisioned capacity units: Accept the defaults for all other options on this page.\n6. Scroll to the bottom and click Create table:\nThe Tables list view will load and you will see a notification that your table is being created. After a 30 seconds or so, you will see a success notification:\nCreating a DynamoDB Table with Local and Global Secondary Indexes 1. On the right-hand side of the page, click Create table:\n2. Enter the following in the Table details section:\nTable name: Partition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select 3. In the Settings section, select Customize settings.\n4. Under Read/write capacity settings, ensure Provisioned is selected for Capacity mode, and enter the following:\nRead capacity: Provisioned capacity units: Write capacity: Provisioned capacity units: 5. Scroll down to the Secondary indexes section and click Create local index:\nThe New local secondary index dialog box will appear.\n6. Enter the following to configure your local secondary index:\nSort Key: Name: Enter Type: Select Attribute projections: Select An LSI (Local Secondary Index) has the same partition key as the table’s primary key and will share the provisioned capacity of the table in contrast to global secondary indexes which provision their own capacity.\n7. To finish creating the local secondary index, at the bottom, click Create index:\n8. Scroll to the bottom and click Create table.\nAfter roughly 30 seconds you will the table become active:\nIn contrast to a Local Secondary Index, a Global Secondary Index is an index with a partition and sort key that can be different from those in the table. It is considered “global” because queries on the index can span all of the data in a table, across all partitions.\n9. Click Create table once more to start creating another table.\n10. Enter the following in the Table details section:\nTable Name: Partition key: Name: Enter Type_: _Select Sort key: Name: Enter Type: Select 11. In the Settings section, select Customize settings.\n12. In the Read/write capacity settings section, ensure the Capacity mode is Provisioned, and enter the following:\nRead capacity: Provisioned capacity units: Enter Write capacity: Provisioned capacity units: Enter 13. Scroll down to the Secondary indexes section, and click Create global index:\nThe New global secondary index dialog form will appear.\n14. Enter the following:\nPartition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 15. To finish creating the global secondary index, at the bottom, click Create index.\n16. Click Create global index again and enter the following:\nPartition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 17. To finish creating the global secondary index, at the bottom, click Create index.\n18. Scroll to the bottom and click Create table.\nOnce again, you will see your table created after roughly 30 seconds.\nInserting Items Into a DynamoDB Table 1. In the left-hand menu, click Explore items:\n2. In the Tables list, select You will see nothing under Items returned because there are no items stored.\n3. On the right-hand side, click Create item:\nThe Create item form will load and you will see a list of Attributes.\n4. In the Value textbox next to Name - Partition key, enter a name for your forum (can be anything you wish):\n5. To add another attribute for this item, click Add new attribute and select String from the list of types:\n6. In the Attribute name textbox, enter Description and in the Value textbox, enter any value you’d like:\n7. At the bottom, click Create item:\n8. Repeat steps 3-7 three more times so that end up with four entries in the table:\n9. Select the table and click Create Item.\n10. Provide any values you’d like for , and , keeping in mind that the value must match the name of one of your forums.\nNote: is a \" \" table with the Local Secondary Index. For being able to save a item, you have to provide:\n(the table Primary Key) (the table Sort Key) (the Local Secondary Index Sort Key) Note: You will have to click Add new attribute to add the CreationDate attribute and specify a value.\n11. At the bottom, click Create item.\n12. Repeat steps 9-11 three more times until you have four items in the table:\nEditing DynamoDB Table Items 1. On the Explore items page, select the table:\n2. Select any item in the table and click on its name to get to the Item editor page:\n3. Click inside any value and make an update to its contents:\nWarning: Note that modifying the partition key will result in changing the values of the item keys. This will delete and recreate the item with new keys.\n4. At the bottom of the page, click Save changes:\nQuerying a DynamoDB Table 1. In the left-hand menu, click PartiQL editor:\nThe PartiQL editor page will load.\nPartiQL is a SQL (Structured Query Language) compatible language for Amazon DynamoDB. As well as querying tables, you can use it to insert new items and update existing ones.\n2. Under Tables, click the three dots next to the and click Scan table:\nThe Query 1 editor will be populated with a PartiQL query that selects all items from the .\n3. To execute the PartiQL table, under the editor, click Run:\n4. Scroll down to see the results under Items returned:\nNotice that you have a choice of viewing the results in tabular form or in JSON (Java Script Object Notation):\n5. To query for a specific item, replace the contents of the Query 1 editor with the following, and click Run:\nSELECT * FROM \"Thread\" WHERE \"Subject\" = 'Intro to cool stuff' This time, you will only see items returned that satisfy the value of the WHERE condition.\nNote: Change the value of the WHERE condition to match an item you created if you don’t see a result.\nPartiQL supports most standard features of SQL which means you can query, select, and sort your data in sophisticated ways.\nTypically, using the Amazon DynamoDB Console to query items is useful for one-off reports and debugging or troubleshooting. Like most databases, DynamoDB can be accessed programmatically by other systems and software applications through either the AWS SDK (software development kit) or DyanmoDB’s HTTP API (application programming interface).\nYou can learn more about using PartiQL with Amazon DynamoDB by visiting the Working with PartiQL Query Language section of the Amazon DynamoDB developer guide.\nDeleting a DynamoDB Table 1. In the left-hand menu, click Tables:\n2. In the Tables table, select the Thread table:\n3. On the right-hand side, click Delete:\nThe Delete table confirmation modal will appear.\nNotice that you have the ability to create a backup for a table before deleting it.\n4. In the confirmation textbox, enter delete and click Delete table:\nYou will see a message summarizing the deletion:\n5. To continue, click Go to tables:\n6. To update the Tables table, click the refresh icon:\nYou will now see only two tables listed.\n","description":"Creating a DynamoDB Amazon DynamoDB Table","title":"Introduction to DynamoDB","uri":"/en/docs/aws-certified-developer-associate/dynamodb/introduction-dynamodb/"},{"content":"Google maps Route\n","description":"Israel - Haifa - Bahai Gardens","title":"Israel - Haifa - Bahai Gardens","uri":"/en/posts/photos/22-07-02-israel-haifa-bahai-gardens/"},{"content":"Interim metrics still in process\nFor 2020:\nTime spent studying/practicing: ~5500 hours ","description":"Certified IT knowledge for the year 2020","title":"IT courses 2020","uri":"/en/posts/diploma/"},{"content":"Web / Browser get base URL const getBaseURL = url =\u003e url.replace(/[?#].*$/, ''); getBaseURL('http://url.com/page?name=Adam\u0026surname=Smith'); // 'http://url.com/page' const url = new URL(\"https://example.com/login?user=someguy\u0026page=news\"); url.origin // \"https://example.com\" url.host // \"example.com\" url.protocol // \"https:\" url.pathname // \"/login\" url.searchParams.get('user') // \"someuser\" get URL parameters as object const getURLParameters = url =\u003e (url.match(/([^?=\u0026]+)(=([^\u0026]*))/g) || []).reduce( (a, v) =\u003e ( (a[v.slice(0, v.indexOf('='))] = v.slice(v.indexOf('=') + 1)), a ), {} ); getURLParameters('google.com'); // {} getURLParameters('http://url.com/page?name=Adam\u0026surname=Smith'); // {name: 'Adam', surname: 'Smith'} // One line Object.fromEntries('http://url.com/page?name=Adam\u0026surname=Smith'.split('?')[1].split('\u0026').map(x=\u003ex.split('='))) if DOC element contains another element const elementContains = (parent, child) =\u003e parent !== child \u0026\u0026 parent.contains(child); elementContains( document.querySelector('head'), document.querySelector('title') ); // true elementContains(document.querySelector('body'), document.querySelector('body')); // false Date const {locale, timeZone} = Intl.DateTimeFormat().resolvedOptions(); is Date valid const isDateValid = (...val) =\u003e !Number.isNaN(new Date(...val).valueOf()); isDateValid('December 17, 1995 03:24:00'); // true isDateValid('1995-12-17T03:24:00'); // true isDateValid('1995-12-17 T03:24:00'); // false isDateValid('Duck'); // false isDateValid(1995, 11, 17); // true isDateValid(1995, 11, 17, 'Duck'); // false isDateValid({}); // false UNIX timestamp from Date const getTimestamp = (date = new Date()) =\u003e Math.floor(date.getTime() / 1000); getTimestamp(); // 1602162242 Login Secure Your Node.js App with JSON Web Tokens\nclient.ts // client.ts import axios, { AxiosInstance } from 'axios'; export class Client { private _client: AxiosInstance; constructor(accessToken?: string, url?: string) { const apiUrl = this.selectApiTarget(); let headers = {}; if (accessToken !== undefined) { headers = { 'Authorization': `Bearer ${accessToken}` }; } this._client = axios.create({ baseURL: url || apiUrl, headers: headers, }); } private selectApiTarget(): string { let backendUrl = config.backend.url; if (window.location.host.includes(\"node.sharedtodos.com\")) { backendUrl = config.backend.url.slice().replace(\"api.sharedtodos.com\", \"node-api.sharedtodos.com\"); } return `${backendUrl}/api/v1/`; } async getLoggedInUser(): Promise\u003cUser\u003e { return await this._client.get('/user/me').then((response) =\u003e response.data); } async forgetLoggedInUser(): Promise\u003cvoid\u003e { return await this._client.delete('/user/me').then((response) =\u003e response.data); } async getTasks(listId: number): Promise\u003cTask[]\u003e { return await this._client.get(`boards/${listId}/tasks`).then((response) =\u003e response.data); } async deleteTask(listId: number, taskId: number) { return await this._client.delete(`boards/${listId}/tasks/${taskId}`).then((response) =\u003e response.data); } async createTask(listId: number, title: string, description: string) { const task: Task = { title: title, description: description, }; return await this._client.post(`boards/${listId}/tasks`, task); } async updateTask(listId: number, taskId: string, task: Task) { return await this._client.put(`boards/${listId}/tasks/${taskId}`, task); } async login(email: string): Promise\u003cstring\u003e { let data = new FormData(); data.append('user_email', email); return await this._client.post(`login`, data, { headers: {'Content-Type': 'multipart/form-data' } }).then((response) =\u003e response.data.access_token); } } export const getClient = (accessToken?, url?): Client =\u003e new Client(accessToken, url); config.ts // config.ts interface ConfigOptions { backend: { url: string }; auth0: any; authentication: { provider: string }; authorization: { embedUrl: string }; } declare global { interface Window { _env_: any; } } const Config: ConfigOptions = { backend: { url: process.env.REACT_APP_BACKEND_URL || window?._env_?.BACKEND_URL || \"http://localhost:8008\", }, auth0: { domain: process.env.AUTH0_DOMAIN || window?._env_?.AUTH0_DOMAIN || \"acalla-demoapp.us.auth0.com\", clientId: process.env.AUTH0_CLIENT_ID || window?._env_?.AUTH0_CLIENT_ID || \"myClientID\", audience: process.env.AUTH0_AUDIENCE || window?._env_?.AUTH0_AUDIENCE || \"https://demoapi.server.com/v1/\", }, authentication: { provider: \"auth0\", }, authorization: { embedUrl: window?._env_?.AUTHZ_EMBED_URL || \"http://localhost:3000\", } }; export default Config; Resources react cheatsheet ","description":"JavaScript code snippets","title":"JavaScript code snippets","uri":"/en/posts/js-snippets/"},{"content":"About AWS Lambda AWS Lambda User Guide AWS Lambda is a serverless computing service that runs program code in response to certain events and is responsible for automatically allocating the necessary computing resources.\nAWS Lambda automatically runs program code in response to various events, such as HTTP requests through Amazon API Gateway, changing objects in Amazon Simple Storage Service garbage cans (Amazon S3), updating tables in Amazon DynamoDB or changing states in AWS Step Functions.\nSupports for Java, Go, PowerShell, Node.js, C#, Python and Ruby. It also provides a Runtime API which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function’s handler method when the function is invoked. You can include a runtime in your function’s deployment package in the form of an executable file named bootstrap\nWhen you publish a version, AWS Lambda makes a snapshot copy of the Lambda function code (and configuration) in the $LATEST version. A published version is immutable.\nLambda execution role is an IAM role that grants the function permission to access AWS services and resources. Under Attach permissions policies, choose the AWS managed policies AWSLambdaBasicExecutionRole and AWSXRayDaemonWriteAccess.\nAWS managed policies for Lambda features\nDigest Types of lambda invocation RequestResponse. Event. Dryrun. Lambda execution context is a temporary runtime environment that initializes any external dependencies of our Lambda function code, such as database connections or HTTP endpoints Lambda Environment variables are variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration. Lambda concurrent executions = (invocations per second) x (average execution duration in seconds). Concurrency limit of lambda execution, Default 1000 Reserved - 900 unreserved 100. Will get throttled if it exceeds concurrency limit AWS_PROXY in API gateway is primarily used for Lambda proxy integration. A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. Lambda authorizer can be used for custom authorization scheme. 2 types: Token based. Request parameter based Lambda authorizer. Lambda deployment configuration: HalfAtATime Canary Linear. AWS Lambda compute platform deployments cannot use an in-place deployment type Increasing memory in lambda will increase CPU in lambda Lambda Versioning: By default, each AWS Lambda function has a single current version of the code. Clients of Lambda function can call a specific version or at the latest implementation Lambda Alias: You can create one or more aliases for our AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency Lambda Layer - Layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package Amazon DynamoDB is integrated with AWS Lambda so that you can trigger pieces of code that automatically respond to events in DynamoDB Streams. AWSLambdaDynamoDBExecutionRole is required to enable Lambda to work with DynamoDB API Gateway - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. Integrating Cloud Watch Events with lambda can be used for scheduling events If there is an incompatible output returned from a Lambda proxy integration backend, it will return 502 To resolve lambda throttled exception when using Cognito events, perform retry on sync. Lambda Event hook running order: start -\u003e BeforeAllowTraffic -\u003e AllowTraffic -\u003e After AllowTraffic -\u003e End AWS Lambda runs function code securely within a VPC b default. To enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC Lambda Asynchronous invocation can be triggered by Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config. Lambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html Lambda provides 500 MB of additional disk space as a workspace. Lambda logs all stout for a lambda function to CloudWatch Logs. Any additional logging calls used in the function will also be sent to CloudWatch Logs. To connect to a VPC, lambda function execution role must have the following permissions: ec2:Create Networkinterface, ec2:DescribeNetworkinterfaces, ec2:Delete Networkinterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy When lambda execution is hit by concurrency limit, you need to request AWS to increase concurrency limit For stream-based services like Dynamo b streams, that don’t invoke Lambda functions directly, the event source mapping configuration should be made on the Lambda side. A deployment package is a ZIP archive that contains your function code and dependencies. You can unload the package directly to lambda. Or you can use an Amazon S3 bucket and then upload it to lambda. If the deployment package is larger than 50 MB. you must use Amazon 53 Lambda can incur a first run penalty also called cold starts. Cold starts can cause slower than expected behavior on infrequently run functions or functions with high concurrency demands Price Price\nPrice x86\n0.000016667 USD per gigabyte-second 0,20 USD per 1 million requests Arm price\n0,0000133334 USD for each gigabyte-second 0,20 USD for 1 million queries Practice In the AWS Management Console search bar, type Lambda and select Lambda under “Services”:\nhttps://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#\nOn page Functions click Create a function\nAuthor from scratch is selected and enter the following values in the bottom form:\nFunction name: *MyCustomFunc Runtime: Node.js 16.X I select this section because I use the cloudacademy account. This role gives you permission to create functions\nPermissions: Change default execution role. Execution Role: Select Use an existing role. Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole Create function I’m writing a function to view the log, I’ll add a print to the terminal. And I’ll also add processing of the message I receive (In the next step in the testing section)\nThe function takes as an object event which contains an array of Records. On the 1st (0) position the object Sns (name of the service SNS Notifications).\nIn the object itself there will be 2 values:\ncook_secs - cooking time (microwave) req_secs - cooking time (prepare) console.log('Loading function'); exports.handler = function(event, context) { console.log(JSON.stringify(event, null, 2)); const message = JSON.parse(event.Records[0].Sns.Message); if (message.cook_secs \u003c message.req_secs) { if (message.pre) { context.succeed(\"User ended \" + message.pre + \" preset early\"); } else { context.succeed(\"User ended custom cook time early\"); } } context.succeed(); }; Deploy Test This functionality allows you to test how the function reacts to certain events. Let’s try to add an event from SNS Notifications.\nLet’s choose from the list\nWe get a template in which we make some changes, adjust the field Message - the one that we will process in our function.\nField Message - string, so our object will need to be wrapped in quotes\nTo make the handler understand that we put quotation marks inside quotation marks, we must put a special symbol \\ before the quotation mark.\nFinally we update one line and save it - Create\nNow we click the Test button.\nSince cook_secs in our event was less than req_secs, the function printed the first condition, and below in Function Logs we see the message that we print when we initialize the Loading function\nQuestions Q1 When working with a published version of the AWS Lambda function, you should note that the _____.\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager\nC\nQ2 A developer is building a streamlined development process for Lambda functions related to S3 storage.The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\nWhich of the following AWS solutions offers is best suited for this objective?\nAWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model\n2\nQ3 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\nQ4 A developer is designing a web application that allows the users to post comments and receive a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ5 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\nResources Community posts ","description":"A step-by-step guide to AWS Lambda","title":"Lambda","uri":"/en/docs/aws-certified-developer-associate/lambda/"},{"content":"MacBook Pro Specification 13-inch Apple M1 Pro M1 2020 16 GB RAM 512 GB SSD QWERTY = English/Hebrew macOS Monterey (Update always) Homebrew Install Homebrew as package manager for macOS:\n## paste in terminal and follow the instructions /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Update everything in Homebrew to recent version:\nbrew update Add additional source for casks:\nbrew tap homebrew/cask-versions Install GUI applications (read more about these in GUI Applications):\nbrew install --cask \\ google-chrome \\ firefox \\ visual-studio-code \\ all-in-one-messenger \\ sublime-text \\ docker \\ rectangle \\ discord \\ vlc \\ figma \\ grammarly \\ macx-youtube-downloader \\ notion \\ postman \\ tor-browser \\ transmission \\ utm \\ viber \\ yandex-disk \\ zoom \\ mongodb-compass \\ disk-inventory-x \\ obs \\ spotify \\ iterm2 \\ deepl \\ syncthing Install terminal applications (read more about these in Terminal Applications):\nbrew install \\ git \\ ffmpeg \\ nvm \\ jupyterlab Additional GUI Applications Kotatogram Kotatogram - Experimental fork of Telegram Desktop. Folders with features\nGUI Applications Google Chrome Google Chrome (web development, web browsing)\nPreferences set default browser always show bookmarks import bookmarks from previous machine Chrome Developer Tools Network -\u003e only “Fetch/XHR” Search Shortcuts. Add Shortucts for different search engines. chrome://settings/searchEngines Yandex, search only in Russia. Shortcut: vv url: https://yandex.ru/{yandex:searchPath}?text=%s\u0026{yandex:referralID}\u0026lr=101443\u0026rstr=-225 Youtube Shortcut: yy url: https://www.youtube.com/results?search_query=%s\u0026page={startPage?}\u0026utm_source=opensearch Chrome Extensions Google Translate DeepL Translate - AI translator React Developer Tools Pocket Session Buddy (Manage Browser Tabs and Bokmarks) LanguageTool (multilingual grammar, style, and spell checker) RSS Feed Reader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) Inoreader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) 30 Seconds of Knowledge (random code snippet on a new tab) JSON Formatter picture-in-picture (yutube/video above other screens) Visual CSS Editor (Customize any website visually) Opus●Guide (Step-by-step for instructions) Disk Inventory X Disk Inventory X (disk usage utility for macOS)\nDocker Docker (Docker, see setup)\nused for running databases (e.g. PostgreSQL, MongoDB) in container without cluttering the Mac Preferences enable “Use Docker Compose” Firefox Firefox (web development)\nVisual Studio Code Visual Studio Code (web development IDE)\nSettings\nSublime Text Sublime Text (editor)\nMaccy Maccy (clipboard manager)\nenable “Launch at Login” OBS OBS (for video recording and live streaming)\nfor Native Mac Screen recorder Base (Canvas) 2880x1800 (Ratio: 16:10) Output 1728x1080 Spotify Spotify\nSyncthing syncthing - Sync folders/files between devices. I use to backup all photos/video from mobile to PC\nTransmission Transmission (A torrent client that I use. Very minimal in its UI but very powerful and has all the features that I need)\nUTM UTM (Virtual machines UI using QEMU)\ndownload ubuntu for arm, doc On error with shared folder: Could not connect: Connection refused open in browser: http://127.0.0.1:9843/ For Debian install spice-webdavd for shared folder. https://packages.debian.org/search?keywords=spice-webdavd, https://github.com/utmapp/UTM/issues/1204 sudo apt install spice-vdagent spice-webdavd -y VLC VLC (video player)\nuse as default for video files Terminal Applications nvm nvm (node version manager)\njupyterlab jupyterlab (Jupyter - python development, fast code snippets)\njupyter notebook - to start jupyter notebook ffmpeg ffmpeg (Converting video and audio)\ncompress video: ffmpeg -i input.mp4 -c:v libx264 -crf 23 -preset slow -c:a aac -b:a 192k output.mp4 # or ffmpeg -i input.mp4 output.avi convert video to .gif: - ffmpeg \\ -i input.mp4 \\ -ss 00:00:00.000 \\ -pix_fmt rgb24 \\ -r 10 \\ -s 960x540 \\ -t 00:00:10.000 \\ output.gif NVM for Node/npm The node version manager (NVM) is used to install and manage multiple Node versions. After you have installed it via Homebrew in a previous step, type the following commands to complete the installation:\necho \"source $(brew --prefix nvm)/nvm.sh\" \u003e\u003e ~/.zshrc source ~/.zshrc ## or alias ## zshsource Now install the latest LTS version on the command line:\nnvm install \u003clatest LTS version from https://nodejs.org/en/\u003e Afterward, check whether the installation was successful and whether the node package manager (npm) got installed along the way:\nnode -v \u0026\u0026 npm -v Update npm to its latest version:\nnpm install -g npm@latest And set defaults for npm:\nnpm set init.author.name \"your name\" npm set init.author.email \"you@example.com\" npm set init.author.url \"example.com\" If you are a library author, log in to npm too:\nnpm adduser That’s it. If you want to list all your Node.js installation, type the following:\nnvm list If you want to install a newer Node.js version, then type:\nnvm install \u003cversion\u003e --reinstall-packages-from=$(nvm current) nvm use \u003cversion\u003e nvm alias default \u003cversion\u003e Optionally install yarn if you use it as alternative to npm:\nnpm install -g yarn yarn -v If you want to list all globally installed packages, run this command:\nnpm list -g --depth=0 That’s it. You have a running version of Node.js and its package manager.\nOH MY ZSH MacOS already comes with zsh as default shell. Install Oh My Zsh for an improved (plugins, themes, …) experience. Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.\nInstall:\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Update everything (e.g. plugins) in Oh My Zsh to recent version:\nomz update Install fonts for themes:\nbrew tap homebrew/cask-fonts brew install --cask font-hack-nerd-font iTerm2 Install theme Theme description brew install romkatv/powerlevel10k/powerlevel10k echo \"source $(brew --prefix)/opt/powerlevel10k/powerlevel10k.zsh-theme\" \u003e\u003e~/.zshrc Enable suggestions git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions echo \"plugins=(zsh-autosuggestions)\" \u003e\u003e~/.zshrc Open new tab(CMD+T)/restart iTerm to proceed with theme setup\nTerminal Script and Aliases Update .zprofile. Еhe changes will take effect after restarting the terminal\nvi ~/.zprofile Automatic software updates Add script to zprofile that updates everything:\nUpdate, upgrade all and cleanup softwareupdate - system software update tool We can execute this command on strartup, but i prefer handle it. When I kick of upd command in terminal, it will update everythin I need:\nalias upd='brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i' Add aliases to latest versions pip \u0026 python\nalias pip=pip3 alias python=python3 Final view of .zprofile\n... alias pip=pip3 alias python=python3 alias upd='omz update; brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i' Links https://www.robinwieruch.de/mac-setup-web-development/ https://sourabhbajaj.com/mac-setup/iTerm/ack.html https://www.engineeringwithutsav.com/blog/spice-up-your-macos-terminal ","description":"How I set up my M1 MacBook Pro software development...","title":"Mac Setup 2022","uri":"/en/posts/mac-setup-development/"},{"content":"This article offers an example of the basic Markdown syntax that can be used and also shows whether the basic elements of HTML are decorated with CSS.\nHeaders Header 1 ======== Header 2 -------- Header 1 Header 2 # h1 ## h2 ### h3 #### h4 ##### h5 ###### h6 h1 h2 h3 h4 h5 h6 Paragraph To insert an empty string, you need to put the word wrap symbol twice (press Enter)\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit! Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae. Images ![Image alt text](/path/to/img.jpg) ![Image alt text](/path/to/img.jpg \"title\") ![Image alt text][img] [img]: http://foo.com/img.jpg Emphasis *italic* _italic_ **bold** __bold__ ***bold italic*** ___bold italic___ ~~strikethrough~~ `code` italic italic\nbold bold bold italic bold italic\nstrikethrough\ncode\nLinks [link](http://google.com) [link][google] [google]: http://google.com \u003chttp://google.com\u003e Blockquotes The blockquote element represents the content that is quoted from another source, optionally with a quotation that must be in the element footer or cite, and optional line changes such as annotations and abbreviations.\nBlock quote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Notethat you can use the syntax Markdown inside the block quote.\nBlock quote with authorship Don’t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n\u003eThis is an example quote, \u003ein which before each line \u003eangle bracket is used. \u003eThis is an example quote, in which the corner bracket is placed only before the beginning of the new paragraph. \u003eSecond paragraph. This is an example quote, in which before each line angle bracket is used.\nThis is an example quote, in which the corner bracket is placed only before the beginning of the new paragraph. Second paragraph.\n\u003e Level One Citation \u003e\u003e Second Level Citation \u003e\u003e\u003e Third Level Citation \u003e \u003eLevel One Citation Level One Citation\nSecond Level Citation\nThird Level Citation\nLevel One Citation\nTables | Name | Age | | ----- | --- | | Bob | 27 | | Alice | 23 | Name Age Bob 27 Alice 23 The cells in the delimitation row use only symbols - and :. The symbol : is placed at the beginning, at the end, or on both sides of the cell contents of the dividing row to indicate the alignment of the text in the corresponding column on the left, right side, or center.\n| Column on the left | Column on the right | Column on the center | | :----------------- | ------------------: | :------------------: | | Text | Text | Text | Column on the left Column on the right Column on the center Text Text Text Markdown inside the table | Italics | Bold | Code | | --------- | -------- | ------ | | *italics* | **bold** | `code` | Italics Bold Code italics bold code Code Blocks Code block with inverted quotes \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block with four spaces indent \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code Unit with Hugo Internal Shorted Backlight \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Lists * Item 1 * Item 2 - Item 1 - Item 2 - [ ] Checkbox off - [x] Checkbox on 1. Item 1 2. Item 2 Item 1 Item 2 Item 1\nItem 2\nCheckbox off\nCheckbox on\nItem 1 Item 2 Make the headers uniform. At the end of the title, do not put a point.\nCorrect Wrong Getting the Creating a Cluster Get the Creating a Cluster Get Create Cluster If you want to describe the sequence of actions, use the numbered list. At the end of the lines, put a period.\nIf the order of items is not important, use the marked list. Make it one of the ways:\nIf the entries in the list are separate sentences, start them with a capital letter and put a period at the end. If the introductory phrase and the list make up one sentence, the entries in the list should start with a lowercase letter and end with a semicolon. The last list item ends with a dot. If the list consists of parameter names or values (without explanation), do not put characters at the end of lines. Ordered list First item Second item Third item To create an ordered numbered list, use the digits with the symbol . or ). The recommended markup format is 1 and ..\n1. First item 1. Second item 1. Third item First item Second item Third item To create a nested ordered list, add a indent to the entries in the child list. The allowed indentation is from two to five spaces. The recommended indent size is four spaces.\nFor example, markup:\n1. First paragraph 1. Sub-paragraph 1. Sub-paragraph 1. Second paragraph First paragraph Sub-paragraph Sub-paragraph Second paragraph Unordered list List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other eleemnts - abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n💡 Data structure is a container that stores data in a specific format. This container decides how the outside world can read or change this data.\nThe above quote is taken from Rob Pike’s book talk during Gopherfest, November 18, 2015. ↩︎\n","description":"Markdown cheatsheet","title":"Markdown Cheat Sheet","uri":"/en/posts/markdown-syntax/"},{"content":"","description":"","title":"Posts","uri":"/en/docs/archive/"},{"content":"","description":"","title":"Posts Archive","uri":"/en/posts/archive/"},{"content":"Lab Process Amazon SNS Notifications with AWS Lambda Creating an Amazon SNS Topic 1. In the AWS Management Console search bar, enter SNS, and click the Simple Notification Service result under Services:\nIn the left-hand side menu, click Topics: If you can’t see the left-hand menu, to expand it, click the following:\nClick Create topic: In the Create topic form, ensure to have selected the Standard type, and enter the following values accepting the defaults for values not specified: Name: lab-topic You can leave the Display name field empty for this Lab. When you create topics where the recipients receive messages over SMS (Short Message Service) you are required to provide a value.\nAt the bottom of the form, click Create topic: Creating an AWS Lambda Function 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\nYou will see the Functions list page.\nClick Create function: In the Create function form, ensure Author from scratch is selected: In the Create function form, enter lab-function in the Function name field: In the Create function form, in the Runtime drop-down, select Python 3.8: In the Create function form, click Change default execution role and select Use an existing role: In the Existing role drop-down, select lambda_s3_put: The role you have selected has been pre-populated for this Lab. Usually when using Lambda you will create a specific role for your function.\nTo create your function, click Create function: Implementing an AWS Lambda Function to Upload to S3 Scroll down to the Code source section and double-click lambda_function.py.\nIn the code editor, replace the contents with the following Python code:\nfrom datetime import datetime import boto3 account_id = boto3.client('sts').get_caller_identity()[\"Account\"] s3 = boto3.resource('s3') def lambda_handler(event, context): record = event['Records'][0]['Sns'] message = record['Message'] subject = record['Subject'] print(\"Subject: %s\" % subject) print(\"Message: %s\" % message) s3.Object(f\"sns-lab-bucket-{account_id}\", subject).put(Body=message) return \"SUCCESS\" The function code you entered processes a message from SNS. The code uploads a file into an S3 Bucket which was pre-created as a part of this lab. The name of the file will be the subject of the message and the content of the file will be the message body.\nYou can use a Lambda function to do many different things. Some examples include:\nProcess web-requests Put a custom metric into AWS CloudWatch Add or update a record in a database Post a web-request to an external service To save your changes and deploy your function, at the top of the Code source section, click Deploy: You will see a notification that your function has been deployed:\n4. To add an SNS trigger, in the Function overview section, click Add trigger:\n​\nIn the Select a trigger dropdown, enter SNS, and click the SNS result: ​\nIn the SNS topic drop-down, select lab-topic: ​\nThe SNS topic field will be filled with the ARN (Amazon Resource Name) of your SNS topic.\nTo add your SNS trigger, click Add: ​\nYou will see a notification that your trigger has been added:\nIn SNS terminology, by adding an SNS trigger you have “subscribed” your Lambda function to the SNS topic.\nPublishing a Message to an Amazon SNS Topic Navigate to the AWS SNS service.\nIn the left-hand side menu, click Topics:\nIn the list of topics, click lab-topic: Click Publish message: In the Message details section of the form, in the Subject field, enter lab-subject: In the Message body section of the form, in the Message body to send to the endpoint textbox, enter Lab Message: Usually when you publish a message to an SNS topic, you would include meaningful data in the message body. The content of the message body is often called the “payload” of a message. In SNS, the payload can be plain text, or it can be a structured payload such as JSON, XML, or some other format. The service or device subscribed to your topic can use the data in the payload to determine what action to take in response to receiving a message.\nTo publish your message, click Publish message: You will see a notification, similar to the following, confirming your message has been published:\nVerifying the AWS Lambda Function Processed the Message 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\nIn the list of S3 Buckets, click the Bucket beginning with sns-lab-bucket-: In the list of objects you will see a file called lab-subject: This file was uploaded to the S3 bucket by your Lambda function.\n","description":"Process Amazon SNS Notifications with AWS Lambda","title":"Process Amazon SNS Notifications with AWS Lambda","uri":"/en/docs/aws-certified-developer-associate/sns/aws-lambda-sns-notifications/"},{"content":"On this page you can find 45 random questions.\nQ1 - Q10 Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\nHTTP proxy mock AWS service proxy Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\nAmazon API Gateway supports mock integrations for API methods.\n2\nQ2 You are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.\nWhat steps can you take in this situation? (Choose 2 answers)\nUse a template to first create Resource A with the ARN as an output value. Use a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt. Hard code the ARN value output from creating Resource A into the second template. Just create Resource B. Explanation http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n2\nQ3 A company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.\nWhich API command can a developer use to ensure cache storage consistency with minimal duplication?\nCreateReusableDelegationSet with Route 53 CreateStackSet with CloudFormation CreateGlobalReplicationGroup with ElastiCache CreateCachePolicy with CloudFront Explanation https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html\n4\nQ4 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\nWhat is the easiest way to develop these simple applications?\nInstall AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model\nhttps://aws.amazon.com/serverless/sam/\n1\nQ5 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\nCan delete only if unused.\n1\nQ6 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n3\nQ7 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\nQ8 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\nWhat action will best solve this issue?\nUse AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n3\nQ9 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\nQ10 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ11 - Q20 1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.\nmemory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation 4\n2 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\nThe metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to ElastiCache is required for test or development purposes, it can be done through a VPN.\n2\n3 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\n4 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\nWhich of the following options would meet this requirement?\nUse an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n2\n5 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\nWhich AWS service should be used to refactor this architecture?\nAWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4\n6 Which load balancer would you use for services which use HTTP or HTTPS traffic?\nExplanation Application Load Balancer (ALB). 7 What are possible target groups for ALB (Application Load Balancer)?\nExplanation EC2 tasks ECS instances Lambda functions Private IP Addresses 8 Your would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.\nWhich solution below would be best to implement for this?\nEnable X-Forwarded For on the web servers and use a Classic Load Balancer Configure proxy servers to forward the traffic to the correct instances Use Classic Load Balancer with path-based routing rules to forward the traffic to the correct instances Use Application Load Balancer with host-based routing rules to forward the traffic to the correct instances Explanation Application Load Balancer with host-based routing rules\nhttps://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\n4\n9 A company uses Amazon DynamoDB for managing and tracking orders. DynamoDB table is partitioned based on the order date. The company receives a huge increase in orders during a sales event, causing DynamoDB writes to throttle, and the consumed throughput is below the provisioned throughput.\nAccording to AWS best practices, how can this issue be resolved with MINIMAL costs?\nCreate a new Dynamo DB table for every order date Add a random number suffix to the partition key values Add a global secondary index to the DynamoDB table Increase the read and write capacity units of the DynamoDB table Explanation A randomizing strategy can greatly improve write throughput. But it’s difficult to read a specific item because you don’t know which suffix value was used when writing the item.\nChoosing the Right DynamoDB Partition Key\nUsing write sharding to distribute workloads evenly\n2\n10 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\nQ21 - Q30 1 How AWS Fargate different from AWS ECS?\nExplanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don’t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n2 What is Chaos Engineering?\nExplanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n3 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\nWhat are the best practice guidelines for use of the root account?\nNever use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1\n4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\nEnable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template\n4\n5 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\nAssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1\n6 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\nWhich of the following APIs will be leveraged to provide temporary security credentials?\nAssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\n(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA\n3\n7 You built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.\nWhat steps will you take to keep a strong performance?\nRemove the hot shards Merge the hot shards Split the hot shards Increase the shard capacity Explanation https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\nSplit the hot shards\n3\n8 Jasmin needs to perform ad-hoc business analytics queries on well-structured dat1. Data comes in constantly at a high velocity. Jasmin’s team can understand SQL.\nWhat AWS service(s) should Jasmin look to first?\nEMR using Hive EMR running Apache Spark Kinesis Firehose + RDS Kinesis Firehose + RedShift Explanation RedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol\nhttps://aws.amazon.com/kinesis/data-firehose/features/\n4\n9 Key rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?\nKMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key. KMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys. Key rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed. Key rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key. Explanation When you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key’s older cryptographic material so it can be used to decrypt data that it has encrypted.\n10 Alan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.\nWhich statements are true with regard to this scenario? (Choose 2 answers)\nCreate Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts. Can share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys. Make encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts. You can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used. Explanation https://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n1, 2\nQ31 - Q40 1 When working with a published version of the AWS Lambda function, you should note that the _____.\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager\n3\n2 A Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.\nWhich of the following enables monitoring of the metric in Amazon CloudWatch?\nRetrieve the log data from AWS CloudTrail using the LookupEvents API Call Retrieve the log data from CloudWatch using the GetMetricData API call Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application Install the Amazon CloudWatch logs agent on the EC2 instance that the application is running on Explanation 4\n3 A developer is building a streamlined development process for Lambda functions related to S3 storage. The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\nWhich of the following AWS solutions offers is best suited for this objective?\nAWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model\n2\n4 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically 5 Developer wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.\nWhich methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)\nControl access based on the time of day Control access based on IP Address Control access based on Active Directory group Control access based on CIDR block Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html CIDRs - A set of Classless Inter-Domain Routings\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html\n1, 2, 4\n6 To ensure that an encryption key was not corrupted in transit, Elastic Transcoder uses a(n) ____ digest of the decryption key as a checksum.\nBLAKE2 SHA-1 SHA-2 MD5 Explanation https://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html\nMD5 digest (or checksum)\n4\n7 Dan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.\nWhat are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)\nUse the encrypted SSL/TLS endpoint. Enable encryption in the bucket policy. Encrypt it on the client-side before uploading. Set the server-side encryption option on upload. Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n1, 3\n8 In a move toward using microservices, a company’s Management team has asked all Development teams to build their services so that API requests depend only on that services data store. One team is building a Payments service that has its own database. The service floods data that originates in the Accounts database. Both are using Amazon DynamoDB.\nWhat approach will result in the simplest, decoupled, and reliable method to get near-real-time updates from the Accounts database?\nUse Amazon Glue to perform frequent updates from the Accounts database to the Payments database Use Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database. Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database. Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database. Explanation 3\n9 You’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.\nWhich options are supported notification methods? (Choose 3 answers)\nHTTP or HTTPS POST notifications Email using SMTP or plain text Kinesis Stream Invoking of a Lambda function Explanation https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html\n1, 2, 4\n10 Which endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?\nSNS E-Mail SQS Kinesis Explanation https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html\n3\nQ41 - Q50 1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\nWhich of the following Step Function workflows can be used to meet requirements?\nAsynchronous Express Workflow Synchronous Express Workflow Standard Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n2\n2 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\nWhich AWS service will be your main tool?\nAWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks\n1\n3 A team of Developers must migrate an application running inside an AWS Elastic Beastalk environment from a Classic Load Balancer to an Application Load Balancer.\nWhich steps should be taken to accomplish the task using the AWS Management Console?\n1 Select a new load balancer type before running the deployment. Update the application code in the existing deployment. Deploy the new version of the application code to the environment. 2 Create a new environment with the same configurations except for the load balancer type. Deploy the same application versions as used in the original environment. Run the Swap-environment-cnames action. 3 Clone the existing environment, changing the associated load balancer type. Deploy the same application version as used in the original environment. Run the Swap-environment-cnames action. 4 Edit the environment definitions in the existing deployment. Change the associated load balancer type according to the requirements. Rebuild the environment with the new load balancer type. Explanation 4\n4 A developer is deploying an application that will store files in an Amazon S3 bucket. The files must be encrypted at rest. The developer wants to automatically replicate the files to an S3 bucket in a different AWS Region for disaster recovery.\nHow can the developer accomplish this task with the LEAST amount of configuration?\nEncrypt the files by using server-side encryption with S3 managed encryption keys (SSE-S3). Enable S3 bucket replication. Encrypt the files by using server-side encryption (SSE) with an AWS Key Management Service (AWS KMS) customer master key (CMK). Enable S3 bucket replication. Use the s3 sync command to sync the files to the S3 bucket in the other Region. Configure an S3 Lifecycle configuration to automatically transfer files to the S3 bucket in the other Region. Explanation 2\n5 A serverless application is using AWS Step Functions to process data and save it to a database. The application needs to validate some data with an external service before saving the dat1. The application will call the external service from an AWS Lambda function, and the external service will take a few hours to validate the dat1. The external service will respond to a webhook when the validation is complete.\nA developer needs to pause the Step Functions workflow and wait for the response from the external service.\nWhat should the developer do to meet this requirement?\nUse the .waitForTaskToken option in the Lambda function task state. Pass the token in the body. Use the .waitForTaskToken option in the Lambda function task state. Pass the invocation request. Call the Lambda function in synchronous mode. Wait for the external service to complete the processing. Call the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing. Explanation 4\n6 A company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.\nWhat is causing the duplicate entries?\nThe S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket. The Lambda function did not run correctly, and Lambda retried the invocation with a delay. Amazon S3 is delivering the same event multiple times. The application stopped intermittently and then resumed, splitting the logs into multiple smaller files. Explanation 1\n7 An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function.\nHow can the developer inspect the timing of the DynamoDB API calls?\nAdd DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs Limit Lambda to no more than five concurrent invocations. Monitor from the Lambda console. Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service. Explanation 4\n8 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance. How can the application find this information?\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1\n9 A developer is creating a serverless application that uses an AWS Lambda function The developer will use AWS CloudFormation to deploy the application. The application will write logs to Amazon CloudWatch Logs. The developer has created a log group in a CloudFormation template for the application to use. The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime.\nWhich solution will meet this requirement?\nUse the AWS::Include transform in CloudFormation to provide the log group’s name to the application. Pass the log group’s name to the application in the user data section of the CloudFormation template Use the CloudFormation template’s Mappings section to specify the log group’s name for the application. Pass the log group’s Amazon Resource Name (ARN) as an environment variable to the Lambda function. Explanation 4\n10 A developer needs to use the AWS CLI on an on-premises development server temporarily to access AWS services while performing maintenance. The developer needs to authenticate to AWS with their identity for several hours.\nWhat is the MOST secure way to call AWS CLI commands with the developer’s IAM identity?\nSpecify the developer’s IAM access key ID and secret access key as parameters for each CLI command. Run the AWS configure CLI command. Provide the developer’s IAM access key ID and secret access key. Specify the developer’s IAM profile as a parameter for each CLI command. Run the get-session-token CLI command with the developer’s IAM user. Use the returned credentials to call the CLI. Explanation 4\nQ51 - Q60 1 Explanation 2 Explanation 3 Explanation 4 Explanation 5 Explanation 6 A developer notices timeouts from the AWS CLI when the developer runs list commands.\nWhat should the developer do to avoid these timeouts?\nUse the --page-size parameter to request a smaller number of items. Use shorthand syntax to separate the list by a single space. Use the yaml-stream output for faster viewing of large datasets. Use quotation marks around strings to enclose data structure. Explanation 1\n7 A company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.\nWhich CodeDeploy predefined configuration will meet these requirements?\nCodeDeployDefault.ECSCanary10Percent15Minutes CodeDeployDefault.LambdaCanary10Percent5Minutes CodeDeployDefault.LambdaCanary10Percent15Minutes CodeDeployDefault.ECSLinear10PercentEvery1 Minutes Explanation 1\n8 Explanation 9 A microservices application is deployed across multiple containers in Amazon Elastic Container Service (Amazon ECS). To improve performance, a developer wants to capture trace information between the microservices and visualize the microservices architecture.\nWhich solution will meet these requirements?\nBuild the container from the amazon/aws-xray-daemon base image. Use the AWS X-Ray SDK to instrument the application. Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices. Install the AWS X-Ray daemon on each of the ECS instances. Configure AWS CloudTrail data events to capture the traffic between the microservices. Explanation 3\n10 A company is running an application on Amazon Elastic Container Service (Amazon ECS). When the company deploys a new version of the application, the company initially needs to expose 10% of live traffic to the new version. After a period of time, the company needs to immediately route all the remaining live traffic to the new version.\nWhich ECS deployment should the company use to meet these requirements?\nRolling update Blue/green with canary Blue/green with all at once Blue/green with linear Explanation 2\n","description":"AWS exam questions Certified Developer 2022","title":"Questions","uri":"/en/docs/aws-certified-developer-associate/questions/"},{"content":"About Relational Database Service Managed DB service that uses SQL as query language Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.\nDocumentation User Guide Supports engines:\nAmazon Aurora with MySQL compatibility: 5432 Amazon Aurora with PostgreSQL compatibility: 5432 MySQL: 3306 MariaDB: 3306 PostgreSQL: 5432 Oracle: 1521 SQL Server: 1433 Engine modes:\nUsed in CreateDBCluster\nglobal parallelquery serverless multimaster Backups Backups are enabled by default in RDS Automated backups\nDaily full backup (during maintenance window) Backups of transaction logs (every 5 minutes) 7 days retention (can increase upto 35) DB Snapshots\nManually triggered by the user Can retain backup as long as you want Auto scaling When RDS detects you’re running out of space, it scales automatically Digest To verify slowly running queries enable slow query log. TDE (Transparent data encryption) supports encryption on Microsoft SQL server AWS Systems Manager Parameter Store provides secure, hierarchical storage for confiquration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values AWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases. Price Current price\nUse Cases Type: Relational\nThis type services: Aurora, Redshift, RDS\nEcommerce websites, Traditional sites etc.\nAmazon Relational Database Service (Amazon RDS) on [AWS Outposts](AWS Outposts) allows you to deploy fully managed database instances in your on-premises environment\nQuestions Q1 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically Q2 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\nWhich approach meets these requirements?\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager\nSecrets Manager offers secret rotation\n4\nQ3 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically ","description":"Set up, operate, and scale a relational database in the cloud with just a few clicks.","title":"RDS","uri":"/en/docs/aws-certified-developer-associate/rds/"},{"content":"About Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-premises.\nDocumentation User Guide A highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing, and health checking.\nCan create and manage your public DNS records.\nWhat is the difference between Route 53 and DNS?\nYour DNS is the service that translates your domain name into an IP address. AWS Route 53 is a smart DNS system that can dynamically change your origin address based on load, and even perform load balancing before traffic even reaches your servers.\nAlternatives Cloudflare DNS. Google Cloud DNS. Azure DNS. GoDaddy Premium DNS. DNSMadeEasy. ClouDNS. UltraDNS. NS1. Routing Policies Simple routing policy – route internet traffic to a single resource that performs a given function for your domain. You can’t create multiple records that have the same name and type, but you can specify multiple values in the same record, such as multiple IP addresses.\nFailover routing policy – use when you want to configure active-passive failover.\nGeolocation routing policy – use when you want to route internet traffic to your resources based on the location of your users.\nGeoproximity routing policy – use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.\nYou can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. The effect of changing the bias for your resources depends on a number of factors, including the following: The number of resources that you have. How close the resources are to one another. The number of users that you have near the border area between geographic regions. Latency routing policy – use when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency.\nIP-based routing policy – use when you want to route traffic based on your users’ locations, and know where the IP address or traffic is coming from.\nMultivalue answer routing policy – use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.\nWeighted routing policy – use to route traffic to multiple resources in proportions that you specify.\nWhen you register a domain or transfer domain registration to Route 53, it configures the domain to renew automatically. The automatic renewal period is typically one year, although the registries for some top-level domains (TLDs) have longer renewal periods.\nWhen you register a domain with Route 53, it creates a hosted zone that has the same name as the domain, assigns four name servers to the hosted zone, and updates the domain to use those name servers.\nDigest Route 53 is AWS DNS service Map domain names to EC2 instances, Load Balancers and 53 buckets Routing Policies Simple - Traffic routed to a single resource Weighted - Traffic routed to a resource = weight assigned to the resource/sum of all the weights Latency - serves requests from the AWS region with low latency Geographical - routes the traffic based on the location of the request origin Failover - routes traffic to primary when primary healthy; secondary when primary is unhealthy Multivalue Answer - routs randomly to multiple healthy resources VPC - private network on AWS platform Subnet, NAT Instance, NAT Gatewav, Internet Gatewav, NACLs, Route Table VPC Wizard Single public subnet Public and Private subnet (NAT) Public and private subnet and AWS managed VPN access Private subnet only and AWS managed VPN access VPC Peering - helps transfer of data VPC Flow logs - helps capture information about incoming/outgoing traffic Direct Connect - dedicated connection from on premises network to VPC Price Pay only for what you use.\nCurrent price\nUse Cases Domain Registration / transfer Manage network traffic globally Set up private DNS ","description":"Amazon Route 53 - A reliable and cost-effective way to route end users to Internet applications","title":"Route 53","uri":"/en/docs/aws-certified-developer-associate/route53/"},{"content":"mongodb free tier vs documentdb\nProject structure - src - app.py - mongo.py - .env - requirements.txt - serverless.yml Sources\nAdd AIM user Setup specific user for serverless deployment\nusername: serverless-deployer\naws aim documentation Set policy Create:\nServerLessDeployerPolicyGroup ServerLessDeployerPolicy Policy:\n{ \"Statement\": [ { \"Action\": [ \"apigateway:*\", \"cloudformation:CancelUpdateStack\", \"cloudformation:ContinueUpdateRollback\", \"cloudformation:CreateChangeSet\", \"cloudformation:CreateStack\", \"cloudformation:CreateUploadBucket\", \"cloudformation:DeleteStack\", \"cloudformation:Describe*\", \"cloudformation:EstimateTemplateCost\", \"cloudformation:ExecuteChangeSet\", \"cloudformation:Get*\", \"cloudformation:List*\", \"cloudformation:UpdateStack\", \"cloudformation:UpdateTerminationProtection\", \"cloudformation:ValidateTemplate\", \"dynamodb:CreateTable\", \"dynamodb:DeleteTable\", \"dynamodb:DescribeTable\", \"dynamodb:DescribeTimeToLive\", \"dynamodb:UpdateTimeToLive\", \"ec2:AttachInternetGateway\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CreateInternetGateway\", \"ec2:CreateNetworkAcl\", \"ec2:CreateNetworkAclEntry\", \"ec2:CreateRouteTable\", \"ec2:CreateSecurityGroup\", \"ec2:CreateSubnet\", \"ec2:CreateTags\", \"ec2:CreateVpc\", \"ec2:DeleteInternetGateway\", \"ec2:DeleteNetworkAcl\", \"ec2:DeleteNetworkAclEntry\", \"ec2:DeleteRouteTable\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteSubnet\", \"ec2:DeleteVpc\", \"ec2:Describe*\", \"ec2:DetachInternetGateway\", \"ec2:ModifyVpcAttribute\", \"events:DeleteRule\", \"events:DescribeRule\", \"events:ListRuleNamesByTarget\", \"events:ListRules\", \"events:ListTargetsByRule\", \"events:PutRule\", \"events:PutTargets\", \"events:RemoveTargets\", \"iam:AttachRolePolicy\", \"iam:CreateRole\", \"iam:DeleteRole\", \"iam:DeleteRolePolicy\", \"iam:DetachRolePolicy\", \"iam:GetRole\", \"iam:PassRole\", \"iam:PutRolePolicy\", \"iot:CreateTopicRule\", \"iot:DeleteTopicRule\", \"iot:DisableTopicRule\", \"iot:EnableTopicRule\", \"iot:ReplaceTopicRule\", \"kinesis:CreateStream\", \"kinesis:DeleteStream\", \"kinesis:DescribeStream\", \"lambda:*\", \"logs:CreateLogDelivery\", \"logs:CreateLogGroup\", \"logs:DeleteLogGroup\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:FilterLogEvents\", \"logs:GetLogEvents\", \"logs:PutSubscriptionFilter\", \"s3:CreateBucket\", \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:ListAllMyBuckets\", \"s3:ListBucket\", \"s3:PutBucketNotification\", \"s3:PutBucketPolicy\", \"s3:PutBucketTagging\", \"s3:PutBucketWebsite\", \"s3:PutEncryptionConfiguration\", \"s3:PutObject\", \"sns:CreateTopic\", \"sns:DeleteTopic\", \"sns:GetSubscriptionAttributes\", \"sns:GetTopicAttributes\", \"sns:ListSubscriptions\", \"sns:ListSubscriptionsByTopic\", \"sns:ListTopics\", \"sns:SetSubscriptionAttributes\", \"sns:SetTopicAttributes\", \"sns:Subscribe\", \"sns:Unsubscribe\", \"states:CreateStateMachine\", \"states:DeleteStateMachine\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ], \"Version\": \"2012-10-17\" } Create user copy the API Key \u0026 Secret\nNeed during setup aws cli/serverless\nCreate serverless.yml In the root folder create:\norg: romankurnovskii app: app-name service: app-service-name frameworkVersion: '3' useDotenv: true custom: wsgi: app: src/app.app packRequirements: false provider: name: aws deploymentMethod: direct region: eu-west-1 runtime: python3.9 architecture: arm64 versionFunctions: false memorySize: 128 functions: api: handler: wsgi_handler.handler events: - httpApi: '*' environment: MONGO_CONNECTION_STRING: ${env:MONGO_CONNECTION_STRING} MONGO_COLLECTION_DB_NAME: ${env:MONGO_COLLECTION_DB_NAME} package: patterns: - '!.dynamodb/**' - '!.git/**' - '!.vscode/**' - '!.env' - '!node_modules/**' - '!tmp/**' - '!venv/**' - '!__pycache__/**' plugins: - serverless-wsgi - serverless-python-requirements Create Flask app Prerequisites python -m venv ./venv source ./venv/bin/activate App src/app.py\nfrom flask import Flask, ObjectId, request, jsonify, make_response from flask_cors import CORS import json from src.mongo import my_db users_collection = my_db.users app = Flask(__name__) cors = CORS(app) @app.route(\"/\", methods=['GET']) def get_user(user_id): user_id = request.args.get('id') user = users_collection.find_one({\"_id\": ObjectId(user_id)}) if not user: return jsonify({'error': 'data not found'}), 404 return jsonify({'user': user}) @app.route('/', methods=['PUT']) def create_record(): record = json.loads(request.data) user_id = record.get('user_id', None) users_collection.update_one({\"_id\": ObjectId(user_id)}, record) @app.route(\"/\") def hello(): return jsonify(message='Hello!') @app.errorhandler(404) def resource_not_found(e): return make_response(jsonify(error='Not found!'), 404) def internal_server_error(e): return 'error', 500 app.register_error_handler(500, internal_server_error) src/mongo.py\nimport os from pymongo import MongoClient MONGO_CONNECTION_STRING = os.environ.get( \"MONGO_CONNECTION_STRING\", default=\"mongodb://localhost:27017/\" ) MONGO_COLLECTION_DB_NAME = os.environ.get( \"MONGO_COLLECTION_DB_NAME\", default=\"test-mydb\" ) db_client = MongoClient(MONGO_CONNECTION_STRING) my_db = db_client[MONGO_COLLECTION_DB_NAME] .env\nMONGO_CONNECTION_STRING=mongodb+srv://login:password@cluster0.XXXXX.mongodb.net/mydb?retryWrites=true\u0026w=majority MONGO_COLLECTION_DB_NAME=mydb src/requirements.txt\ncertifi==2022.6.15 charset-normalizer==2.1.1 click==7.1.2 dnspython==2.2.1 ecdsa==0.18.0 Flask==1.1.4 Flask-Cors==3.0.10 idna==3.3 importlib-metadata==4.12.0 itsdangerous==1.1.0 Jinja2==2.11.3 jmespath==1.0.1 MarkupSafe==2.0.1 pyasn1==0.4.8 pymongo==4.2.0 python-dateutil==2.8.2 python-dotenv==0.20.0 requests==2.28.1 rsa==4.9 six==1.16.0 urllib3==1.26.12 Werkzeug==1.0.1 zipp==3.8.1 Deployment serverless login install dependencies with:\nnpm install and\npip install -r requirements.txt and then perform deployment with:\nserverless deploy After running deploy, you should see output similar to:\nDeploying app-service-name to stage dev (eu-west-1) ✔ Service deployed to stack app-service-name (182s) Local development Thanks to capabilities of serverless-wsgi, it is also possible to run your application locally, however, in order to do that, you will need to first install werkzeug dependency, as well as all other dependencies listed in requirements.txt. It is recommended to use a dedicated virtual environment for that purpose. You can install all needed dependencies with the following commands:\nAlready in requirements.txt:\npip install werkzeug pip install -r requirements.txt At this point, you can run your application locally with the following command:\nserverless wsgi serve For additional local development capabilities of serverless-wsgi plugin, please refer to corresponding GitHub repository.\n","description":"Create AWS serverless application on Flask + API Gateway + Lambda + MongoDB","title":"Serverless: Flask+API Gateway+Lambda+MongoDB","uri":"/en/posts/serverless-flask-lambda-api-gateway-mongodb/"},{"content":"Lab Sessionizing Clickstream Data with Amazon Kinesis Data Analytics Creating an Amazon Kinesis Data Analytics Application 1. In the AWS Management Console search bar, enter Kinesis, and click the Kinesis result under Services:\nYou will be taken to the Amazon Kinesis dashboard.\nIn this lab, a Kinesis Data Stream has been pre-created for you. Under Data Streams you will see Total data streams is one:\n2. In the left-hand menu, click Analytics applications and under that click SQL applications:\n3. To start creating a Kinesis Data Analytics application, under Data Analytics, click Create SQL application (legacy):\nYou will be taken to the Create legacy SQL application form.\n4. In the Application configuration section, and enter lab-application in the Application name textbox:\n5. At the bottom of the page, click Create legacy SQL application:\nYou will be taken to a page displaying details of your application and you will see a notification that your application has been created:\nYou will come back to this page later in the lab to connect the pre-created Kinesis Data Stream as a data source for your Kinesis Data Analytics application.\n6. To navigate to the Kinesis Data Streams list page, in the left-hand side menu, click Data streams:\nYou will see one data stream listed called lab-stream.\n7. To view the details of the pre-created data stream, in the list, click lab-stream:\nYou will be taken to the Stream details page and you will see a series of tabs with Monitoring selected.\n8. To see the configuration details of the data stream, click Configuration:\nTake a moment to look at the details on this page, there are several Kinesis Data Stream configuration options that you should be aware of:\nData Stream capacity: The number of shards in the Data Stream. Each shard has a maximum read and write capacity. To increase the total capacity of a data stream you can add shards. Encryption: Kinesis Data Streams can be encrypted using an AWS managed or customer-managed, KMS key. Data retention: A Kinesis Data Stream can retain data for a configurable amount of time between 24 and 168 hours. Enhanced (shard-level) metrics: More detailed CloudWatch metrics can be enabled for a Data Stream, these enhanced metrics have an extra cost. In this lab, you will be working with a small amount of sample data, so there is one shard configured.\nLeave these options without changing them.\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nSimulating a Real-Time Clickstream 1. To create a template JSON file for a click event, enter the following command into the shell:\necho '{ \"user_id\": \"$USER_ID\", \"event_timestamp\": \"$EVENT_TIMESTAMP\", \"event_name\": \"$EVENT_NAME\", \"event_type\": \"click\", \"device_type\": \"desktop\" }' \u003e click.json There are two parts to this command, the first uses the built-in Bash command echo to print a JSON template. The second part uses a feature of the Bash shell called redirection, it redirects the output of the echo command to a file (creating it if doesn’t exist) called click.json.\nThe template contains five fields, the event_type, and device_type fields are hardcoded, in a non-lab environment, you may encounter streams that come from different types of devices and streams that contain more than one type of event (clickstream events alongside sales or transaction data for example). The other fields will be populated dynamically.\n2. To put records into Kinesis and simulate a clickstream, enter the following command:\nUSER_IDS=(user1 user2 user3) EVENTS=(checkout search category detail navigate) for i in $(seq 1 3000); do echo \"Iteration: ${i}\" export USER_ID=\"${USER_IDS[RANDOM%${#USER_IDS[@]}]}\"; export EVENT_NAME=\"${EVENTS[RANDOM%${#EVENTS[@]}]}\"; export EVENT_TIMESTAMP=$(($(date +%s) * 1000)) JSON=$(cat click.json | envsubst) echo $JSON aws kinesis put-record --stream-name lab-stream --data \"${JSON}\" --partition-key 1 --region us-west-2 session_interval=15 click_interval=2 if ! (($i%60)); then echo \"Sleeping for ${session_interval} seconds\" \u0026\u0026 sleep ${session_interval} else echo \"Sleeping for ${click_interval} second(s)\" \u0026\u0026 sleep ${click_interval} fi done You will see the templated JSON and also the JSON response from Kinesis for each record put into the Data Stream:\nThis command simulates a real-time click-stream with the following characteristics:\nCreates three thousand events Events have a two-second interval between them After every sixty events (two minutes) there is a fifteen-second interval, later you will assume a gap of ten seconds or more is a session boundary The command has a number of parts:\nSetup of sample user ids and event types at the beginning A loop that will execute three thousand times and a sleep statement Statements that randomly select a user id and an event type, and assign them along with the current timestamp to variables A statement that uses the envsubst command to substitute defined environment variables in the JSON template A statement invoking the AWS command-line interface tool, putting the templated JSON record into the Kinesis Data Stream A condition at the end of the loop that either sleeps for a few seconds or, periodically for longer, simulating the end of a session Leave the command running.\nNavigate to Kinesis Data Analytics in the AWS Management Console. 4. In the list of applications, to expand the application, click lab-application:\n5. To connect your Data Analytics application to the pre-created Data Stream, click **Configure under Source stream **form:\nThe Configure source for lab-application form will load.\n6. Under Source, ensure Kinesis data stream is selected:\n7. In the Kinesis data stream, click Browse to select the radio button for lab-stream and click Choose:\n8. Under Access permissions, select Choose from IAM roles that Kinesis Data Analytics can assume:\n9. In the IAM role list, select the role beginning with cloudacademy-lab-data-analytics:\nIf you don’t see the above role listed click the refresh button:\n10. To start discovering the schema of the records you added to the Data Stream, click Discover schema:\nAfter a moment or two, you will see a notification that the discovery was successful and below, some of the records will be displayed:\n11. To finish connecting your Data Analytics application to your Data Stream, click Save changes:\nYou will be redirected to the page for your Kinesis Data Analytics application. Leave this page open in a browser tab.\nSessionizing the Clickstream Data using Amazon Kinesis Data Analytics 1. Return to the page for your Kinesis Data Analytics application in the AWS Management Console.\n2. To start your application and expand the Steps to configure your application, click Configure SQL:\n3. In the SQL code editor, replace the existing contents with the following SQL commands\nCREATE OR REPLACE STREAM \"INTERMEDIATE_SQL_STREAM\" ( \"event_timestamp\" TIMESTAMP, \"user_id\" VARCHAR(7), \"device_type\" VARCHAR(10), \"session_timestamp\" TIMESTAMP ); CREATE OR REPLACE PUMP \"STREAM_PUMP1\" AS INSERT INTO \"INTERMEDIATE_SQL_STREAM\" SELECT STREAM TO_TIMESTAMP(\"event_timestamp\") as \"event_timestamp\", \"user_id\", \"device_type\", CASE WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) \u003e (10 * 1000) THEN TO_TIMESTAMP(\"event_timestamp\") WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) IS NULL THEN TO_TIMESTAMP(\"event_timestamp\") ELSE NULL END AS \"session_timestamp\" FROM \"SOURCE_SQL_STREAM_001\"; These statements do the following:\nDefines an intermediate stream to insert data into called INTERMEDIATE_SQL_STREAM Creates a PUMP that selects data from the source stream The SELECT statement uses the LAG function to determine if there is a ten-second interval between the last event and the current event The LAG function statements are used with PARTITION statements to restrict the LAG function by the user You should know that Kinesis Data Analytics natively assumes Unix timestamps include milliseconds. The stream you simulated is providing timestamps with milliseconds. This is why the CASE WHEN statement that checks for a ten-second interval includes (10 * 1000), it’s multiplying ten by one thousand to get ten seconds in milliseconds.\nTip: you can increase the height of the SQL editor text-box by dragging the grey bar at the bottom.\n4. To execute the SQL statements, click Save and run application:\nThe query will take up to a couple of minutes to execute and start returning results.\nOccasionally you may see an error caused by the fifteen-second interval, if you do, re-run the query by clicking Save and run application again.\nTake a look at the results. Notice that only some records have a value for session_timestamp. This is because the CASE WHEN statement in the query supplies a value of null when:\nThe interval between event timestamps is less than ten seconds There is no preceding event Also notice that below the SQL Code editor, there are two streams, the INTERMEDIATE_SQL_STREAM, and an error_stream. The error stream is where any errors that occur during the execution of the SQL will be delivered to.\n5. In the SQL editor window, under the current SQL statements, add the following:\nCREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\" ( \"user_id\" CHAR(7), \"session_id\" VARCHAR(50), \"session_time\" VARCHAR(20), \"latest_time\" VARCHAR(20) ); CREATE OR REPLACE PUMP \"STREAM_PUMP2\" AS INSERT INTO \"DESTINATION_SQL_STREAM\" SELECT STREAM \"user_id\", \"user_id\"||'_'||\"device_type\"||'_'||TIMESTAMP_TO_CHAR('HH:mm:ss', LAST_VALUE(\"session_timestamp\") IGNORE NULLS OVER (PARTITION BY \"user_id\" RANGE INTERVAL '24' HOUR PRECEDING)) AS \"session_id\", TIMESTAMP_TO_CHAR('HH:mm:ss', \"session_timestamp\") AS \"session_time\", TIMESTAMP_TO_CHAR('HH:mm:ss', \"event_timestamp\") AS \"latest_time\" FROM \"INTERMEDIATE_SQL_STREAM\" WHERE \"user_id\" = 'user1'; These SQL statements do the following:\nCreates a stream called DESTINATION_SQL_STREAM Creates a PUMP that selects from the INTERMEDIATE_SQL_STREAM Constructs a session_id by combining the user, device type and time Restricts the query to user1 using a WHERE clause Something else to note about these statements is that the session and event timestamps are being converted to times.\n6. To run the updated query, click Save and run application.\nYou will see results similar to:\nYour times will be different.\nNotice that the session_time values are more than ten seconds apart. And that the seconds’ interval of the latest_time column between the rows that have a session time, is ten seconds or less.\n7. To see only the rows for new sessions, replace the last line of the query with the following:\nWHERE \"session_timestamp\" IS NOT NULL; This change to the WHERE clause of the last SQL statement removes the restriction of the query to user1, and removes rows where the value of session_timestamp is null.\n8. Click Save and run application to re-run your query.\nYou will see results similar to the following:\nYour results will be different.\nThe results now contain only session boundary rows for each of the users.\nLeave this browser tab open with the query running in Kinesis Data Analytics.\nCreating an AWS Lambda function to Store Sessions in an Amazon DynamoDB Table 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\n2. To start creating your function, click Create function:\n3. Under Create function, ensure Author from scratch is selected:\n4. Under Basic information, in the Function name text-box, enter lab-function:\n5. In the Runtime drop-down, select the latest Python 3.x version available.\n6. To expand the role selection form, click Change default execution role.\n7. Under Execution role, select the Use an existing role radio button:\n8. To assign an execution role, in the Existing role drop-down, select the role called cloudacademy-lab-lambda:\n9. To create your function, click Create function:\nYou will be taken a page where you can configure your function, and you will see a notification that your function has been successfully created:\n10. Scroll down to the Code source section and in the code editor double-click the lambda_function.py file.\n11. To update your Lambda function’s implementation, replace the code in the editor window with the following:\nfrom __future__ import print_function import boto3 import base64 from json import loads dynamodb_client = boto3.client('dynamodb') table_name = \"CloudAcademyLabs\" def lambda_handler(event, context): payload = event['records'] output = [] success = 0 failure = 0 for record in payload: try: payload = base64.b64decode(record['data']) data_item = loads(payload) ddb_item = { 'session_id': { 'S': data_item['session_id'] }, 'session_time': { 'S': data_item['session_time'] }, 'user_id': { 'S': data_item['user_id'] } } dynamodb_client.put_item(TableName=table_name, Item=ddb_item) success += 1 output.append({'recordId': record['recordId'], 'result': 'Ok'}) except Exception: failure += 1 output.append({'recordId': record['recordId'], 'result': 'DeliveryFailed'}) print('Successfully delivered {0} records, failed to deliver {1} records'.format(success, failure)) return {'records': output} This python code processes a record from Kinesis Data Analytics and puts it into a DynamoDB table.\nThe implementation is based on one provided by AWS. The only change is the statements that construct the ddb_item. They have been modified to match the data being supplied by your Kinesis Data Analytics application.\n12. To deploy your function, at the top, click Deploy:\nYou will see a notification that your function has been deployed:\n13. To configure a timeout for your function, click the Configuration tab, and click Edit:\n14. Under Timeout, enter 1 in the min text-box, and 0 in the sec text-box:\nYou are updating the timeout because the default of three seconds is too low when processing data from Kinesis Data Analytics, and may lead to failures caused by the function timing out. AWS recommends setting a higher timeout to avoid such failures. 15. To save your function’s updated timeout, click Save:\nYou will see a notification that your change to the timeout has been saved:\nConfiguring Amazon Kinesis Data Analytics to Use Your AWS Lambda Function as a Destination 1. Navigate to Kinesis Data Analytics in the AWS Management Console.\n2. In the list of applications, to expand the application, click lab-application:\n3. To begin configuring your Lambda as a destination, expand the Steps to configure your application and click Add destination:\nThe Configure destination form will load.\n4. Under Destination select AWS Lambda function:\n5. Under AWS Lambda function, click Browse and check radio box for lab-function followed by clicking Choose:\nThis is the Lambda function you created in the previous lab step.\n6. Under Access permissions, ensure Choose from IAM roles that Kinesis Data Analytics can assume is selected:\n7. In the IAM role drop-down, select the role called cloudacademy-lab-lambda:\nThis is a role that has been pre-created for this lab and allows Kinesis Data Analytics to invoke your Lambda function.\n8. In the In-application stream section, under Connect in-application stream, select Choose an existing in-application stream:\n9. In the In-application stream name drop-down, select DESTINATION_SQL_STREAM:\n10. To finish connecting your Kinesis Data Analytics application to your Lambda function, click Save changes:\nYour Kinesis Data Analytics application is being updated. Please be aware that it can take up to three minutes to complete.\nOnce complete the details page for Kinesis Data Analytics application will load.\n11. In the AWS Management Console search bar, enter DynamoDB, and click the DynamoDB result under Services:\n12. In the left-hand menu, click Tables:\n13. In the list of tables, click CloudAcademyLabs:\nThis table was pre-created as a part of this lab.\n14. To see items in the DynamoDB table, click the **Explore Table **Items button:\nYou will see the items in the table listed similar to:\nThese items have been inserted into the DyanmoDB table by your Lambda function, it’s being invoked by your Kinesis Data Analytics application.\n","description":"Sessionizing Clickstream Data with Amazon Kinesis Data Analytics","title":"Sessionizing Clickstream Data with Amazon Kinesis Data Analytics","uri":"/en/docs/aws-certified-developer-associate/kinesis/sessionizing-clickstream-data-kinesis-data-analytics/"},{"content":"About AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services.\nDocumentation User Guide Step Functions is a serverless function orches­trator that makes it easy to sequence Lambda functions \u0026 multiple AWS services into busine­ss-­cri­tical applic­ations.\nAlternatives AWS lambda Airflow Google Cloud Workflows Microsoft Flow Price Pay only for what you use\nCurrent price\nFree Tier: 4,000 state transitions per month\nUse Cases Step Functions is an easy-to-use function orchestra that makes it possible to string Lambda functions and multiple AWS services into business-critical applications.\nStep Functions manages the operations and underlying infrastructure for you to ensure your application is available at any scale.\nWith Step Functions, you are able to easily coordinate complex processes composed of different tasks.\nWithout using this service you have to coordinate each Lambda Function yourself and manage every kind of error in all steps of this complex process.\nAWS Step Functions is a useful service for breaking down complex processes into smaller and easier tasks\nAutomate Extract, Transform, and Load (ETL) process Orchestrate microservices Workflow configuration AWS service integrations Component reuse Built-in error handling Type: Orches­tration, Workflows\nStep Function Standard Workflows are optimized for long-running processes.\nExpress Workflows are better for event-driven workloads.\nPractice Introduction to AWS Step Functions\nQuestions Q1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\nWhich of the following Step Function workflows can be used to meet requirements?\nAsynchronous Express Workflow Standard Workflow Synchronous Express Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n3\n","description":"Amazon Step Functions","title":"Step Functions","uri":"/en/docs/aws-certified-developer-associate/step-functions/"},{"content":"Common options z\tcompress with gzip c\tcreate an archive u\tappend files which are newer than the corresponding copy ibn the archive f\tfilename of the archive v\tverbose, display what is inflated or deflated a\tunlike of z, determine compression based on file extension Create tar named archive.tar containing directory tar cf archive.tar /path/files Concatenate files into a single tar tar -cf archive.tar /path/files Extract the contents from archive.tar tar xf archive.tar Create a gzip compressed tar file name archive.tar.gz tar czf archive.tar.gz /path/files Extract a gzip compressed tar file tar xzf archive.tar.gz Create a tar file with bzip2 compression tar cjf archive.tar.bz2 /path/files Extract a bzip2 compressed tar file tar xjf archive.tar.bz2 List content of tar file tar -tvf archive.tar ","description":"tar command Cheat Sheet","title":"Tar command Cheat Sheet","uri":"/en/posts/cheat-sheet-command-tar/"},{"content":"most popular docker images ## lists the images docker pull imagename ## Pull an image or a repository from a registry docker ps -a ## See a list of all containers, even the ones not running docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \"imagename\" mapping port 4000 to 80 docker rmi ## removes the image docker rm ## removes the container docker stop ## stops the container docker volume ls ## lists the volumes docker kill ## kills the container docker logs ## see logs docker inspect ## shows all the info of a container docker docker cp ## Copy files/folders between a container and the local filesystem docker pull imagename ## Pull an image or a repository from a registry docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \"imagename\" mapping port 4000 to 80 docker run -d -p 4000:80 imagename ## Same thing, but in detached mode docker exec -it [container-id] bash ## Enter a running container docker ps ## See a list of all running containers docker stop \u003chash\u003e ## Gracefully stop the specified container docker ps -a ## See a list of all containers, even the ones not running docker kill \u003chash\u003e ## Force shutdown of the specified container docker rm \u003chash\u003e ## Remove the specified container from this machine docker rm -f \u003chash\u003e ## Remove force specified container from this machine docker rm $(docker ps -a -q) ## Remove all containers from this machine docker images -a ## Show all images on this machine docker rmi \u003cimagename\u003e ## Remove the specified image from this machine docker rmi $(docker images -q) ## Remove all images from this machine docker top \u003ccontainer-id\u003e ## Display the running processes of a container docker logs \u003ccontainer-id\u003e -f ## Live tail a container's logs docker login ## Log in this CLI session using your Docker credentials docker tag \u003cimage\u003e username/repository:tag ## Tag \u003cimage\u003e for upload to registry docker push username/repository:tag ## Upload tagged image to registry docker run username/repository:tag ## Run image from a registry docker system prune ## Remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes. (Docker 17.06.1-ce and superior) docker system prune -a ## Remove all unused containers, networks, images not just dangling ones (Docker 17.06.1-ce and superior) docker volume prune ## Remove all unused local volumes docker network prune ## Remove all unused networks docker compose docker-compose up # Create and start containers docker-compose up -d # Create and start containers in detached mode docker-compose down # Stop and remove containers, networks, images, and volumes docker-compose logs # View output from containers docker-compose restart # Restart all service docker-compose pull # Pull all image service docker-compose build # Build all image service docker-compose config # Validate and view the Compose file docker-compose scale \u003cservice_name\u003e=\u003creplica\u003e # Scale special service(s) docker-compose top # Display the running processes docker-compose run -rm -p 2022:22 web bash # Start web service and runs bash as its command, remove old container. docker services docker service create \u003coptions\u003e \u003cimage\u003e \u003ccommand\u003e # Create new service docker service inspect --pretty \u003cservice_name\u003e # Display detailed information Service(s) docker service ls # List Services docker service ps # List the tasks of Services docker service scale \u003cservice_name\u003e=\u003creplica\u003e # Scale special service(s) docker service update \u003coptions\u003e \u003cservice_name\u003e # Update Service options docker stack docker stack ls # List all running applications on this Docker host docker stack deploy -c \u003ccomposefile\u003e \u003cappname\u003e # Run the specified Compose file docker stack services \u003cappname\u003e # List the services associated with an app docker stack ps \u003cappname\u003e # List the running containers associated with an app docker stack rm \u003cappname\u003e # Tear down an application docker machine docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux) docker-machine create -d hyperv --hyperv-virtual-switch \"myswitch\" myvm1 # Win10 docker-machine env myvm1 # View basic information about your node docker-machine ssh myvm1 \"docker node ls\" # List the nodes in your swarm docker-machine ssh myvm1 \"docker node inspect \u003cnode ID\u003e\" # Inspect a node docker-machine ssh myvm1 \"docker swarm join-token -q worker\" # View join token docker-machine ssh myvm1 # Open an SSH session with the VM; type \"exit\" to end docker-machine ssh myvm2 \"docker swarm leave\" # Make the worker leave the swarm docker-machine ssh myvm1 \"docker swarm leave -f\" # Make master leave, kill swarm docker-machine start myvm1 # Start a VM that is currently not running docker-machine stop $(docker-machine ls -q) # Stop all running VMs docker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images docker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir docker-machine ssh myvm1 \"docker stack deploy -c \u003cfile\u003e \u003capp\u003e\" # Deploy an app Options for popular commands docker build Docs Build an image from a Dockerfile.\ndocker build [DOCKERFILE PATH] Example\nBuild an image tagged my-org/my-image where the Dockerfile can be found at /tmp/Dockerfile.\ndocker build -t my-org:my-image -f /tmp/Dockerfile Flags\n--file -f Path where to find the Dockerfile --force-rm Always remove intermediate containers --no-cache Do not use cache when building the image --rm Remove intermediate containers after a successful build (this is true) by default --tag -t Name and optionally a tag in the ‘name:tag’ format docker run Docs\nCreates and starts a container in one operation. Could be used to execute a single command as well as start a long-running container.\nExample\ndocker run -it ubuntu:latest /bin/bash This will start a ubuntu container with the entrypoint /bin/bash. Note that if you do not have the ubuntu image downloaded it will download it before running it.\nFlags\n-it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) --rm Automatically remove the container when it exit. Otherwise it will be stored and visible running docker ps -a. --detach -d Run container in background and print container ID --volume -v Bind mount a volume. Useful for accessing folders on your local disk inside your docker container, like configuration files or storage that should be persisted (database, logs etc.). docker exec Docs\nExecute a command inside a running container.\ndocker exec [CONTAINER ID] Example\ndocker exec [CONTAINER ID] touch /tmp/exec_works Flags\n--detach -d Detached mode: run command in the background -it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) docker images Docs\nList all downloaded/created images.\ndocker images Flags\n-q Only show numeric IDs docker inspect Docs\nShows all the info of a container.\ndocker inspect [CONTAINER ID] docker logs Docs\nGets logs from container.\ndocker logs [CONTAINER ID] Flags\n--details Log extra details --follow -f Follow log output. Do not stop when end of file is reached, but rather wait for additional data to be appended to the input. --timestamps -t Show timestamps docker ps Docs\nShows information about all running containers.\ndocker ps Flags\n--all -a Show all containers (default shows just running) --filter -f Filter output based on conditions provided, docker ps -f=\"name=\"example\" --quiet -q Only display numeric IDs docker rmi Docs\nRemove one or more images.\ndocker rmi [IMAGE ID] Flags\n--force -f Force removal of the image Snippets A collection of useful tips and tricks for Docker.\nDelete all containers NOTE: This will remove ALL your containers.\ndocker container prune OR, if you’re using an older docker client:\ndocker rm $(docker ps -a -q) Delete all untagged containers docker image prune OR, if you’re using an older docker client:\ndocker rmi $(docker images | grep '^\u003cnone\u003e' | awk '{print $3}') Remove all docker images with none tag docker rmi --force $(docker images --filter \"dangling=true\" -q) See all space Docker take up docker system df Get IP address of running container docker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d '\"' -f 4 Kill all running containers docker kill $(docker ps -q) Resources docs.docker.com docker-cheat-sheet docker-cheat-sheet https://sourabhbajaj.com/mac-setup/Docker/ ","description":"Most Popular Docker Commands","title":"Top Docker Commands","uri":"/en/posts/docker-commands/"},{"content":"most popular gg ## go to first line ge ## jump backward to end of words :n ## go To line n cursor movements h ## move left j ## move down k ## move up l ## move right b / w ## Previous/next word (punctuaction) B / W ## Previous/next word (spaces separate words) ge / e ## Previous/next end of word E ## jump to end of words (no punctuation) 0 ## (zero) start of line ^ ## first non-blank character of line $ ## end of line - ## move line upwards, on the first non blank character + ## move line downwards, on the first non blank character \u003center\u003e ## move line downwards, on the first non blank character gg ## go to first line G ## go to last line ngg ## go to line n nG ## go To line n :n ## go To line n ) ## move the cursor forward to the next sentence. ( ## move the cursor backward by a sentence. { ## move the cursor a paragraph backwards } ## move the cursor a paragraph forwards ]] ## move the cursor a section forwards or to the next { [[ ## move the cursor a section backwards or the previous { CTRL-f ## move the cursor forward by a screen of text CTRL-b ## move the cursor backward by a screen of text CTRL-u ## move the cursor up by half a screen CTRL-d ## move the cursor down by half a screen H ## move the cursor to the top of the screen. M ## move the cursor to the middle of the screen. L ## move the cursor to the bottom of the screen. fx ## search line forward for 'x' Fx ## search line backward for 'x' tx ## search line forward before 'x' Tx ## search line backward before 'x' insert mode i ## start insert mode at cursor I ## insert at the beginning of the line a ## append after the cursor A ## append at the end of the line o ## open (append) blank line below current line O ## open blank line above current line Esc ## exit insert mode edit mode r ## replace a single character (does not use insert mode) R ## enter Insert mode, replacing characters rather than inserting J ## join line below to the current one cc ## change (replace) an entire line cw ## change (replace) to the end of word C ## change (replace) to the end of line ct' ## change (replace) until the ' character (can change ' for any character) s ## delete character at cursor and substitute text S ## delete line at cursor and substitute text (same as cc) xp ## transpose two letters (delete and paste, technically) u ## undo CTRL-r ## redo . ## repeat last command ~ ## switch case g~iw ## switch case of current word gUiw ## make current word uppercase guiw ## make current word lowercase gU$ ## make uppercase until end of line gu$ ## make lowercase until end of line \u003e\u003e ## indent line one column to right \u003c\u003c ## indent line one column to left == ## auto-indent current line ddp ## swap current line with next ddkp ## swap current line with previous :%retab ## fix spaces / tabs issues in whole file :r [name] ## insert the file [name] below the cursor. :r !{cmd} ## execute {cmd} and insert its standard output below the cursor. delete text x ## delete current character X ## delete previous character dw ## delete the current word dd ## delete (cut) a line dt' ## delete until the next ' character on the line (replace ' by any character) D ## delete from cursor to end of line :[range]d ## delete [range] lines copy/move text yw ## yank word yy ## yank (copy) a line 2yy ## yank 2 lines y$ ## yank to end of line p ## put (paste) the clipboard after cursor/current line P ## put (paste) before cursor/current line :set paste ## avoid unexpected effects in pasting :registers ## display the contents of all registers \"xyw ## yank word into register x \"xyy ## yank line into register x :[range]y x ## yank [range] lines into register x \"xp ## put the text from register x after the cursor \"xP ## put the text from register x before the cursor \"xgp ## just like \"p\", but leave the cursor just after the new text \"xgP ## just like \"P\", but leave the cursor just after the new text :[line]put x ## put the text from register x after [line] search/replace /pattern ## search for pattern ?pattern ## search backward for pattern n ## repeat search in same direction N ## repeat search in opposite direction * ## search forward, word under cursor # ## search backward, word under cursor set ic ## ignore case: turn on set noic ## ignore case: turn off :%s/old/new/g ## replace all old with new throughout file :%s/old/new/gc ## replace all old with new throughout file with confirmation :argdo %s/old/new/gc | wq ## open multiple files and run this command to replace old with new in every file with confirmation, save and quit visual mode v ## start visual mode, mark lines, then do command (such as y-yank) V ## enter visual line mode o ## move to other end of marked area U ## upper case of marked area CTRL-v ## start visual block mode O ## move to other corner of block aw ## mark a word d / x ## Delete selection s ## Replace selection y ## Yank selection (Copy) ab ## a () block (with braces) ab ## a {} block (with brackets) ib ## inner () block ib ## inner {} block Esc ## exit visual mode commands \u003e ## shift right \u003c ## shift left c ## change (replace) marked text y ## yank (copy) marked text d ## delete marked text ~ ## switch case bookmarks :marks ## list all the current marks ma ## make a bookmark named a at the current cursor position `a ## go to position of bookmark a 'a ## go to the line with bookmark a `. ## go to the line that you last edited macros qa ## start recording macro 'a' q ## end recording macro @a ## replay macro 'a' @: ## replay last command spelling ]s ## next misspelled word [s ## previous misspelled word zg ## add word to wordlist zug ## undo last add word z= ## suggest word exit :q ## quit Vim. This fails when changes have been made. :q! ## quit without writing. :cq ## quit always, without writing. :w ## save without exiting. :wq ## write the current file and exit. :wq! ## write the current file and exit always. :wq {file} ## write to {file}. Exit if not editing the last :wq! {file} ## write to {file} and exit always. :[range]wq[!] ## same as above, but only write the lines in [range]. ZZ ## write current file, if modified, and exit. ZQ ## quit current file and exit (same as \":q!\"). multiple files :e filename edit a file in a new buffer :tabe filename edit a file in a new tab (Vim7, gVim) :ls list all buffers :bn go to next buffer :bp go to previous buffer :bd delete a buffer (close a file) :b1 show buffer 1 :b vimrc show buffer whose filename begins with \"vimrc\" :bufdo \u003ccommand\u003e run 'command(s)' in all buffers :[range]bufdo \u003ccommand\u003e run 'command(s)' for buffers in 'range' windows :sp f split open f :vsp f vsplit open f CTRL-w s split windows CTRL-w w switch between windows CTRL-w q quit a window CTRL-w v split windows vertically CTRL-w x swap windows CTRL-w h left window CTRL-w j down window CTRL-w k up window CTRL-w l right window CTRL-w + increase window height CTRL-w - decrease window height CTRL-w \u003c increase window width CTRL-w \u003e decrease window width CTRL-w = equal window CTRL-w o close other windows zz Centers the window to the current line programming % show matching brace, bracket, or parenthese gf edit the file whose name is under or after the cursor gd when the cursor is on a local variable or function, jump to its declaration '' return to the line where the cursor was before the latest jump gi return to insert mode where you inserted text the last time CTRL-o move to previous position you were at CTRL-i move to more recent position you were at Resources vim faq vim cheat sheet vim cheat sheet ","description":"Most Popular Vim Commands","title":"Top Vim Commands","uri":"/en/posts/vim-commands/"},{"content":"Uploading a File to Amazon S3 Introduction When you upload a folder from your local system or another machine, Amazon S3 uploads all the files and subfolders from the specified local folder to your bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. In this lab step, you will upload a file to your bucket. The process is similar to uploading a single file, multiple files, or a folder with files in it.\nIn order to complete this lab step, you have to upload the cloudacademy-logo.png file from your local file storage into an S3 folder you created earlier.\nDownload the Cloud Academy logo from the following location: https://s3-us-west-2.amazonaws.com/clouda-labs/scripts/s3/cloudacademy-logo.png (If the image is not downloaded for you, simply right-click the image and select Save image as to download it to your local file system.)\nInstructions Click on the cloudfolder folder. You are placed within the empty folder in your S3 bucket: Note: Click the folder name itself, not the checkbox for the folder name. If you select the folder checkbox then upload a file, it will be placed above the folder (not inside it).\nClick the Upload button.\nClick Add Files:\nA file picker will appear.\nBrowse to and select the local copy of cloudacademy-logo.png file that you downloaded earlier: The logo is added to the list of files that are ready to upload. You have several options at this point:\nAdd more files Upload However, there is another method that some users prefer to add files for upload.\nCheck the file and click on Remove:\nThis time, rather than browsing to a file, drag and drop the logo file onto the wizard. The wizard adds it to the list of files to upload.\nScroll to the bottom of the page and click Upload to upload the file:\nYou will see a blue notification that the file is uploading and then a green notification that the upload has been completed successfully.\nThe file is placed in the folder in your bucket:\n","description":"Uploading a File to Amazon S3","title":"Upload a file to S3","uri":"/en/docs/aws-certified-developer-associate/s3/upload-file-to-s3/"},{"content":"About AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\nDocumentation User Guide X-Ray allows software engineers to view the state of a system at a glance, identify potential bottlenecks, and make informed operational decisions to improve performance and reliability. X-Ray inspects application code using a combination of machine and customer-provided data to identify potential bottlenecks and analyze performance and performance trends for each test scenario.\nTerminology AWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application\nX-Ray Trace Hierarchy: Trace \u003e Segment \u003e Sub Segment\nTrace\nAn X-Ray trace is a set of data points that share the same trace ID.\nSegments\nA segment is a JSON representation of a request that your application serves.\nA trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases.\nSubsegments\nSubsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.\nAnnotations\nAn X-Ray annotation is system-defined, or user-defined data associated with a segment A segment can contain multiple annotations. Annotations are used to describe the request, the response, and other information about the segment Can be used for adding system or user-defined data to segments and subsegments that you want to index for search. Sampling\nX-Ray traces are sampled at a rate that you specify. The rate is specified in the sampling_rate field of the sampling object in the config object.\nMetadata\nX-Ray traces contain metadata that is useful for understanding the trace.\nMetadata (Key / value pairs) is not indexed and cannot be used for searching Digest Trace request across microservices/AWS services\nAnalyze, Troubleshoot errors, Solve performance issues Gather tracing information From applications/components/AWS Services Tools to view, filter and gain insights (Ex: Service Map) How does Tracing work?\nUnique trace ID assigned to every client request X-Amzn-Trace-Id:Root=1-5759e988-bd862e3fe Each service in request chain sends traces to X-Ray with trace ID X-Ray gathers all the information and provides visualization How do you reduce performance impact due to tracing? Sampling - Only a sub set of requests are sampled (Configurable) How can AWS Services and your applications send tracing info? Step 1 : Update Application Code Using X-Ray SDK Step 2: Use X-Ray agents (EASY to use in some services! Ex: AWS Lambda) Segments and Sub-segments can include an annotations object containing one or more fields that X-Ray indexes for use with Filter Expressions. It is indexed. Use up to 50 annotations per trace.\nTotal sampled request per second = Reservoir size + ((incoming requests per second - reservoir size) * fixed rate)\nDefault sampling X-ray SDK first request each second and 5% of any additional requests\nTracing header can be added in http request header\nAnnotations vs Segments vs Subsegments vs metadata\nX-ray daemon listens for traffic on UDP port 2000\nX-ray SDK provides interceptors to add your code to trace incoming HTTP requests.\nX-ray in EC2: You need the X-Ray daemon to be running on your EC2 instances in order to send data to X-Ray. User data script could be used to install the X-Ray daemon in EC2 instance.\nX-ray in ECS: In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.\nX-ray in elastic beanstalk: Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-service architecture.\nA segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about - downstream calls that your application made to fulfill the original request.\nAdd annotations to subsegment document if you want to trace downstream calls.\nSegments and subsegment can include a metadata object containing one or more fields with values of any type, including objects and arrays.\nTracing header is added in the HTTP request header. A tracing header (X-Amzn-Trace-ld) can originate from the X-Ray SDK, an AWS service, or the - client request.\nUse the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in - order to develop the custom debug tool\nPrice Current price\nUse Cases Type: Developer Tools\nAlternatives Google Stackdriver Azure Monitor Elastic Observability Datadog Splunk AWS X-Ray supports applications running on:\nAmazon Elastic Compute Cloud (Amazon EC2) Amazon EC2 Container Service (Amazon ECS) AWS Lambda WS Elastic Beanstalk Practice Questions Q1 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\nWhich AWS service will be your main tool?\nAWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks\n1\n","description":"Analyze and debug production, distributed applications","title":"X-Ray","uri":"/en/docs/aws-certified-developer-associate/xray/"}]